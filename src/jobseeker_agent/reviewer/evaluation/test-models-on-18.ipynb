{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "acf75652",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_prompt = r\"\"\"\n",
    "You are an expert in job-candidates matching.\n",
    "From the job description: {job_description}, \n",
    "company name: {company_name},\n",
    "job title: {job_title},\n",
    "and location: {location},\n",
    "and my profil: {profil_pro},\n",
    "\n",
    "Identify which of the following criteria are met by the job description:\n",
    "\n",
    "## The Job\n",
    "### Required expertise\n",
    "- Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill: (+2)\n",
    "- Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP): (+2)\n",
    "- Agentic workflows (ie. langchain, tool use, prompt engineering, etc.) are part of the job: (+2), +1 more if a large part of the job is dedicated to this.\n",
    "- Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences: (-2 if this domain/tool is central to the role, defined as being in the job title, company name, or a primary responsibility/requirement; -1 if it is a secondary qualification).\n",
    "- Requires a programming language I am not familiar with, AND does not mention Python: (-1)\n",
    "- More focused on infrastructure (databases, cloud, Docker) than on algorithms: (-3)\n",
    "- Vague description of actual tasks for a data scientist/engineer job: (-1)\n",
    "- 'Optimization' mentioned primarily for performance/infrastructure (e.g., inference speed, cloud costs, MLOps): (-3)\n",
    "- 'optimization' mentioned primarily in the context of quantum algorithms: (-4)\n",
    "- The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified. : (+0.5)\n",
    "- Requires \"deep expertise\" / \"senior-level experience\" / \"mastery\" of MLOps, large-scale training, or inference optimization (beyond just \"good fundamentals\" or \"being comfortable\"): (-1)\n",
    "- Requires a PhD in a field close to mine (or even if it is just a plus) (has to be explicitly mentioned in the job description. Having experience leading research teams does not imply a PhD): (+1.5)\n",
    "- Does not mention a PhD but requires experience doing research: (+1)\n",
    "### Type of role\n",
    "- More managerial than technical role: (-2)\n",
    "- Involves leading a team of highly qualified/experienced people (junior excluded): (-1) In a domain I am not familiar with: (-1)\n",
    "- Involves coaching world-class scientists: (-2)\n",
    "\n",
    "## The Company\n",
    "- Top-tier company (e.g., Google, Apple, Meta, Helsing, Mistral AI, Perplexity, OpenAI, Anthropic, Nvidia): (+2) (Do not trust the description of the company in the job description for this criteria, but your prior knowledge about the company if any.)\n",
    "- More than 150 employees: (-1)\n",
    "- Offers a full-remote option: (+2)\n",
    "- Consulting job for a standard/low-tier consulting firm: (-2)\n",
    "- In the defense sector: (+2)\n",
    "- In the robotics sector: (+2)\n",
    "- If not french, requires security clearance: (-1.5)\n",
    "\n",
    "^ only mention the lines that are relevant to the job description, with associated score bonus or penalty. \n",
    "For example, do not output \"- Leading a team: No (+0)\". Instead do not output anything for this criteria.\n",
    "For each line that is present in the result, mention the sentence/line that satisfies the criteria..\n",
    "Use strictly the elements above for score computation, not the synthesis below.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4170fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobseeker_agent.utils.paths import load_prompt, load_full_job\n",
    "\n",
    "profil_pro = load_prompt(\"profil_pro\")\n",
    "job_id = 18\n",
    "job = load_full_job(job_id)\n",
    "job_description = job[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6be1ecb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 18,\n",
       " 'title': 'Applied ML/AI Engineer - Monitoring',\n",
       " 'company': 'Sifflet',\n",
       " 'location': 'Paris, Île-de-France, France',\n",
       " 'job_link': 'https://fr.linkedin.com/jobs/view/applied-ml-ai-engineer-monitoring-at-sifflet-4314688170',\n",
       " 'posted_date': '2025-10-14',\n",
       " 'status': 'Open',\n",
       " 'workplace_type': 'Remote',\n",
       " 'description': \"**About Sifflet  \\n  \\n** We are building the world’s best data observability platform to help\\ncompanies excel at data-driven decision making.  \\n  \\nToday, half of a data team’s time is spent troubleshooting data quality\\nissues. Sifflet is putting an end to that. Our solution allows data engineers\\nand data consumers to visualize how data flows between their services, define\\ndata quality checks, and quickly find the root cause of any data anomaly.  \\n  \\n**About The Job  \\n  \\n** The monitoring team implements the foundational capabilities of Sifflet:\\ndetecting data quality issues across a wide range of data warehouses and\\ndatabases.  \\n  \\nSifflet's monitoring capabilities rely heavily on machine learning (ML)\\ntechniques. Most advanced data quality checks are based on time series\\nforecasting models that detect unexpected distribution changes while\\naccounting for seasonality and one-off events. Additionally, ML-based features\\nare present throughout our product, be it for intelligent alert grouping,\\nautomated incident description, or automated monitor suggestions.  \\n  \\nAs a machine learning engineer on the monitoring team, you will:  \\n  \\n\\n  * Build automated data profiling systems that learn normal data patterns and detect deviations.\\n  * Deploy time series forecasting models that understand data seasonality and business cycles.\\n  * Create intelligent alerting systems that reduce noise through ML-powered incident correlation.\\n  * Implement generative AI workflows across the product, such as enabling users to describe their monitoring needs in natural language.\\n  * Contribute to product decisions and identify areas where adding ML/AI-based features can solve customer pain points.  \\n  \\n\\nAs we’re a small team, you will be expected to design, implement, deploy and\\nmaintain your projects in production, and integrate them with other services.\\nThus, this role includes a significant software engineering component.  \\n  \\nSome projects you could be working on  \\n  \\n\\n  * Automated monitor recommendations based on data profiling metrics.\\n  * Automated root cause analysis of any data quality incident, building upon the many sources of metadata Sifflet collects (table lineage, query history, past monitor failures…).  \\n  \\n\\nOur stack  \\n  \\n\\n  * The monitoring engine is built with Python 3 and its large data/ML ecosystem (notably PyTorch).\\n  * The web API is written in (modern) Java with Spring Boot 3, the web frontend is a VueJS application written in Typescript. You may occasionally need to make minor changes to this code base.\\n  * Infrastructure: Kubernetes (AWS EKS clusters), MySQL (on AWS RDS), Temporal for job orchestration\\n  * Plus a few supporting services: Gitlab CI, Prometheus/Loki/Grafana, Sentry…  \\n  \\n\\nWhile not directly part of our stack, expect to gain a lot of knowledge on\\nmany products in the modern data ecosystem. The subtleties of BigQuery or\\nSnowflake will soon be very familiar to you.  \\n  \\nAre we the company you’re looking for?  \\n  \\n\\n  * We have offices in Paris, but we’re very remote friendly - several team members are fully remote.\\n  * We offer competitive salary and company equity.\\n  * We have experts on many topics, so there’s always someone to help. We also have tech talks where everyone can discuss a cool project or technology.\\n  * We’re constantly exposed to the intricacies of the modern data ecosystem - you’ll become very knowledgeable about data engineering and the modern data stack, and about how data is used in enterprises.\\n  * Our culture emphasises teamwork to efficiently deliver projects to production.\\n  * We’re building a genuinely great product, and we think you’ll love the team!\\n  * More than three years of experience in a ML engineer role or equivalent. Hands-on production experience is appreciated.\\n  * General knowledge of the “modern data stack” ecosystem, especially data warehouses and databases. You don’t have to know everything upfront of course, you’ll pick up what you need on the job.\\n  * Experience with the Python ML ecosystem.\\n  * You value ownership of your projects from design to production, and aren’t afraid of taking initiatives.  \\n  \\n\\nAll written communication at Sifflet is in English, but the engineering team\\nroutinely uses French, so some level of fluency in French is required.  \\n  \\nNone of the people who joined Sifflet perfectly matched the described\\nrequirements for the role. If you’re interested in this position but don’t\\ntick all the boxes above, feel free to apply anyway!  \\n  \\n\\n  * A first call with either Benoît (Head of Engineering) or Pierre (Monitoring Team Lead) (30 minutes)\\n  * One coding interview and one system design interview (1h30 each)\\n  * One call with a product manager (30 minutes)  \\n  \\n\\nAt this point we generally know if we want to extend an offer, but we're happy\\nto organize additional sessions so you can better know the team and the\\ncompany.\\n\\nShow more  Show less\\n\\n\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f98b580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**About Sifflet  \n",
      "  \n",
      "** We are building the world’s best data observability platform to help\n",
      "companies excel at data-driven decision making.  \n",
      "  \n",
      "Today, half of a data team’s time is spent troubleshooting data quality\n",
      "issues. Sifflet is putting an end to that. Our solution allows data engineers\n",
      "and data consumers to visualize how data flows between their services, define\n",
      "data quality checks, and quickly find the root cause of any data anomaly.  \n",
      "  \n",
      "**About The Job  \n",
      "  \n",
      "** The monitoring team implements the foundational capabilities of Sifflet:\n",
      "detecting data quality issues across a wide range of data warehouses and\n",
      "databases.  \n",
      "  \n",
      "Sifflet's monitoring capabilities rely heavily on machine learning (ML)\n",
      "techniques. Most advanced data quality checks are based on time series\n",
      "forecasting models that detect unexpected distribution changes while\n",
      "accounting for seasonality and one-off events. Additionally, ML-based features\n",
      "are present throughout our product, be it for intelligent alert grouping,\n",
      "automated incident description, or automated monitor suggestions.  \n",
      "  \n",
      "As a machine learning engineer on the monitoring team, you will:  \n",
      "  \n",
      "\n",
      "  * Build automated data profiling systems that learn normal data patterns and detect deviations.\n",
      "  * Deploy time series forecasting models that understand data seasonality and business cycles.\n",
      "  * Create intelligent alerting systems that reduce noise through ML-powered incident correlation.\n",
      "  * Implement generative AI workflows across the product, such as enabling users to describe their monitoring needs in natural language.\n",
      "  * Contribute to product decisions and identify areas where adding ML/AI-based features can solve customer pain points.  \n",
      "  \n",
      "\n",
      "As we’re a small team, you will be expected to design, implement, deploy and\n",
      "maintain your projects in production, and integrate them with other services.\n",
      "Thus, this role includes a significant software engineering component.  \n",
      "  \n",
      "Some projects you could be working on  \n",
      "  \n",
      "\n",
      "  * Automated monitor recommendations based on data profiling metrics.\n",
      "  * Automated root cause analysis of any data quality incident, building upon the many sources of metadata Sifflet collects (table lineage, query history, past monitor failures…).  \n",
      "  \n",
      "\n",
      "Our stack  \n",
      "  \n",
      "\n",
      "  * The monitoring engine is built with Python 3 and its large data/ML ecosystem (notably PyTorch).\n",
      "  * The web API is written in (modern) Java with Spring Boot 3, the web frontend is a VueJS application written in Typescript. You may occasionally need to make minor changes to this code base.\n",
      "  * Infrastructure: Kubernetes (AWS EKS clusters), MySQL (on AWS RDS), Temporal for job orchestration\n",
      "  * Plus a few supporting services: Gitlab CI, Prometheus/Loki/Grafana, Sentry…  \n",
      "  \n",
      "\n",
      "While not directly part of our stack, expect to gain a lot of knowledge on\n",
      "many products in the modern data ecosystem. The subtleties of BigQuery or\n",
      "Snowflake will soon be very familiar to you.  \n",
      "  \n",
      "Are we the company you’re looking for?  \n",
      "  \n",
      "\n",
      "  * We have offices in Paris, but we’re very remote friendly - several team members are fully remote.\n",
      "  * We offer competitive salary and company equity.\n",
      "  * We have experts on many topics, so there’s always someone to help. We also have tech talks where everyone can discuss a cool project or technology.\n",
      "  * We’re constantly exposed to the intricacies of the modern data ecosystem - you’ll become very knowledgeable about data engineering and the modern data stack, and about how data is used in enterprises.\n",
      "  * Our culture emphasises teamwork to efficiently deliver projects to production.\n",
      "  * We’re building a genuinely great product, and we think you’ll love the team!\n",
      "  * More than three years of experience in a ML engineer role or equivalent. Hands-on production experience is appreciated.\n",
      "  * General knowledge of the “modern data stack” ecosystem, especially data warehouses and databases. You don’t have to know everything upfront of course, you’ll pick up what you need on the job.\n",
      "  * Experience with the Python ML ecosystem.\n",
      "  * You value ownership of your projects from design to production, and aren’t afraid of taking initiatives.  \n",
      "  \n",
      "\n",
      "All written communication at Sifflet is in English, but the engineering team\n",
      "routinely uses French, so some level of fluency in French is required.  \n",
      "  \n",
      "None of the people who joined Sifflet perfectly matched the described\n",
      "requirements for the role. If you’re interested in this position but don’t\n",
      "tick all the boxes above, feel free to apply anyway!  \n",
      "  \n",
      "\n",
      "  * A first call with either Benoît (Head of Engineering) or Pierre (Monitoring Team Lead) (30 minutes)\n",
      "  * One coding interview and one system design interview (1h30 each)\n",
      "  * One call with a product manager (30 minutes)  \n",
      "  \n",
      "\n",
      "At this point we generally know if we want to extend an offer, but we're happy\n",
      "to organize additional sessions so you can better know the team and the\n",
      "company.\n",
      "\n",
      "Show more  Show less\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6fd2413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Dict, Union\n",
    "\n",
    "class Evaluation(TypedDict):\n",
    "    \"\"\"Evaluation of the job description.\"\"\"\n",
    "    criteria: Annotated[str, ..., \"The criteria that are met by the job description.\"]\n",
    "    evidence: Annotated[str, ..., \"The evidence for the criteria that are met by the job description.\"]\n",
    "    score: Annotated[float, ..., \"The score for the criteria that are met by the job description.\"]\n",
    "\n",
    "class JobReviewResponse(TypedDict):\n",
    "    \"\"\"Response structure for job review.\"\"\"\n",
    "    evaluation_grid: Annotated[List[Evaluation], ..., \"List of evaluations for each relevant evaluation criterion\"]\n",
    "    score: Annotated[float, ..., \"raw score computed from the evaluation grid. Can be negative.\"]\n",
    "\n",
    "class JobReviewCorrectionResponse(TypedDict):\n",
    "    \"\"\"Response structure for job review correction.\"\"\"\n",
    "    correction: Annotated[str, ..., \"Correction of the evaluation grid.\"]\n",
    "    evaluation_grid: Annotated[List[Evaluation], ..., \"List of evaluations for each relevant evaluation criterion\"]\n",
    "    score: Annotated[float, ..., \"raw score computed from the evaluation grid. Can be negative.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ad835df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobseeker_agent.utils.llm import get_llm\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import json\n",
    "def review(model, job_description, job_title, company_name, location, with_correction=False):\n",
    "    llm = get_llm(model)\n",
    "    llm = llm.with_structured_output(JobReviewResponse)\n",
    "    message = HumanMessage(\n",
    "        content=review_prompt.format(job_description=job_description, profil_pro=profil_pro, job_title=job_title, company_name=company_name, location=location)\n",
    "    )\n",
    "    response = llm.invoke([message])\n",
    "    if with_correction:\n",
    "        messages = [\n",
    "            message,\n",
    "            AIMessage(content=json.dumps(response)),\n",
    "            HumanMessage(content=\"Please correct the evaluation grid. Evaluate each element. Is it correct ? Are there any missing element ? If elements are removed from the evaluation grid, don't put them in the evaluation grid.\")\n",
    "        ]\n",
    "        response = llm.invoke(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e54c016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chargement du modèle OpenAI : gpt-4o\n",
      "4.5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "model = \"gpt-4o\"\n",
    "result = review(model, with_correction=True)\n",
    "print(result[\"score\"])\n",
    "# print the dict in easy to read format\n",
    "# print(json.dumps(result, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6871af3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761664652.990170 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"evaluation_grid\": [\n",
      "        {\n",
      "            \"criteria\": \"Heavily features agentic workflows (ie. langchain, tool use, prompt engineering, etc.)\",\n",
      "            \"evidence\": \"Implement generative AI workflows across the product, such as enabling users to describe their monitoring needs in natural language.\",\n",
      "            \"score\": 3.0\n",
      "        },\n",
      "        {\n",
      "            \"criteria\": \"Requires a programming language I am not familiar with (and not Python)\",\n",
      "            \"evidence\": \"The web API is written in (modern) Java with Spring Boot 3, the web frontend is a VueJS application written in Typescript. You may occasionally need to make minor changes to this code base.\",\n",
      "            \"score\": -1.0\n",
      "        },\n",
      "        {\n",
      "            \"criteria\": \"The job is based in France and requires a good english level\",\n",
      "            \"evidence\": \"We have offices in Paris, but we\\u2019re very remote friendly - several team members are fully remote. All written communication at Sifflet is in English, but the engineering team routinely uses French, so some level of fluency in French is required.\",\n",
      "            \"score\": 0.5\n",
      "        },\n",
      "        {\n",
      "            \"criteria\": \"Offers a full-remote option\",\n",
      "            \"evidence\": \"We have offices in Paris, but we\\u2019re very remote friendly - several team members are fully remote.\",\n",
      "            \"score\": 2.0\n",
      "        }\n",
      "    ],\n",
      "    \"score\": 4.5\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = \"gemini-2.5-flash\"\n",
    "result = review(model)\n",
    "\n",
    "# print the dict in easy to read format\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d52028e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-4o\n",
      "✅ Chargement du modèle OpenAI : gpt-4o\n",
      "✅ Chargement du modèle OpenAI : gpt-5-nano\n",
      "✅ Chargement du modèle OpenAI : gpt-5-nano\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "models = [\"gpt-4.1\", \"gpt-4o\", \"gpt-5-nano\", \"gpt-5-mini\", \"gpt-5\"]\n",
    "responses = []\n",
    "corrections = [True, False]\n",
    "for model in models:\n",
    "    for correction in corrections:\n",
    "        start_time = time.time()\n",
    "        response = review(model, correction)\n",
    "        end_time = time.time()\n",
    "        response[\"model\"] = model\n",
    "        response[\"time\"] = end_time - start_time\n",
    "        response[\"correction\"] = correction\n",
    "        responses.append(response)\n",
    "\n",
    "# print the dict in easy to read format\n",
    "# print(json.dumps(responses, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "641dcb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_responses = responses.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0312b196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761729285.206628 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "models = [\"gemini-2.5-flash\"]\n",
    "google_responses = []\n",
    "corrections = [False]\n",
    "for model in models:\n",
    "    for correction in corrections:\n",
    "        start_time = time.time()\n",
    "        response = review(model, correction)\n",
    "        end_time = time.time()\n",
    "        response[\"model\"] = model\n",
    "        response[\"time\"] = end_time - start_time\n",
    "        response[\"correction\"] = correction\n",
    "        google_responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a2c69c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              model  correction  score       time\n",
      "0  gemini-2.5-flash       False    4.5  35.850183\n"
     ]
    }
   ],
   "source": [
    "# show a table with score, time and model\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(google_responses)\n",
    "df = df[[\"model\", \"correction\", \"score\", \"time\"]]\n",
    "# print(df[df[\"score\"] ==4.5])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09338d",
   "metadata": {},
   "source": [
    "Les deux meilleurs modèles sont gpt-4.1 avec correction et gpt-5-mini sans correction.\n",
    "Le premier met quasi 2x moins de temps que le second. Du coup pour l'instant il gagne.\n",
    "à voir quels sont "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11422973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "GPT-4.1\n",
      "[{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill', 'evidence': 'The job description does not mention Reinforcement Learning (RL) anywhere.', 'score': 0}, {'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)', 'evidence': 'There is no explicit mention of algorithmic/mathematical optimization, Operations Research, or related terms in the job description.', 'score': 0}, {'criteria': 'Agentic workflows (ie. langchain, tool use, prompt engineering, etc.) are part of the job', 'evidence': \"The job description mentions 'Implement generative AI workflows across the product, such as enabling users to describe their monitoring needs in natural language.' This suggests some involvement with prompt engineering and generative AI workflows, but does not explicitly mention agentic workflows or frameworks like LangChain.\", 'score': 2}, {'criteria': 'Requires strong expertise in a topic/domain I am not familiar with', 'evidence': 'The required expertise is in Python ML ecosystem, time series, and data engineering. These are within your skill set. No penalty.', 'score': 0}, {'criteria': 'Requires a programming language I am not familiar with, AND does not mention Python', 'evidence': 'Python 3 is the main language for the monitoring engine. Java is used for the web API, but Python is primary for ML. No penalty.', 'score': 0}, {'criteria': 'More focused on infrastructure (databases, cloud, Docker) than on algorithms', 'evidence': 'The job is focused on building ML-based monitoring and alerting systems, not primarily on infrastructure. Infrastructure is mentioned as part of the stack, but not as the main focus.', 'score': 0}, {'criteria': 'Vague description of actual tasks for a data scientist/engineer job', 'evidence': 'The job description is quite specific about the tasks: building data profiling systems, deploying time series models, creating alerting systems, etc.', 'score': 0}, {'criteria': \"'Optimization' mentioned primarily for performance/infrastructure (e.g., inference speed, cloud costs, MLOps)\", 'evidence': 'Optimization is not mentioned in this context.', 'score': 0}, {'criteria': \"'optimization' mentioned primarily in the context of quantum algorithms\", 'evidence': 'Quantum algorithms are not mentioned.', 'score': 0}, {'criteria': 'The job is based in France and requires a good english level', 'evidence': 'The job is based in France (Paris office) and requires English for written communication.', 'score': 0.5}, {'criteria': 'Offers a full-remote option', 'evidence': \"The job description says: 'We have offices in Paris, but we’re very remote friendly - several team members are fully remote.'\", 'score': 2}, {'criteria': 'Requires lots of experience in large scale training/inference/MLOps', 'evidence': 'There is no mention of large scale training/inference/MLOps as a requirement.', 'score': 0}, {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)', 'evidence': 'A PhD is not mentioned as a requirement or a plus.', 'score': 0}, {'criteria': 'More managerial than technical role', 'evidence': 'The role is technical, not managerial.', 'score': 0}, {'criteria': 'Involves leading a team of highly qualified/experienced people (junior excluded)', 'evidence': 'No mention of team leadership responsibilities.', 'score': 0}, {'criteria': 'Involves coaching world-class scientists', 'evidence': 'No mention of coaching world-class scientists.', 'score': 0}, {'criteria': 'Top-tier company (e.g., Google, Apple, Meta, Helsing, Mistral AI, Perplexity, OpenAI, Anthropic, Nvidia)', 'evidence': 'Sifflet is not a top-tier company as defined here.', 'score': 0}, {'criteria': 'More than 150 employees', 'evidence': 'The company size is not specified, but Sifflet is described as a small team. No penalty.', 'score': 0}, {'criteria': 'Consulting job for a standard/low-tier consulting firm', 'evidence': 'Not a consulting job.', 'score': 0}, {'criteria': 'In the defense sector', 'evidence': 'Not in the defense sector.', 'score': 0}, {'criteria': 'In the robotics sector', 'evidence': 'Not in the robotics sector.', 'score': 0}, {'criteria': 'If not french, requires security clearance', 'evidence': 'No mention of security clearance.', 'score': 0}]\n",
      "____________\n",
      "GPT-5-MINI\n",
      "[{'criteria': 'Agentic workflows (ie. langchain, tool use, prompt engineering, etc.) are part of the job (+2)', 'evidence': '\"Implement generative AI workflows across the product, such as enabling users to describe their monitoring needs in natural language.\"', 'score': 2}, {'criteria': 'The job is based in France and requires a good english level (+0.5)', 'evidence': '\"We have offices in Paris, but we’re very remote friendly - several team members are fully remote.\"; \"All written communication at Sifflet is in English, but the engineering team routinely uses French, so some level of fluency in French is required.\"', 'score': 0.5}, {'criteria': 'Offers a full-remote option (+2)', 'evidence': '\"We have offices in Paris, but we’re very remote friendly - several team members are fully remote.\"', 'score': 2}]\n"
     ]
    }
   ],
   "source": [
    "for response in responses:\n",
    "    if response[\"model\"] == \"gpt-4.1\" and response[\"correction\"]:\n",
    "        print(\"____________\")\n",
    "        print(response[\"model\"].upper())\n",
    "        print(response[\"correction\"])\n",
    "    if response[\"model\"] == \"gpt-5-mini\" and not response[\"correction\"]:\n",
    "        print(\"____________\")\n",
    "        print(response[\"model\"].upper())\n",
    "        print(response[\"correction\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e56ec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "GPT-4O\n",
      "The job is based in France and requires a good English level\n",
      "\"All written communication at Sifflet is in English, but the engineering team routinely uses French, so some level of fluency in French is required.\"\n",
      "Offers a full-remote option\n",
      "\"We have offices in Paris, but we’re very remote friendly - several team members are fully remote.\"\n",
      "Requires a PhD in a field close to mine (or even if it is just a plus)\n",
      "\"More than three years of experience in a ML engineer role or equivalent. Hands-on production experience is appreciated.\"\n",
      "Requires a programming language I am not familiar with (and not Python)\n",
      "\"The web API is written in (modern) Java with Spring Boot 3, the web frontend is a VueJS application written in Typescript.\"\n",
      "Requires strong expertise in a topic/domain I am not familiar with\n",
      "\"Sifflet's monitoring capabilities rely heavily on machine learning (ML) techniques. Most advanced data quality checks are based on time series forecasting models.\"\n",
      "More focused on infrastructure (databases, cloud, Docker) than on algorithms\n",
      "\"Infrastructure: Kubernetes (AWS EKS clusters), MySQL (on AWS RDS), Temporal for job orchestration.\"\n",
      "____________\n",
      "GPT-5-MINI\n",
      "Heavily features agentic workflows (ie. langchain, tool use, prompt engineering, etc.): (+3)\n",
      "Implement generative AI workflows across the product, such as enabling users to describe their monitoring needs in natural language.\n",
      "The job is based in France and requires a good english level: (+0.5)\n",
      "We have offices in Paris, but we’re very remote friendly ... All written communication at Sifflet is in English, but the engineering team routinely uses French, so some level of fluency in French is required.\n",
      "Offers a full-remote option: (+2)\n",
      "We have offices in Paris, but we’re very remote friendly - several team members are fully remote.\n",
      "____________\n",
      "GPT-5\n",
      "Offers a full-remote option\n",
      "\"We have offices in Paris, but we’re very remote friendly - several team members are fully remote.\"\n",
      "The job is based in France and requires a good english level\n",
      "\"We have offices in Paris...\" and \"All written communication at Sifflet is in English, but the engineering team routinely uses French, so some level of fluency in French is required.\"\n",
      "____________\n",
      "GEMINI-2.5-FLASH\n",
      "Heavily features agentic workflows (ie. langchain, tool use, prompt engineering, etc.)\n",
      "Implement generative AI workflows across the product, such as enabling users to describe their monitoring needs in natural language.\n",
      "Requires a programming language I am not familiar with (and not Python)\n",
      "The web API is written in (modern) Java with Spring Boot 3, the web frontend is a VueJS application written in Typescript. You may occasionally need to make minor changes to this code base.\n",
      "The job is based in France and requires a good english level\n",
      "We have offices in Paris, but we’re very remote friendly - several team members are fully remote. All written communication at Sifflet is in English, but the engineering team routinely uses French, so some level of fluency in French is required.\n",
      "Requires lots of experience in large scale training/inference/MLOps\n",
      "As we’re a small team, you will be expected to design, implement, deploy and maintain your projects in production, and integrate them with other services. Thus, this role includes a significant software engineering component.\n",
      "Offers a full-remote option\n",
      "We have offices in Paris, but we’re very remote friendly - several team members are fully remote.\n",
      "____________\n",
      "GEMINI-2.5-PRO\n",
      "Heavily features agentic workflows (ie. langchain, tool use, prompt engineering, etc.)\n",
      "Implement generative AI workflows across the product, such as enabling users to describe their monitoring needs in natural language.\n",
      "Requires strong expertise in a topic/domain I am not familiar with\n",
      "General knowledge of the “modern data stack” ecosystem, especially data warehouses and databases.\n",
      "Requires a programming language I am not familiar with (and not Python)\n",
      "The web API is written in (modern) Java with Spring Boot 3, the web frontend is a VueJS application written in Typescript. You may occasionally need to make minor changes to this code base.\n",
      "The job is based in France and requires a good english level\n",
      "All written communication at Sifflet is in English, but the engineering team routinely uses French, so some level of fluency in French is required.\n",
      "Requires lots of experience in large scale training/inference/MLOps\n",
      "As we’re a small team, you will be expected to design, implement, deploy and maintain your projects in production, and integrate them with other services.\n",
      "Offers a full-remote option\n",
      "We have offices in Paris, but we’re very remote friendly - several team members are fully remote.\n"
     ]
    }
   ],
   "source": [
    "from numpy.char import upper\n",
    "\n",
    "\n",
    "for r in responses:\n",
    "    print(\"____________\")\n",
    "    print(r[\"model\"].upper())\n",
    "    for e in r[\"evaluation_grid\"]:\n",
    "        print(e[\"criteria\"])\n",
    "        print(e[\"evidence\"])\n",
    "    # print(e[\"criteria\"] for e in r[\"evaluation_grid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37309f3",
   "metadata": {},
   "source": [
    "# Evaluation sur Les exemples du jeu d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30623f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_tested = [(\"gpt-4.1\", True), (\"gpt-5-mini\", False), (\"gemini-2.5-flash\", False)]\n",
    "oracle_model = (\"gpt-5\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ccae689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobseeker_agent.utils.paths import get_data_path\n",
    "evals_path = get_data_path() / \"reviewer\" / \"tests\" / \"5\" / \"evals.json\"\n",
    "with open(evals_path, \"r\") as f:\n",
    "    evals = json.load(f)\n",
    "\n",
    "ids = []\n",
    "for ev in evals:\n",
    "    ids.append(ev[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b23c79cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 1\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761740195.939168 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 2\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761740330.323214 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 3\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761740469.829023 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 4\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761740598.169456 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 5\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761740803.188805 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 6\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761740976.450089 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 7\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761741094.993964 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 8\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761741286.783700 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 9\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761741438.654276 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 10\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761741580.157041 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 11\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761741720.866506 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 12\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761741838.563378 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for job 13\n",
      "✅ Chargement du modèle OpenAI : gpt-5\n",
      "✅ Chargement du modèle OpenAI : gpt-4.1\n",
      "✅ Chargement du modèle OpenAI : gpt-5-mini\n",
      "✅ Chargement du modèle Gemini : gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761741990.196153 9513333 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models_tested:\n\u001b[32m     18\u001b[39m     start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     result[model[\u001b[32m0\u001b[39m]] = \u001b[43mreview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     end_time = time.time()\n\u001b[32m     21\u001b[39m     result[model[\u001b[32m0\u001b[39m]][\u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m] = end_time - start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mreview\u001b[39m\u001b[34m(model, job_description, job_title, company_name, location, with_correction)\u001b[39m\n\u001b[32m      6\u001b[39m llm = llm.with_structured_output(JobReviewResponse)\n\u001b[32m      7\u001b[39m message = HumanMessage(\n\u001b[32m      8\u001b[39m     content=review_prompt.format(job_description=job_description, profil_pro=profil_pro, job_title=job_title, company_name=company_name, location=location)\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_correction:\n\u001b[32m     12\u001b[39m     messages = [\n\u001b[32m     13\u001b[39m         message,\n\u001b[32m     14\u001b[39m         AIMessage(content=json.dumps(response)),\n\u001b[32m     15\u001b[39m         HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mPlease correct the evaluation grid. Evaluate each element. Is it correct ? Are there any missing element ? If elements are removed from the evaluation grid, don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt put them in the evaluation grid.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/langchain_core/runnables/base.py:3244\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3242\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3244\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3245\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3246\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/langchain_core/runnables/base.py:5711\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5704\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5705\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5706\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5709\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5710\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5711\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5712\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5713\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5714\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5715\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1676\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1673\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1674\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1676\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1790\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m   1789\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.max_retries\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:238\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    231\u001b[39m params = (\n\u001b[32m    232\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    237\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:208\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    210\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlocation is not supported\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m exc.message:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:869\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    866\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    868\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:75\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/grpc/_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/grpc/_interceptor.py:329\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interceptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:78\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     64\u001b[39m     grpc_request = {\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     68\u001b[39m     }\n\u001b[32m     69\u001b[39m     _LOGGER.debug(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         },\n\u001b[32m     77\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m response = \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     80\u001b[39m     response_metadata = response.trailing_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/grpc/_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     new_method,\n\u001b[32m    308\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     new_compression,\n\u001b[32m    313\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/grpc/_channel.py:1192\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1185\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1190\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1191\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     state, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/grpc/_channel.py:1165\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1148\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1149\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1150\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1151\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1163\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1164\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1165\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1166\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "job_id = 3\n",
    "for id in ids:\n",
    "    print(f\"Evaluation for job {id}\")\n",
    "    result = {}\n",
    "    job = load_full_job(id)\n",
    "    job_description = job[\"description\"]\n",
    "    job_title = job[\"title\"]\n",
    "    company_name = job[\"company\"]\n",
    "    location = job[\"location\"]\n",
    "    start_time = time.time()\n",
    "    result[\"id\"] = id\n",
    "    result[\"job_description\"] = job_description\n",
    "    result[\"oracle\"] = review(oracle_model[0], job_description, job_title, company_name, location,  oracle_model[1])\n",
    "    end_time = time.time()\n",
    "    result[\"oracle\"][\"time\"] = end_time - start_time\n",
    "    for model in models_tested:\n",
    "        start_time = time.time()\n",
    "        result[model[0]] = review(model[0], job_description, job_title, company_name, location, model[1])\n",
    "        end_time = time.time()\n",
    "        result[model[0]][\"time\"] = end_time - start_time\n",
    "    results.append(result)\n",
    "\n",
    "# results\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c9693c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'job_description': 'Description not found.',\n",
       "  'oracle': {'evaluation_grid': [], 'score': 0, 'time': 26.32979917526245},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': 'Vague description of actual tasks for a data scientist/engineer job',\n",
       "     'evidence': \"Job description is 'Description not found.' No details are provided about the actual tasks or requirements.\",\n",
       "     'score': -1},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': \"Job title and location: 'Research Scientist (AI) - Science Team', Paris, Île-de-France, France. The job title is in English, and the location is in France.\",\n",
       "     'score': 0.5}],\n",
       "   'score': -0.5,\n",
       "   'time': 5.39702296257019},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': 'Vague description of actual tasks for a data scientist/engineer job',\n",
       "     'evidence': 'Description not found.',\n",
       "     'score': -1}],\n",
       "   'score': -1,\n",
       "   'time': 16.703629970550537},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [],\n",
       "   'score': 0,\n",
       "   'time': 10.142383098602295}},\n",
       " {'id': 2,\n",
       "  'job_description': \"AryaXAI stands at the forefront of AI innovation, revolutionizing AI for\\nmission-critical businesses by building explainable, safe, and aligned systems\\nthat scale responsibly. Our mission is to create AI tools that empower\\nresearchers, engineers, and organizations to unlock AI's full potential while\\nmaintaining transparency and safety.  \\n  \\nOur team thrives on a shared passion for cutting-edge innovation,\\ncollaboration, and a relentless drive for excellence. At AryaXAI, everyone\\ncontributes hands-on to our mission in a flat organizational structure that\\nvalues curiosity, initiative, and exceptional performance.  \\n  \\nAs a research scientist at AryaXAI, you will be uniquely positioned in our\\nteam to work on very large-scale industry problems and push forward the\\nfrontiers of AI technologies. You will become a part of the unique atmosphere\\nwhere startup culture meets research innovation, with key outcomes of speed\\nand reliability.  \\n  \\n**Responsibilities  \\n  \\n**\\n\\n  * You'll work on advanced problems related to AI explainability, AI safety, and AI alignment.\\n  * You'll have flexibility in picking up the specialization areas within ML/DL and problem types that address the above challenges.\\n  * Create new techniques around ML Observability & Alignment.\\n  * Collaborate with MLEs and SDE to roll out the features and manage their quality until they are fully stable.\\n  * Create and maintain technical and product documentation.\\n  * Publish papers in open forums like arxiv and present in industry forums like ICLR NeurIPS etc.   \\n  \\n\\n**Qualifications  \\n  \\n**\\n\\n  * Has a solid academic background in concepts of machine learning or deep learning or reinforcement learning.\\n  * Master or Ph.D in key engineering topics like computer science or Mathematics is required\\n  * Should have published peer-reviewed papers or contributed to opensource tools\\n  * Hands-on experience in working with deep learning frameworks like Tensorflow, Pytorch etc\\n  * Enjoys working on various DL problems that involve using different types of training data sets - textual, tabular, categorical, images etc\\n  * Comfortable deploying code in cloud environments/on-premise environments.\\n  * Good fundamentals in MLOps and productionising ML models.\\n  * Prior experience on working on ML explainability methods - LRP, SHAPE, LIME, IG, CEM etc.\\n  * 2+ years of hands-on experience in Deep Learning or Machine Learning.\\n  * Hands-on experience in implementing techniques like Transformer models, GANs, Deep Learning, etc.\\n\\nShow more  Show less\\n\\n\",\n",
       "  'oracle': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': '\"Has a solid academic background in concepts of machine learning or deep learning or reinforcement learning.\"',\n",
       "     'score': 2},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': '\"Prior experience on working on ML explainability methods - LRP, SHAPE, LIME, IG, CEM etc.\"',\n",
       "     'score': -2},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'Location: \"Paris, Île-de-France, France\" and the job description is written in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': '\"Master or Ph.D in key engineering topics like computer science or Mathematics is required\"',\n",
       "     'score': 1.5}],\n",
       "   'score': 2,\n",
       "   'time': 89.27481865882874},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': \"Qualifications: 'Has a solid academic background in concepts of machine learning or deep learning or reinforcement learning.'\",\n",
       "     'score': 2},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': \"Qualifications: 'Master or Ph.D in key engineering topics like computer science or Mathematics is required'\",\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': \"Job title: 'AI Research Scientist (Paris)', location: 'Paris, Île-de-France, France', description is in English.\",\n",
       "     'score': 0.5},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'The job requires prior experience on working on ML explainability methods - LRP, SHAPE, LIME, IG, CEM etc. Your profile does not mention hands-on experience with these specific explainability methods.',\n",
       "     'score': -2}],\n",
       "   'score': 2,\n",
       "   'time': 4.935046195983887},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': '\"Has a solid academic background in concepts of machine learning or deep learning or reinforcement learning.\"',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (explicitly mentioned)',\n",
       "     'evidence': '\"Master or Ph.D in key engineering topics like computer science or Mathematics is required\"',\n",
       "     'score': 1.5},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences (central to the role)\",\n",
       "     'evidence': '\"Prior experience on working on ML explainability methods - LRP, SHAPE, LIME, IG, CEM etc.\"',\n",
       "     'score': -2},\n",
       "    {'criteria': 'The job is based in France and description is in English (job is in France and description provided in English)',\n",
       "     'evidence': '\"job title: AI Research Scientist (Paris), and location: Paris, Île-de-France, France\" (job description text is in English)',\n",
       "     'score': 0.5}],\n",
       "   'score': 2,\n",
       "   'time': 29.283622980117798},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': 'Has a solid academic background in concepts of machine learning or deep learning or reinforcement learning.',\n",
       "     'score': 2.0},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'Prior experience on working on ML explainability methods - LRP, SHAPE, LIME, IG, CEM etc.',\n",
       "     'score': -1.0},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'AI Research Scientist (Paris) and the job description is in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'Master or Ph.D in key engineering topics like computer science or Mathematics is required',\n",
       "     'score': 1.5}],\n",
       "   'score': 3,\n",
       "   'time': 23.149363040924072}},\n",
       " {'id': 3,\n",
       "  'job_description': 'Develop the first useful quantum algorithms.  \\n  \\nHaiqu is building a world-leading software platform that enables previously\\nimpossible applications on modern quantum processors. Our full-stack approach\\nhas shown industry-first results in pilots with leading quantum early\\nadopters. Backed by top investors, we offer a unique opportunity to join a\\nfast-growing team to drive theoretical and practical quantum algorithm\\ndevelopment in the early FTQC (EFTQC) era.  \\n  \\nWe seek candidates with a solid academic background in theoretical and applied\\nquantum algorithms, a demonstrated capability for independent research, and a\\npassion for testing out theoretical concepts on real quantum hardware.  \\n  \\n**Responsibilities:  \\n  \\n** As a Research Scientist in Quantum Algorithms, you will explore and develop\\nnear-to-mid-term quantum applications on NISQ and EFTQC quantum architectures.\\nYou will run and design research projects on characterization, benchmarking,\\nand resource estimation for quantum algorithms, and conceptualize and\\nprototype proof-of-principle demonstrations. To this end, you will conduct\\nboth theoretical algorithm research and run practical experiments using state-\\nof-the-art quantum software and hardware.  \\n  \\n**Requirements  \\n  \\n**\\n\\n  * PhD in physics or related field (Computer Science, Mathematics)\\n  * A research background in quantum computing, quantum algorithms, quantum information theory, or computational physics with a demonstrated publication record\\n  * A good knowledge of quantum computing, including Error Correction, with a particular emphasis on recent research in quantum algorithms. Experience in resource estimation and practical optimization of quantum algorithms is a plus, as is good familiarity with quantum compilation and Tensor Network methods\\n  * Strong programming skills (preferably in Python) and a familiarity with at least one practical quantum computing framework (e.g. Qiskit)\\n  * Experience with ML frameworks (PyTorch, TensorFlow) is a plus\\n  * Excellent communication and presentation skills  \\n  \\n\\n**Benefits  \\n  \\n** We offer competitive compensation with significant equity and benefits.\\nJoin a diverse, flexible workplace building the future of quantum computing.\\n\\nShow more  Show less\\n\\n',\n",
       "  'oracle': {'evaluation_grid': [{'criteria': \"'optimization' mentioned primarily in the context of quantum algorithms: (-4)\",\n",
       "     'evidence': '\"Experience in resource estimation and practical optimization of quantum algorithms is a plus.\"',\n",
       "     'score': -4},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences: (-2 if this domain/tool is central to the role)\",\n",
       "     'evidence': '\"A research background in quantum computing, quantum algorithms, quantum information theory, or computational physics with a demonstrated publication record\"',\n",
       "     'score': -2},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified. : (+0.5)',\n",
       "     'evidence': 'Location: Paris, Île-de-France, France; job description is written in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus) (has to be explicitly mentioned in the job description. Having experience leading research teams does not imply a PhD): (+1.5)',\n",
       "     'evidence': '\"PhD in physics or related field (Computer Science, Mathematics)\"',\n",
       "     'score': 1.5}],\n",
       "   'score': -4,\n",
       "   'time': 77.11481022834778},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': '* PhD in physics or related field (Computer Science, Mathematics)',\n",
       "     'score': 1.5},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences (central to the role)\",\n",
       "     'evidence': 'We seek candidates with a solid academic background in theoretical and applied quantum algorithms... A research background in quantum computing, quantum algorithms, quantum information theory, or computational physics with a demonstrated publication record',\n",
       "     'score': -2},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'Location: Paris, Île-de-France, France. The job description is in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': \"'optimization' mentioned primarily in the context of quantum algorithms\",\n",
       "     'evidence': 'Experience in resource estimation and practical optimization of quantum algorithms is a plus',\n",
       "     'score': -4}],\n",
       "   'score': -4,\n",
       "   'time': 6.3487629890441895},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': '\"Experience in resource estimation and practical optimization of quantum algorithms is a plus, as is good familiarity with quantum compilation and Tensor Network methods\"',\n",
       "     'score': 2},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': '\"Research background in quantum computing, quantum algorithms, quantum information theory...\" and \"familiarity with at least one practical quantum computing framework (e.g. Qiskit)\" (job title: Quantum Algrithms Researcher)',\n",
       "     'score': -2},\n",
       "    {'criteria': \"'optimization' mentioned primarily in the context of quantum algorithms\",\n",
       "     'evidence': '\"run and design research projects on characterization, benchmarking, and resource estimation for quantum algorithms\" / \"practical optimization of quantum algorithms\"',\n",
       "     'score': -4},\n",
       "    {'criteria': 'The job is based in France and the description is in English',\n",
       "     'evidence': 'Location: \"Paris, Île-de-France, France\" and the job description text is written in English (e.g. \"Haiqu is building a world-leading software platform ...\")',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (explicitly mentioned)',\n",
       "     'evidence': '\"PhD in physics or related field (Computer Science, Mathematics)\"',\n",
       "     'score': 1.5}],\n",
       "   'score': -2,\n",
       "   'time': 31.813884258270264},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [{'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'Develop the first useful quantum algorithms. / theoretical and practical quantum algorithm development / Research Scientist in Quantum Algorithms / research background in quantum computing, quantum algorithms, quantum information theory',\n",
       "     'score': -2.0},\n",
       "    {'criteria': \"'optimization' mentioned primarily in the context of quantum algorithms\",\n",
       "     'evidence': 'Experience in resource estimation and practical optimization of quantum algorithms is a plus',\n",
       "     'score': -4.0},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'location: Paris, Île-de-France, France',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'PhD in physics or related field (Computer Science, Mathematics)',\n",
       "     'score': 1.5}],\n",
       "   'score': -4,\n",
       "   'time': 15.185747861862183}},\n",
       " {'id': 4,\n",
       "  'job_description': '**About Mistral  \\n  \\n** At Mistral AI, we believe in the power of AI to simplify tasks, save time,\\nand enhance learning and creativity. Our technology is designed to integrate\\nseamlessly into daily working life.  \\n  \\nWe democratize AI through high-performance, optimized, open-source and\\ncutting-edge models, products and solutions. Our comprehensive AI platform is\\ndesigned to meet enterprise as well as personal needs. Our offerings include\\nLe Chat, La Plateforme, Mistral Code and Mistral Compute - a suite that brings\\nfrontier intelligence to end-users.  \\n  \\nWe are a dynamic, collaborative team passionate about AI and its potential to\\ntransform society. Our diverse workforce thrives in competitive environments\\nand is committed to driving innovation. Our teams are distributed between\\nFrance, USA, UK, Germany and Singapore. We are creative, low-ego and team-\\nspirited.  \\n  \\nJoin us to be part of a pioneering company shaping the future of AI. Together,\\nwe can make a meaningful impact. See more about our culture on\\nhttps://mistral.ai/careers.  \\n  \\n_Mistral AI participates in the E-Verify program  \\n  \\n_ Role Summary  \\n  \\n**About The Research Engineering Team  \\n  \\n** The team spans **Platform** (shared infra & clean code) and\\n**Embedded**(inside research squads). Engineers can move along the\\nresearch↔production spectrum as needs or interests evolve.  \\n  \\nAs a Research Engineer – ML track, you’ll build and optimise the large-scale\\nlearning systems that power our open-weight models. Working hand-in-hand with\\nResearch Scientists, you’ll either join:  \\n  \\n\\n  * Platform RE Team: Enhance the shared training framework, data pipelines and cluster tooling used by every team; or\\n  * Embedded RE Team: Sit inside a research squad (Alignment, Pre-training, Multimodal, …) and turn fresh ideas into repeatable, scalable code  \\n  \\n  \\n\\n**Location:** Paris / London (hybrid) or remote from EU/UK  \\n  \\nWhat will you do  \\n  \\n\\n  * Accelerate researchers by taking on the heavy parts of large-scale ML pipelines and building robust tools\\n  * Interface cutting-edge research with production: integrate checkpoints, streamline evaluation, and expose APIs\\n  * Conduct experiments on the latest deep-learning techniques (sparsified 70 B + runs, distributed training on thousands of GPUs)\\n  * Design, implement and benchmark ML algorithms; write clear, efficient code in Python\\n  * Deliver prototypes that become production-grade components for Le Chat and our enterprise API  \\n  \\n  \\n\\n**About You  \\n  \\n**\\n\\n  * Master’s or PhD in Computer Science (or equivalent proven track record)\\n  * 4 + years working on large-scale ML codebases\\n  * Hands-on with PyTorch, JAX or TensorFlow; comfortable with distributed training (DeepSpeed / FSDP / SLURM / K8s)\\n  * Experience in deep learning, NLP or LLMs; bonus for CUDA or data-pipeline chops\\n  * Strong software-design instincts: testing, code review, CI/CD\\n  * Self-starter, low-ego, collaborative  \\n  \\n  \\n\\n**Benefits  \\n  \\n** France  \\n  \\n💰 Competitive cash salary and equity  \\n  \\n🥕 Food: Daily lunch vouchers  \\n  \\n🥎 Sport: Monthly contribution to a Gympass subscription  \\n  \\n🚴 Transportation: Monthly contribution to a mobility pass  \\n  \\n🧑\\u200d⚕️ Health: Full health insurance for you and your family  \\n  \\n🍼 Parental: Generous parental leave policy  \\n  \\n🌎 Visa sponsorship  \\n  \\nUK  \\n  \\n💰 Competitive cash salary and equity  \\n  \\n🚑 Insurance  \\n  \\n🚴 Transportation: Reimburse office parking charges, or £90 per month for\\npublic transport  \\n  \\n🥎 Sport: £90 per month reimbursement for gym membership  \\n  \\n🥕 Meal voucher: £200 monthly allowance for meals  \\n  \\n💰 Pension plan: SmartPension (percentages are 5% Employee & 3% Employer)  \\n  \\nWe may use artificial intelligence (AI) tools to support parts of the hiring\\nprocess, such as reviewing applications, analyzing resumes, or assessing\\nresponses. These tools assist our recruitment team but do not replace human\\njudgment. Final hiring decisions are ultimately made by humans. If you would\\nlike more information about how your data is processed, please contact us.\\n\\nShow more  Show less\\n\\n',\n",
       "  'oracle': {'evaluation_grid': [{'criteria': 'Top-tier company (e.g., Google, Apple, Meta, Helsing, Mistral AI, Perplexity, OpenAI, Anthropic, Nvidia)',\n",
       "     'evidence': 'company name: Mistral AI',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Offers a full-remote option',\n",
       "     'evidence': 'Location: Paris / London (hybrid) or remote from EU/UK',\n",
       "     'score': 2},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'Description is in English; Location: Paris / London (hybrid) or remote from EU/UK',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus) (has to be explicitly mentioned in the job description. Having experience leading research teams does not imply a PhD)',\n",
       "     'evidence': 'Master’s or PhD in Computer Science (or equivalent proven track record)',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'Requires \"deep expertise\" / \"senior-level experience\" / \"mastery\" of MLOps, large-scale training, or inference optimization (beyond just \"good fundamentals\" or \"being comfortable\")',\n",
       "     'evidence': '4 + years working on large-scale ML codebases; Conduct experiments on the latest deep-learning techniques (sparsified 70 B + runs, distributed training on thousands of GPUs)',\n",
       "     'score': -1},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'Hands-on with PyTorch, JAX or TensorFlow; comfortable with distributed training (DeepSpeed / FSDP / SLURM / K8s)',\n",
       "     'score': -2}],\n",
       "   'score': 3,\n",
       "   'time': 73.00970101356506},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': \"About You: Master's or PhD in Computer Science (or equivalent proven track record)\",\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'Top-tier company',\n",
       "     'evidence': 'Company name: Mistral AI (recognized as a top-tier AI company)',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Offers a full-remote option',\n",
       "     'evidence': 'Location: Paris / London (hybrid) or remote from EU/UK',\n",
       "     'score': 2},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'Location: Paris, Île-de-France, France; job description is in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'Hands-on with PyTorch, JAX or TensorFlow; comfortable with distributed training (DeepSpeed / FSDP / SLURM / K8s). Your profile does not mention experience with JAX, DeepSpeed, FSDP, SLURM, or K8s, which are central to the role.',\n",
       "     'score': -2},\n",
       "    {'criteria': \"Requires 'deep expertise' / 'senior-level experience' / 'mastery' of MLOps, large-scale training, or inference optimization (beyond just 'good fundamentals' or 'being comfortable')\",\n",
       "     'evidence': '4+ years working on large-scale ML codebases; experience with distributed training on thousands of GPUs; strong software-design instincts: testing, code review, CI/CD.',\n",
       "     'score': -1}],\n",
       "   'score': 3,\n",
       "   'time': 4.868955135345459},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': 'Top-tier company: Mistral AI',\n",
       "     'evidence': \"Company: Mistral AI (job header and 'About Mistral' section).\",\n",
       "     'score': 2},\n",
       "    {'criteria': 'PhD mentioned as acceptable/desired',\n",
       "     'evidence': '\"Master’s or PhD in Computer Science (or equivalent proven track record)\"',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'Offers full-remote option',\n",
       "     'evidence': '\"Location: Paris / London (hybrid) or remote from EU/UK\"',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Job is based in France and the description is in English',\n",
       "     'evidence': 'Location includes Paris and the full job description is written in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires demonstrated expertise in specific tooling absent from profile (distributed-training stack)',\n",
       "     'evidence': '\"Hands-on with PyTorch, JAX or TensorFlow; comfortable with distributed training (DeepSpeed / FSDP / SLURM / K8s)\" — these specific tools (DeepSpeed, FSDP, SLURM, K8s) are not listed in your profile as hands-on experience.',\n",
       "     'score': -2}],\n",
       "   'score': 4,\n",
       "   'time': 34.40561294555664},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [{'criteria': \"'Optimization' mentioned primarily for performance/infrastructure (e.g., inference speed, cloud costs, MLOps)\",\n",
       "     'evidence': 'you’ll build and optimise the large-scale learning systems that power our open-weight models. and Conduct experiments on the latest deep-learning techniques (sparsified 70 B + runs, distributed training on thousands of GPUs).',\n",
       "     'score': -3.0},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'Location: Paris / London (hybrid) or remote from EU/UK and the entire job description being in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'Master’s or PhD in Computer Science (or equivalent proven track record)',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'Top-tier company (e.g., Google, Apple, Meta, Helsing, Mistral AI, Perplexity, OpenAI, Anthropic, Nvidia)',\n",
       "     'evidence': 'company name: Mistral AI',\n",
       "     'score': 2.0},\n",
       "    {'criteria': 'Offers a full-remote option',\n",
       "     'evidence': 'Location: Paris / London (hybrid) or remote from EU/UK',\n",
       "     'score': 2.0}],\n",
       "   'score': 3,\n",
       "   'time': 46.47428297996521}},\n",
       " {'id': 5,\n",
       "  'job_description': \"As a research scientist on our team, you will partner with research engineers,\\nworking on fundamental research problems and collaborating with Datadog’s\\nProduct and Engineering teams to help translate research advances into\\ntangible benefits for our customers.  \\n  \\nBuilding on our proven track record of AI-powered solutions (e.g., Bits AI,\\nWatchdog, and Toto), Datadog AI Research is tackling high-risk, high-reward\\nprojects grounded in real-world challenges in cloud observability and\\nsecurity.  \\n  \\nWe are currently focused on three key research areas:  \\n  \\n\\n  * Observability Foundation Models – Building state-of-the-art models for advanced forecasting, anomaly detection, and multi-modal telemetry analysis (logs, metrics, traces, etc.). These models will also provide the foundation for our agents (described below) to natively analyze telemetry data. \\n  * Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents in production environments, pushing the boundaries of multi-step planning, reasoning, and domain-specific knowledge. \\n  * Production Code Repair Agents – Developing agents and models that leverage code, logs, runtime data, and other signals to identify, fix, and even preempt performance issues and security vulnerabilities in production code.   \\n  \\n\\n**What You’ll Do:  \\n  \\n**\\n\\n  * Conduct cutting-edge research in Generative AI and Machine Learning, aiming to build specialized Foundation Models and AI Agents for observability, site reliability engineering, and code repair\\n  * Leverage large-scale distributed training infrastructure to pre-train and post-train state-of-the-art models on diverse, real-world telemetry data\\n  * Build simulated environments to facilitate on-policy agentic training and evaluation.\\n  * Lead and contribute to research publications, present findings at top-tier conferences (e.g., NeurIPS, ICLR, ICML), and help open-source key model artifacts and benchmarks\\n  * Collaborate with cross-functional teams (e.g., Product, Engineering) to integrate advanced AI capabilities – like multi-modal analysis or automated incident resolution planning – into Datadog’s product ecosystem\\n  * Stay at the forefront of LLMs, Foundation Models, and Generative AI research and engage with the external research community\\n  * Foster a culture of scientific rigor, innovation, and practical impact, e.g., by actively participating in reading groups and mentoring interns  \\n  \\n\\n**Who You Are:  \\n  \\n**\\n\\n  * You hold a PhD in Computer Science, Machine Learning, or a related field, with deep expertise in areas like generative modeling, AI agents, reinforcement learning, or natural language processing (or have equivalent experience)\\n  * You possess extensive experience in designing and implementing deep learning models and agents, and have a strong background in distributed training frameworks (e.g., DeepSpeed, Megatron-LM) and ML libraries (PyTorch, TensorFlow)\\n  * You have a proven track record of conducting impactful research in the field with publications at top-tier venues (e.g., NeurIPS, ICLR, ICML, TMLR)\\n  * You're familiar with efficient training, post-training, fine-tuning, and inference techniques for large foundation models\\n  * You excel at explaining complex models and research findings to both technical and non-technical audiences\\n  * You have strong interest in open-science and open-source contributions, including establishing rigorous benchmarks and sharing research with the community  \\n  \\n\\n**Bonus Points (any of the following):  \\n  \\n**\\n\\n  * You have a demonstrated ability to bridge cutting-edge research and real-world product applications, ideally with an emphasis on large foundation models, generative AI agents, or domain-specific LLM deployments.\\n  * You’re passionate about pushing the boundaries of AI while maintaining a strong focus on customer impact, scalability, and responsible deployment of new technologies\\n  * You have experience writing production data pipelines and applications\\n  * You have hands-on experience with GPU programming and optimization, including experience in CUDA  \\n  \\n\\n _Datadog values people from all walks of life. We understand not everyone\\nwill meet all the above qualifications on day one. That's okay. If you’re\\npassionate about AI Research and want to grow your skills, we encourage you to\\napply.  \\n  \\n_**Benefits and Growth:  \\n  \\n**\\n\\n  * Competitive global benefits\\n  * New hire stock equity (RSUs) and employee stock purchase plan (ESPP)\\n  * Opportunity to collaborate closely with colleagues across the Datadog offices in New York City and Paris\\n  * Opportunity to attend and present at conferences and meetups\\n  * Intra-departmental mentor and buddy program for in-house networking\\n  * An inclusive company culture, ability to join our Community Guilds (Datadog employee resource groups)  \\n  \\n\\n_Benefits and Growth listed above may vary based on the country of your\\nemployment and the nature of your employment with Datadog.  \\n  \\n_**About Datadog:  \\n  \\n** Datadog (NASDAQ: DDOG) is a global SaaS business, delivering a rare\\ncombination of growth and profitability. We are on a mission to break down\\nsilos and solve complexity in the cloud age by enabling digital\\ntransformation, cloud migration, and infrastructure monitoring of our\\ncustomers’ entire technology stacks. Built by engineers, for engineers,\\nDatadog is used by organizations of all sizes across a wide range of\\nindustries. Together, we champion professional development, diversity of\\nthought, innovation, and work excellence to empower continuous growth. Join\\nthe pack and become part of a collaborative, pragmatic, and thoughtful people-\\nfirst community where we solve tough problems, take smart risks, and celebrate\\none another. Learn more about #DatadogLife on Instagram, LinkedIn, and Datadog\\nLearning Center.  \\n  \\n**Equal Opportunity at Datadog:  \\n  \\n** Datadog is proud to offer equal employment opportunity to everyone\\nregardless of race, color, ancestry, religion, sex, national origin, sexual\\norientation, age, citizenship, marital status, disability, gender identity,\\nveteran status, and other characteristics protected by law. We also consider\\nqualified applicants regardless of criminal histories, consistent with legal\\nrequirements. Here are our Candidate Legal Notices for your reference.  \\n  \\nDatadog endeavors to make our Careers Page accessible to all users. If you\\nwould like to contact us regarding the accessibility of our website or need\\nassistance completing the application process, please complete this form. This\\nform is for accommodation requests only and cannot be used to inquire about\\nthe status of applications.  \\n  \\n**Privacy and AI Guidelines:  \\n  \\n** Any information you submit to Datadog as part of your application will be\\nprocessed in accordance with Datadog’s Applicant and Candidate Privacy Notice.\\nFor information on our AI policy, please visit Interviewing at Datadog AI\\nGuidelines.\\n\\nShow more  Show less\\n\\n\",\n",
       "  'oracle': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': '\"You hold a PhD ... with deep expertise in areas like generative modeling, AI agents, reinforcement learning, or natural language processing\"',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': '\"Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents in production environments, pushing the boundaries of multi-step planning, reasoning, and domain-specific knowledge.\"',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Agentic workflows are part of the job (large part of the job is dedicated to this)',\n",
       "     'evidence': '\"Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents...\" and \"Build simulated environments to facilitate on-policy agentic training and evaluation.\" and \"Production Code Repair Agents – Developing agents and models...\"',\n",
       "     'score': 3},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': '\"You possess extensive experience in designing and implementing deep learning models and agents, and have a strong background in distributed training frameworks (e.g., DeepSpeed, Megatron-LM) and ML libraries (PyTorch, TensorFlow)\"',\n",
       "     'score': -2},\n",
       "    {'criteria': 'The job is based in France and requires a good English level (description in English + France-based)',\n",
       "     'evidence': 'Job description is written in English; Location: \"Paris, Île-de-France, France\"',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires deep expertise/senior-level experience in large-scale training',\n",
       "     'evidence': '\"Leverage large-scale distributed training infrastructure to pre-train and post-train state-of-the-art models...\" and \"have a strong background in distributed training frameworks (e.g., DeepSpeed, Megatron-LM)\"',\n",
       "     'score': -1},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or equivalent experience)',\n",
       "     'evidence': '\"You hold a PhD in Computer Science, Machine Learning, or a related field... (or have equivalent experience)\"',\n",
       "     'score': 1.5}],\n",
       "   'score': 6,\n",
       "   'time': 99.96788811683655},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': \"'You hold a PhD in Computer Science, Machine Learning, or a related field, with deep expertise in areas like generative modeling, AI agents, reinforcement learning, or natural language processing (or have equivalent experience)'\",\n",
       "     'score': 2},\n",
       "    {'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': \"The job mentions 'multi-step planning, reasoning, and domain-specific knowledge' as part of the SRE Autonomous Agents, but does not explicitly mention optimization, operations research, or combinatorial optimization. 'Planning' here refers to agentic planning, not mathematical optimization. This does not meet the criterion for explicit mention of algorithmic/mathematical optimization.\",\n",
       "     'score': 0},\n",
       "    {'criteria': 'Agentic workflows (ie. langchain, tool use, prompt engineering, etc.) are part of the job',\n",
       "     'evidence': 'The job is focused on building AI agents (SRE Autonomous Agents, Production Code Repair Agents), which involves agentic workflows, but does not explicitly mention frameworks like langchain or prompt engineering. However, the core of the job is about agentic workflows in the sense of AI agents for automation and reasoning.',\n",
       "     'score': 2},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': \"The job requires 'a strong background in distributed training frameworks (e.g., DeepSpeed, Megatron-LM) and ML libraries (PyTorch, TensorFlow)'. Your profile lists PyTorch, but not distributed training frameworks like DeepSpeed or Megatron-LM. This is a secondary qualification, not central to the job title or primary responsibility.\",\n",
       "     'score': -1},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'Job location: Paris, Île-de-France, France. Description is in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': \"'You hold a PhD in Computer Science, Machine Learning, or a related field, with deep expertise in areas like generative modeling, AI agents, reinforcement learning, or natural language processing (or have equivalent experience)'\",\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'Datadog is a large, global SaaS company (NASDAQ: DDOG), with more than 150 employees.',\n",
       "     'score': -1}],\n",
       "   'score': 4,\n",
       "   'time': 13.541990041732788},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill (+2)',\n",
       "     'evidence': '\"Who You Are: ... deep expertise in areas like generative modeling, AI agents, reinforcement learning, or natural language processing\"',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Agentic workflows (tool-augmented agents, prompt/tool use) are part of the job (+2) + large part of the job (+1)',\n",
       "     'evidence': '\"SRE Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents in production environments\" and multiple references to \"agents\" as a key research area and product integration',\n",
       "     'score': 3},\n",
       "    {'criteria': 'Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile (central: distributed training frameworks e.g., DeepSpeed, Megatron-LM) (-2)',\n",
       "     'evidence': '\"You possess extensive experience ... and have a strong background in distributed training frameworks (e.g., DeepSpeed, Megatron-LM) and ML libraries (PyTorch, TensorFlow)\"',\n",
       "     'score': -2},\n",
       "    {'criteria': \"'Optimization' mentioned primarily for performance/inference (model training/inference efficiency) (-3)\",\n",
       "     'evidence': '\"You\\'re familiar with efficient training, post-training, fine-tuning, and inference techniques for large foundation models\" and \"Leverage large-scale distributed training infrastructure to pre-train and post-train state-of-the-art models\"',\n",
       "     'score': -3},\n",
       "    {'criteria': 'Job is based in France and description is in English (+0.5)',\n",
       "     'evidence': 'Location: \"Paris, Île-de-France, France\" and the full job description is written in English',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires \"deep expertise\" / \"senior-level experience\" / \"mastery\" of MLOps, large-scale training, or inference optimization (penalty -1)',\n",
       "     'evidence': '\"Leverage large-scale distributed training infrastructure ...\" and \"extensive experience in designing and implementing deep learning models ... strong background in distributed training frameworks\" (language indicating senior/deep expertise)',\n",
       "     'score': -1},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (explicitly mentioned) (+1.5)',\n",
       "     'evidence': '\"You hold a PhD in Computer Science, Machine Learning, or a related field, with deep expertise ... (or have equivalent experience)\"',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'Top-tier company (Datadog) (+2)',\n",
       "     'evidence': '\"Datadog (NASDAQ: DDOG) is a global SaaS business...\" (well-known, large public tech company)',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Company has more than 150 employees (-1)',\n",
       "     'evidence': 'Datadog described as a global SaaS business (Datadog is a large public company with well over 150 employees)',\n",
       "     'score': -1}],\n",
       "   'score': 2.0,\n",
       "   'time': 43.57052493095398},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': 'You hold a PhD in Computer Science, Machine Learning, or a related field, with deep expertise in areas like generative modeling, AI agents, reinforcement learning, or natural language processing (or have equivalent experience)',\n",
       "     'score': 2.0},\n",
       "    {'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': 'Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents in production environments, pushing the boundaries of multi-step planning, reasoning, and domain-specific knowledge. and Collaborate with cross-functional teams (e.g., Product, Engineering) to integrate advanced AI capabilities – like multi-modal analysis or automated incident resolution planning – into Datadog’s product ecosystem',\n",
       "     'score': 2.0},\n",
       "    {'criteria': 'Agentic workflows (ie. langchain, tool use, prompt engineering, etc.) are part of the job',\n",
       "     'evidence': 'Observability Foundation Models – Building state-of-the-art models for advanced forecasting, anomaly detection, and multi-modal telemetry analysis (logs, metrics, traces, etc.). These models will also provide the foundation for our agents (described below) to natively analyze telemetry data., Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents in production environments, pushing the boundaries of multi-step planning, reasoning, and domain-specific knowledge., Production Code Repair Agents – Developing agents and models that leverage code, logs, runtime data, and other signals to identify, fix, and even preempt performance issues and security vulnerabilities in production code., Conduct cutting-edge research in Generative AI and Machine Learning, aiming to build specialized Foundation Models and AI Agents for observability, site reliability engineering, and code repair',\n",
       "     'score': 3.0},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'location: Paris, Île-de-France, France and the job description is in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires \"deep expertise\" / \"senior-level experience\" / \"mastery\" of MLOps, large-scale training, or inference optimization (beyond just \"good fundamentals\" or \"being comfortable\")',\n",
       "     'evidence': 'You possess extensive experience in designing and implementing deep learning models and agents, and have a strong background in distributed training frameworks (e.g., DeepSpeed, Megatron-LM) and ML libraries (PyTorch, TensorFlow)',\n",
       "     'score': -1.0},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'You hold a PhD in Computer Science, Machine Learning, or a related field, with deep expertise in areas like generative modeling, AI agents, reinforcement learning, or natural language processing (or have equivalent experience)',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'Top-tier company',\n",
       "     'evidence': 'Based on prior knowledge of Datadog as a leading SaaS company in cloud observability and security with significant AI investment.',\n",
       "     'score': 2.0},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'Datadog (NASDAQ: DDOG) is a global SaaS business...',\n",
       "     'score': -1.0}],\n",
       "   'score': 9,\n",
       "   'time': 30.360820055007935}},\n",
       " {'id': 6,\n",
       "  'job_description': \"Au sein du Département _Corporate Research & Science_, vous intégrerez\\nl’équipe _AI for Manufacturing_ _World Twin_ qui s’intéresse à des\\nproblématiques liées à l’Intelligence Artificielle dans des environnements\\nindustriels mixant le réel et le virtuel, à l’image des jumeaux virtuels.  \\n  \\nQuand la donnée est rare et confidentielle, les univers virtuels peuvent être\\nun atout majeur pour la modélisation mathématique et l’apprentissage\\nstatistique afin de capitaliser et automatiser le savoir-faire industriel.  \\n  \\nSi les techniques d’apprentissage statistique ont prouvé leur efficacité sur\\ndes données réelles collectées par des capteurs physiques (caméras, appareils\\nphoto, etc.), leur déploiement dans des environnements 3D mixant le réel et le\\nvirtuel, reste un défi majeur. Ce domaine soulève encore de nombreuses\\nquestions, en particulier en ce qui concerne la robustesse, la capacité de\\ngénéralisation et l’adaptation aux contraintes industrielles. En nous appuyant\\nsur les avancées des _Vision-Language Models_ et sur de nouveaux paradigmes\\nd’apprentissages, nous cherchons à concevoir des solutions innovantes,\\ncapables de s’adapter à des contextes industriels variés et en constante\\névolution.  \\n  \\n**Vos missions:  \\n  \\n**\\n\\n  * Réaliser des études et référentiels sur des technologies clés autour des jumeaux virtuels et différents paradigmes d’apprentissages au service de l’industrie.\\n  * Développer des prototypes et des composants technologiques innovants.\\n  * Démontrer l’applicabilité et l’usage des composants dans des scénarii adaptés aux besoins de la R&D.\\n  * Au travers des études et prototypes réalisés, il/elle élaborera des recommandations technologiques et il/elle accompagnera leur transfert vers les équipes de développement produit.\\n  * Participation à l’écriture de brevets dans le cadre de ses travaux.  \\n  \\n\\n**Vos qualifications :  \\n  \\n**\\n\\n  * Doctorat (PhD) en Machine Learning / mathématique appliquée ou formation d’ingénieur ou universitaire (de type Bac+5) avec une majeure en Machine Learning / Data Science, vous avez minimum 4 ans d'expériene.\\n  * Vous disposez de solides connaissances en mathématiques appliquées, algorithmique, modélisation 3D et apprentissage statistique.\\n  * Vous disposez de compétences en programmation en python et C++, Frameworks de Deep Learning (e.g. Tensorflow, Pytorch) et en rédaction de document et de présentation.\\n  * Une expérience professionnelle avec les Vision Language Models et/ou le Reinforcement Learning est un véritable plus.\\n  * Vous avez un bon niveau d’anglais\\n  * Outre les compétences techniques, vous êtes doté(e) d’un esprit d’analyse et de synthèse, d’une bonne aptitude à la restitution d’information et à la communication.  \\n  \\n\\nInclusion statement  \\n  \\nAs a game-changer in sustainable technology and innovation, Dassault Systèmes\\nis striving to build more inclusive and diverse teams across the globe. We\\nbelieve that our people are our number one asset and we want all employees to\\nfeel empowered to bring their whole selves to work every day. It is our goal\\nthat our people feel a sense of pride and a passion for belonging. As a\\ncompany leading change, it’s our responsibility to foster opportunities for\\nall people to participate in a harmonized Workforce of the Future.\\n\\nShow more  Show less\\n\\n\",\n",
       "  'oracle': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': '\"Une expérience professionnelle avec les Vision Language Models et/ou le Reinforcement Learning est un véritable plus.\"',\n",
       "     'score': 2},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': '\"Vous disposez de compétences en programmation en python et C++\" (C++ non listé comme compétence dans mon profil récent)',\n",
       "     'score': -2},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': '\"Vous disposez de solides connaissances en ... modélisation 3D\" (modélisation 3D absente de mon profil)',\n",
       "     'score': -2},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': '\"Doctorat (PhD) en Machine Learning / mathématique appliquée ou formation d’ingénieur ...\"',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'Dassault Systèmes est une multinationale bien établie (connaissance a priori: >20 000 employés)',\n",
       "     'score': -1}],\n",
       "   'score': -1.5,\n",
       "   'time': 98.47651195526123},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': '\"Une expérience professionnelle avec les Vision Language Models et/ou le Reinforcement Learning est un véritable plus.\" RL is not a key requirement, but is explicitly mentioned as a plus. According to the criteria, this still counts as +2.',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': '\"solides connaissances en mathématiques appliquées, algorithmique, modélisation 3D et apprentissage statistique.\" This covers mathematical and algorithmic skills, but does not explicitly mention optimization or operations research. The mention of \\'algorithmique\\' and \\'mathématiques appliquées\\' is close, but not explicit. According to the criteria, explicit mention is required. Therefore, this should NOT be included.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': '\"Vous avez un bon niveau d’anglais\" and location is \"Vélizy-Villacoublay, Île-de-France, France\".',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': '\"Doctorat (PhD) en Machine Learning / mathématique appliquée ou formation d’ingénieur ou universitaire (de type Bac+5) avec une majeure en Machine Learning / Data Science, vous avez minimum 4 ans d\\'expériene.\" PhD is explicitly mentioned as a possible requirement.',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'Dassault Systèmes is a large multinational company with thousands of employees.',\n",
       "     'score': -1}],\n",
       "   'score': 3,\n",
       "   'time': 10.802553176879883},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': '\"Une expérience professionnelle avec les Vision Language Models et/ou le Reinforcement Learning est un véritable plus.\"',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': '\"Vous disposez de solides connaissances en mathématiques appliquées, algorithmique, modélisation 3D et apprentissage statistique.\"',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': '\"Doctorat (PhD) en Machine Learning / mathématique appliquée ou formation d’ingénieur ou universitaire (de type Bac+5) avec une majeure en Machine Learning / Data Science, vous avez minimum 4 ans d\\'expériene.\"',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': '\"company name: Dassault Systèmes\" (Dassault Systèmes is a large established company with well over 150 employees)',\n",
       "     'score': -1}],\n",
       "   'score': 4.5,\n",
       "   'time': 32.66773009300232},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': 'Une expérience professionnelle avec les Vision Language Models et/ou le Reinforcement Learning est un véritable plus.',\n",
       "     'score': 2.0},\n",
       "    {'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': 'solides connaissances en mathématiques appliquées, algorithmique, modélisation 3D et apprentissage statistique.',\n",
       "     'score': 2.0},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'Vélizy-Villacoublay, Île-de-France, France and Vous avez un bon niveau d’anglais',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'Doctorat (PhD) en Machine Learning / mathématique appliquée ou formation d’ingénieur ou universitaire (de type Bac+5) avec une majeure en Machine Learning / Data Science',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'Dassault Systèmes is a large company with more than 150 employees.',\n",
       "     'score': -1.0}],\n",
       "   'score': 5,\n",
       "   'time': 24.28971290588379}},\n",
       " {'id': 7,\n",
       "  'job_description': '**Job Description  \\n  \\n** The Road Safety AI team is seeking a motivated candidate, with a solid\\nbackground in Software Development and AI for Computer Vision, to work on the\\ndevelopment and improvement of algorithms to efficiently address Road Safety\\nbusiness or customer needs.  \\n  \\nOur studies cover all aspects of scientific research, from exploration of the\\nState of The Art (SOTA), data collection cleaning and annotation, algorithms\\ndesign and implementation, to the publication of research papers or patents.\\nNot just that, our algorithms are deployed in IDEMIA road safety equipment\\nused to detect traffic law violations and contribute to making Vision Zero\\nreality.  \\n  \\nSome of the studies we work on: vehicle segmentation and tracking, monocular\\n3D vision, phone usage and seatbelt violations’ detection, etc.  \\n  \\n**Key Missions  \\n  \\n**\\n\\n  * Designs, implements, improves and optimizes statistically robust, efficient, secure and scalable algorithms while respecting platforms’ constraints\\n  * Develops test strategies and methodologies to evaluate new or competing algorithms\\n  * Presents algorithms and results for internal community challenges\\n  * Stays up to date on latest state-of-the-art research and algorithm development methodologies\\n  * Provides support to solution design, implementation, customization and operations\\n  * Closes proactively the loop with operational data/logs to ensure the algorithms are fully effective in operations\\n  * Coaches less experienced researchers (depending on seniority)  \\n  \\n\\n**Profile Description  \\n  \\n**\\n\\n  * Graduate from an engineering school or holder of a Master’s degree (M2) in Artificial Intelligence, Computer Vision, or a related field\\n  * PhD in Computer Vision or AI is a plus\\n  * 1 to 5 years of experience in a similar role in R&D or product development\\n  * Strong understanding of Deep Learning and Computer Vision fundamentals\\n  * Proven involvement in at least three personal or professional projects in Computer Vision or Natural Language Processing (NLP)\\n  * Experience in Object Detection and Tracking is a plus\\n  * Proficient in Python and the PyTorch framework\\n  * Strong background in software development and data analysis\\n  * Fluent in English (able to read scientific papers and present technical work)\\n  * Proficient in French (written and spoken)\\n  * Curious, proactive, and autonomous\\n  * Results-driven with good teamwork and knowledge-sharing skills\\n\\nShow more  Show less\\n\\n',\n",
       "  'oracle': {'evaluation_grid': [{'criteria': 'The job is based in France and requires a good English level',\n",
       "     'evidence': 'Location: Courbevoie, Île-de-France, France; \"Fluent in English (able to read scientific papers and present technical work)\"',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or is a plus)',\n",
       "     'evidence': '\"PhD in Computer Vision or AI is a plus\"',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'Company: IDEMIA (large multinational; >150 employees)',\n",
       "     'score': -1}],\n",
       "   'score': 1,\n",
       "   'time': 46.229532957077026},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': \"The job description mentions 'optimizes statistically robust, efficient, secure and scalable algorithms', but this refers to algorithmic efficiency and robustness, not mathematical/operations research optimization. There is no explicit mention of mathematical optimization, operations research, or combinatorial optimization. This criterion is NOT met.\",\n",
       "     'score': 0},\n",
       "    {'criteria': 'Requires a programming language I am not familiar with, AND does not mention Python',\n",
       "     'evidence': 'The job explicitly requires Python and PyTorch, both of which you are familiar with. This criterion is NOT met (no penalty or bonus).',\n",
       "     'score': 0},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'The job is located in Courbevoie, Île-de-France, France, the description is in English, and it requires fluency in both English and French.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': \"The job description states: 'PhD in Computer Vision or AI is a plus.'\",\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'IDEMIA is a large company with more than 150 employees.',\n",
       "     'score': -1}],\n",
       "   'score': 1,\n",
       "   'time': 16.286128044128418},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': 'Requires a PhD in a related field (explicitly mentioned as a plus) (+1.5)',\n",
       "     'evidence': '\"PhD in Computer Vision or AI is a plus\" (Profile Description)',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'The job is based in France and requires a good English level (+0.5)',\n",
       "     'evidence': 'Location: \"Courbevoie, Île-de-France, France\" (job location) and \"Fluent in English (able to read scientific papers and present technical work)\" (Profile Description)',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Company appears to be a large established firm (more than 150 employees) (-1)',\n",
       "     'evidence': 'Company name: \"IDEMIA\" (company listed for the job)',\n",
       "     'score': -1}],\n",
       "   'score': 1.0,\n",
       "   'time': 30.74977397918701},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [{'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'location: Courbevoie, Île-de-France, France and Fluent in English (able to read scientific papers and present technical work)',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'PhD in Computer Vision or AI is a plus',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'IDEMIA is a large multinational company.',\n",
       "     'score': -1.0}],\n",
       "   'score': 1,\n",
       "   'time': 30.37668776512146}},\n",
       " {'id': 8,\n",
       "  'job_description': \"As a research engineer on our team, you will partner with research scientists\\nto turn research ideas into working systems; building the data, tooling, and\\ninfrastructure that enable rapid iteration, trustworthy evaluation, and a\\nsmooth path from prototype to production.  \\n  \\nBuilding on our proven track record of AI-powered solutions (e.g., Bits AI,\\nWatchdog, and Toto), Datadog AI Research is tackling high-risk, high-reward\\nprojects grounded in real-world challenges in cloud observability and\\nsecurity.  \\n  \\nWe are currently focused on three key research areas:  \\n  \\n\\n  * Observability Foundation Models – Building state-of-the-art models for advanced forecasting, anomaly detection, and multi-modal telemetry analysis (logs, metrics, traces, etc.). These models will also provide the foundation for our agents (described below) to natively analyze telemetry data. \\n  * Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents in production environments, pushing the boundaries of multi-step planning, reasoning, and domain-specific knowledge. \\n  * Production Code Repair Agents – Developing agents and models that leverage code, logs, runtime data, and other signals to identify, fix, and even preempt performance issues and security vulnerabilities in production code.   \\n  \\n\\n**What You’ll Do:  \\n  \\n**\\n\\n  * Build and operate datasets, training and evaluation pipelines, benchmarks, and internal tooling \\n  * Implement models, run experiments at scale, and profile for reliability, performance, and cost \\n  * Orchestrate distributed training and distributed RL with Ray, including scheduling, scaling, and failure recovery \\n  * Make the research stack observable, reproducible, and easier to use \\n  * Establish rigorous automated benchmarks and regression tests for forecasting, anomaly detection, multi-modal analysis, agents, and code repair tasks \\n  * Collaborate with Research Scientists, Product, and Engineering to integrate advanced AI capabilities into Datadog’s product ecosystem and to harden prototypes into reliable services \\n  * Contribute high-quality code, documentation, and open-source artifacts that enable the community and internal teams to reproduce, extend, and evaluate results   \\n  \\n\\n**Who You Are:  \\n  \\n**\\n\\n  * You have strong software engineering skills with experience in domains such as observability, SRE, or security \\n  * You have depth in distributed computing and ML systems for training and inference at scale; experience with Ray, Slurm, or similar frameworks is a plus \\n  * You are proficient in Python, familiar with a systems language (e.g., Rust, C++, or Go), and you are comfortable with modern cloud and data infrastructure \\n  * You have practical experience implementing and operating ML training and inference systems (e.g., PyTorch or JAX), including containerization, orchestration, and GPU acceleration \\n  * You are familiar with efficient training, fine-tuning, and inference techniques for large foundation models \\n  * You can explain design and performance trade-offs clearly to both technical and non-technical audiences \\n  * You have a strong interest in open-science and open-source contributions, including establishing rigorous benchmarks and sharing artifacts with the community   \\n  \\n\\n**Bonus Points:  \\n  \\n**\\n\\n  * You have a demonstrated ability to bridge cutting-edge research prototypes and real-world product applications, ideally with large foundation models, generative AI agents, or domain-specific LLM deployments \\n  * You are passionate about pushing the boundaries of AI while maintaining a strong focus on customer impact, scalability, and responsible deployment of new technologies \\n  * You have hands-on experience with GPU programming and optimization, including experience in CUDA \\n  * You have experience writing production data pipelines and applications \\n  * You have experience supporting or contributing to research publications   \\n  \\n\\n_Datadog values people from all walks of life. We understand not everyone will\\nmeet all the above qualifications on day one. That's okay. If you’re\\npassionate about AI Research and want to grow your skills, we encourage you to\\napply.  \\n  \\n_**Benefits and Growth:  \\n  \\n**\\n\\n  * Competitive global benefits \\n  * New hire stock equity (RSUs) and employee stock purchase plan (ESPP) \\n  * Opportunity to collaborate closely with colleagues across the Datadog offices in New York City and Paris \\n  * Opportunity to attend and present at conferences and meetups \\n  * Intra-departmental mentor and buddy program for in-house networking \\n  * An inclusive company culture, ability to join our Community Guilds (Datadog employee resource groups)   \\n  \\n\\n_Benefits and Growth listed above may vary based on the country of your\\nemployment and the nature of your employment with Datadog.  \\n  \\n_**About Datadog:  \\n  \\n** Datadog (NASDAQ: DDOG) is a global SaaS business, delivering a rare\\ncombination of growth and profitability. We are on a mission to break down\\nsilos and solve complexity in the cloud age by enabling digital\\ntransformation, cloud migration, and infrastructure monitoring of our\\ncustomers’ entire technology stacks. Built by engineers, for engineers,\\nDatadog is used by organizations of all sizes across a wide range of\\nindustries. Together, we champion professional development, diversity of\\nthought, innovation, and work excellence to empower continuous growth. Join\\nthe pack and become part of a collaborative, pragmatic, and thoughtful people-\\nfirst community where we solve tough problems, take smart risks, and celebrate\\none another. Learn more about #DatadogLife on Instagram, LinkedIn, and Datadog\\nLearning Center.  \\n  \\n**Equal Opportunity at Datadog:  \\n  \\n** Datadog is proud to offer equal employment opportunity to everyone\\nregardless of race, color, ancestry, religion, sex, national origin, sexual\\norientation, age, citizenship, marital status, disability, gender identity,\\nveteran status, and other characteristics protected by law. We also consider\\nqualified applicants regardless of criminal histories, consistent with legal\\nrequirements. Here are our Candidate Legal Notices for your reference.  \\n  \\nDatadog endeavors to make our Careers Page accessible to all users. If you\\nwould like to contact us regarding the accessibility of our website or need\\nassistance completing the application process, please complete this form. This\\nform is for accommodation requests only and cannot be used to inquire about\\nthe status of applications.  \\n  \\n**Privacy and AI Guidelines:  \\n  \\n** Any information you submit to Datadog as part of your application will be\\nprocessed in accordance with Datadog’s Applicant and Candidate Privacy Notice.\\nFor information on our AI policy, please visit Interviewing at Datadog AI\\nGuidelines.\\n\\nShow more  Show less\\n\\n\",\n",
       "  'oracle': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': '“Orchestrate distributed training and distributed RL with Ray, including scheduling, scaling, and failure recovery”',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Agentic workflows are part of the job, and a large part is dedicated to this',\n",
       "     'evidence': 'Research areas include “Site Reliability Engineering (SRE) Autonomous Agents” and “Production Code Repair Agents”; plus “Establish rigorous automated benchmarks and regression tests for … agents, and code repair tasks.”',\n",
       "     'score': 3},\n",
       "    {'criteria': 'Requires demonstrated expertise in a specific technical domain or toolset absent from my profile',\n",
       "     'evidence': '“Orchestrate distributed training and distributed RL with Ray …” and “experience with Ray, Slurm, or similar frameworks is a plus.”',\n",
       "     'score': -2},\n",
       "    {'criteria': 'More focused on infrastructure (databases, cloud, Docker) than on algorithms',\n",
       "     'evidence': '“Build and operate datasets, training and evaluation pipelines, benchmarks, and internal tooling”; “Make the research stack observable, reproducible, and easier to use”; “Implement models, run experiments at scale, and profile for reliability, performance, and cost.”',\n",
       "     'score': -3},\n",
       "    {'criteria': 'The job is based in France and the description is in English',\n",
       "     'evidence': 'Location: Paris, Île-de-France, France; job description is written in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires deep expertise in large-scale ML systems/training/inference',\n",
       "     'evidence': '“You have depth in distributed computing and ML systems for training and inference at scale”',\n",
       "     'score': -1},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'Datadog is a public company: “Datadog (NASDAQ: DDOG) is a global SaaS business,” indicating a large organization well above 150 employees.',\n",
       "     'score': -1}],\n",
       "   'score': -1.5,\n",
       "   'time': 107.80760669708252},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': \"The job description states: 'Orchestrate distributed training and distributed RL with Ray, including scheduling, scaling, and failure recovery.'\",\n",
       "     'score': 2},\n",
       "    {'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': 'The job description does not mention algorithmic/mathematical optimization, operations research, or related terms.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Agentic workflows (ie. langchain, tool use, prompt engineering, etc.) are part of the job',\n",
       "     'evidence': \"The job description includes: 'Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents in production environments...' and 'Production Code Repair Agents – Developing agents and models...' This is about building AI agents, but does not mention agentic workflow frameworks (langchain, tool use, prompt engineering) explicitly. However, the focus on agents is strong, so +2 is justified.\",\n",
       "     'score': 2},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'The job requires proficiency in Python and familiarity with a systems language (e.g., Rust, C++, or Go). The profile mentions some exposure to C++ and Java, but not Rust or Go. However, Python is the main language, and the systems language is not central. No penalty.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Requires a programming language I am not familiar with, AND does not mention Python',\n",
       "     'evidence': 'The job requires Python as the main language, which is present in the profile. No penalty.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'More focused on infrastructure (databases, cloud, Docker) than on algorithms',\n",
       "     'evidence': 'The job description balances infrastructure (datasets, pipelines, distributed training) and algorithms (model implementation, research, agents). Not more focused on infrastructure than algorithms. No penalty.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Vague description of actual tasks for a data scientist/engineer job',\n",
       "     'evidence': 'The job description is detailed and specific about tasks and responsibilities. No penalty.',\n",
       "     'score': 0},\n",
       "    {'criteria': \"'Optimization' mentioned primarily for performance/infrastructure (e.g., inference speed, cloud costs, MLOps)\",\n",
       "     'evidence': 'Optimization is mentioned in the context of reliability, performance, and cost, but not as the primary focus. The main focus is on research and building agents/models. No penalty.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'Location: Paris, Île-de-France, France. The job description is in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'More managerial than technical role',\n",
       "     'evidence': 'The role is technical, not managerial. No penalty.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Involves leading a team of highly qualified/experienced people (junior excluded)',\n",
       "     'evidence': 'No mention of team leadership responsibilities. No penalty.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Involves coaching world-class scientists',\n",
       "     'evidence': 'No mention of coaching world-class scientists. No penalty.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Top-tier company (e.g., Google, Apple, Meta, Helsing, Mistral AI, Perplexity, OpenAI, Anthropic, Nvidia)',\n",
       "     'evidence': 'Datadog is not considered a top-tier AI research company by the provided definition.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'Datadog is a global SaaS business, publicly traded (NASDAQ: DDOG), and has more than 150 employees.',\n",
       "     'score': -1},\n",
       "    {'criteria': 'Offers a full-remote option',\n",
       "     'evidence': 'No mention of a full-remote option in the job description.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Consulting job for a standard/low-tier consulting firm',\n",
       "     'evidence': 'Not a consulting job.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'In the defense sector',\n",
       "     'evidence': 'Not in the defense sector.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'In the robotics sector',\n",
       "     'evidence': 'Not in the robotics sector.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'If not french, requires security clearance',\n",
       "     'evidence': 'No mention of security clearance.',\n",
       "     'score': 0},\n",
       "    {'criteria': \"Requires 'deep expertise' / 'senior-level experience' / 'mastery' of MLOps, large-scale training, or inference optimization (beyond just 'good fundamentals' or 'being comfortable')\",\n",
       "     'evidence': \"The job requires 'depth in distributed computing and ML systems for training and inference at scale', but does not use terms like 'deep expertise', 'senior-level', or 'mastery'. No penalty.\",\n",
       "     'score': 0},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'The job description does not explicitly mention a PhD as a requirement or a plus.',\n",
       "     'score': 0}],\n",
       "   'score': 3.5,\n",
       "   'time': 13.84049916267395},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill (+2)',\n",
       "     'evidence': '\"Orchestrate distributed training and distributed RL with Ray, including scheduling, scaling, and failure recovery\"',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Agentic workflows / agents are part of the job (+2), and a large part of the job is dedicated to this (+1)',\n",
       "     'evidence': '\"Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents...\"; \"Production Code Repair Agents – Developing agents and models...\"; \"These models will also provide the foundation for our agents...\"',\n",
       "     'score': 3},\n",
       "    {'criteria': 'Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile (central to the role) (-2)',\n",
       "     'evidence': '\"Orchestrate distributed training and distributed RL with Ray, including scheduling, scaling, and failure recovery\"  (no Ray/Slurm experience listed in the provided profile)',\n",
       "     'score': -2},\n",
       "    {'criteria': 'The job is based in France and the description is in English (+0.5)',\n",
       "     'evidence': 'Location: \"Paris, Île-de-France, France\"; full job description written in English',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires \"depth\" / senior-level experience in distributed ML systems / large-scale training or inference (penalty -1)',\n",
       "     'evidence': '\"You have depth in distributed computing and ML systems for training and inference at scale; experience with Ray, Slurm, or similar frameworks is a plus\"; \"You are proficient in Python... comfortable with modern cloud and data infrastructure\"',\n",
       "     'score': -1},\n",
       "    {'criteria': 'Top-tier company (prior knowledge) (+2)',\n",
       "     'evidence': '\"Datadog (NASDAQ: DDOG) is a global SaaS business\" (Datadog is a well-known, large, publicly-traded tech company)',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Company larger than 150 employees (penalty -1)',\n",
       "     'evidence': '\"Datadog (NASDAQ: DDOG) is a global SaaS business... Datadog is used by organizations of all sizes across a wide range of industries.\" (implies a large, established company)',\n",
       "     'score': -1}],\n",
       "   'score': 3.5,\n",
       "   'time': 38.45830798149109},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': 'Orchestrate distributed training and distributed RL with Ray, including scheduling, scaling, and failure recovery',\n",
       "     'score': 2.0},\n",
       "    {'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': 'Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents in production environments, pushing the boundaries of multi-step planning, reasoning, and domain-specific knowledge.',\n",
       "     'score': 2.0},\n",
       "    {'criteria': 'Agentic workflows (ie. langchain, tool use, prompt engineering, etc.) are part of the job',\n",
       "     'evidence': 'Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents in production environments, pushing the boundaries of multi-step planning, reasoning, and domain-specific knowledge. ; Production Code Repair Agents – Developing agents and models that leverage code, logs, runtime data, and other signals to identify, fix, and even preempt performance issues and security vulnerabilities in production code. ; Observability Foundation Models – These models will also provide the foundation for our agents (described below) to natively analyze telemetry data.',\n",
       "     'score': 3.0},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'You have strong software engineering skills with experience in domains such as observability, SRE, or security',\n",
       "     'score': -2.0},\n",
       "    {'criteria': 'More focused on infrastructure (databases, cloud, Docker) than on algorithms',\n",
       "     'evidence': 'Build and operate datasets, training and evaluation pipelines, benchmarks, and internal tooling ; Orchestrate distributed training and distributed RL with Ray, including scheduling, scaling, and failure recovery ; Make the research stack observable, reproducible, and easier to use ; comfortable with modern cloud and data infrastructure ; practical experience implementing and operating ML training and inference systems (e.g., PyTorch or JAX), including containerization, orchestration, and GPU acceleration',\n",
       "     'score': -3.0},\n",
       "    {'criteria': \"'Optimization' mentioned primarily for performance/infrastructure (e.g., inference speed, cloud costs, MLOps)\",\n",
       "     'evidence': 'Implement models, run experiments at scale, and profile for reliability, performance, and cost ; GPU programming and optimization, including experience in CUDA',\n",
       "     'score': -3.0},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'job title: AI Research Engineer - Datadog AI Research (DAIR), and location: Paris, Île-de-France, France',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires \"deep expertise\" / \"senior-level experience\" / \"mastery\" of MLOps, large-scale training, or inference optimization (beyond just \"good fundamentals\" or \"being comfortable\")',\n",
       "     'evidence': 'You have depth in distributed computing and ML systems for training and inference at scale ; You have practical experience implementing and operating ML training and inference systems (e.g., PyTorch or JAX), including containerization, orchestration, and GPU acceleration ; You are familiar with efficient training, fine-tuning, and inference techniques for large foundation models',\n",
       "     'score': -1.0},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'Datadog (NASDAQ: DDOG) is a global SaaS business, delivering a rare combination of growth and profitability.',\n",
       "     'score': -1.0}],\n",
       "   'score': -2.5,\n",
       "   'time': 38.28434896469116}},\n",
       " {'id': 9,\n",
       "  'job_description': \"**Job Description  \\n  \\n****Purpose  \\n  \\n** The Road Safety AI team is seeking a motivated candidate, with a solid\\nbackground in Software Development and AI for Computer Vision, to work on the\\ndevelopment and improvement of algorithms to efficiently address Road Safety\\nbusiness or customer needs.  \\n  \\nOur studies cover all aspects of scientific research, from exploration of the\\nState of The Art (SOTA), data collection cleaning and annotation, algorithms\\ndesign and implementation, to the publication of research papers or patents.\\nNot just that, our algorithms are deployed in IDEMIA road safety equipment\\nused to detect traffic law violations and contribute to making Vision Zero\\nreality.  \\n  \\nSome of the studies we work on: vehicle segmentation and tracking, monocular\\n3D vision, phone usage and seatbelt violations’ detection, etc.  \\n  \\n**Key Missions  \\n  \\n**\\n\\n  * Designs, implements, improves and optimizes statistically robust, efficient, secure and scalable algorithms while respecting platforms’ constraints\\n  * Develops test strategies and methodologies to evaluate new or competing algorithms\\n  * Presents algorithms and results for internal community challenges\\n  * Stays up to date on latest state-of-the-art research and algorithm development methodologies\\n  * Provides support to solution design, implementation, customization and operations\\n  * Closes proactively the loop with operational data/logs to ensure the algorithms are fully effective in operations\\n  * Coaches less experienced researchers (depending on seniority)  \\n  \\n\\n**Profile Description  \\n  \\n****Education** : Engineering School or a Master's (M2) graduate who checks\\nthe following requirements (technical and soft skills). PhD in Computer Vision\\nor AI is a plus.  \\n  \\n**Required Technical Skills**  \\n  \\n\\n  * Strong knowledge of Deep Learning and Computer Vision.\\n    * At least three [personal or professional] projects/experiences in the field of Computer Vision or NLP.\\n    * Experience in Object Detection and Tracking is a plus.\\n  * Proficiency in Python and PyTorch.\\n  * Solid training in data analysis and software development.  \\n\\n**Soft Skills & Languages  \\n  \\n**\\n\\n  * Proficient in English (e.g. reading scientific articles, presenting work)\\n  * Proficient in French\\n  * Curious, proactive, and autonomous\\n  * Results-oriented  \\n  \\n\\n**Seniority level:** A first experience (1-5 years) in a similar position is a\\nplus.\\n\\nShow more  Show less\\n\\n\",\n",
       "  'oracle': {'evaluation_grid': [{'criteria': 'The job is based in France and requires a good English level',\n",
       "     'evidence': 'Location: Osny, Île-de-France, France; and \"Proficient in English (e.g. reading scientific articles, presenting work)\" (description in English)',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': '\"PhD in Computer Vision or AI is a plus.\"',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'IDEMIA is a large multinational with >15,000 employees (public information).',\n",
       "     'score': -1}],\n",
       "   'score': 1,\n",
       "   'time': 53.70189118385315},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'The job requires at least three [personal or professional] projects/experiences in Computer Vision or NLP. Your profile lists one significant personal project in Computer Vision, but not three distinct projects/experiences. However, you do have a strong ML/AI background and a PhD.',\n",
       "     'score': -1},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'PhD in Computer Vision or AI is a plus. You have a PhD in a closely related field (AI, RL, optimization).',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'Job is located in Osny, Île-de-France, France. The description is in English and requires proficiency in English and French.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'IDEMIA is a large company with more than 150 employees.',\n",
       "     'score': -1}],\n",
       "   'score': 0,\n",
       "   'time': 8.974778890609741},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': 'Requires a PhD in a field close to mine (or mentioned as a plus): +1.5',\n",
       "     'evidence': '\"PhD in Computer Vision or AI is a plus.\"',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'Job is based in France and the description/requirements request good English: +0.5',\n",
       "     'evidence': 'Location: Osny, Île-de-France, France; and \"Proficient in English (e.g. reading scientific articles, presenting work)\"',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'More than 150 employees (company is a large multinational): -1',\n",
       "     'evidence': 'company name: IDEMIA (IDEMIA is a large multinational company, known to have >150 employees)',\n",
       "     'score': -1}],\n",
       "   'score': 1.0,\n",
       "   'time': 49.71536207199097},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [{'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'Osny, Île-de-France, France, Proficient in English (e.g. reading scientific articles, presenting work)',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'PhD in Computer Vision or AI is a plus.',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'IDEMIA is a large company with more than 150 employees (prior knowledge).',\n",
       "     'score': -1.0}],\n",
       "   'score': 1,\n",
       "   'time': 27.948697805404663}},\n",
       " {'id': 10,\n",
       "  'job_description': \"**Hexaly is a fast-growing software company in Mathematical Optimization** ,\\nalso known as Operations Research or Decision Science. The team develops\\nHexaly Optimizer, the fastest solver for Routing, Scheduling, Packing, and\\nmore, and Hexaly Studio, an innovative low-code platform to build optimization\\napplications in days. In addition, Hexaly delivers turnkey optimization\\napplications through its professional services. In solid growth and\\nprofitability (+32% in 2022, +23% in 2023, +24% in 2024), the company counts\\namong its customers the most prominent companies worldwide, like Amazon,\\nFedEx, Starbucks, Airbus, Bosch, Sony, Renault, Kirin, Shell, TotalEnergies,\\nRepsol, Publicis, JCDecaux, Softbank, and many others.\\n\\n  \\n\\n**_We currently have 5 Senior Operations Research Scientist positions open at\\nHexaly._**\\n\\n  \\n\\n**Your job**\\n\\n  \\n\\nYou are part of the Hexaly tech team. Daily:\\n\\n  * You develop new features, algorithm improvements, and fix bugs in Hexaly Optimizer.\\n  * You provide Hexaly users with dedicated support and training.\\n  * You participate in Hexaly professional services, such as developing turnkey applications for clients.\\n\\n  \\n\\n**Your profile**\\n\\n  \\n\\n  * **You have at least 5 years of experience in Mathematical Optimization, preferably in business and industry.**\\n  * You have a Ph.D. or a Master's in Operations Research, Computer Science, or Applied Mathematics (university or engineering school).\\n  * You love algorithms and coding.\\n  * You have an eye for detail, like a job well done, and don't fear code reviews.\\n  * You are very organized and can follow processes and practices carefully.\\n  * You can listen and handle direct feedback on all that you do.\\n  * You are highly committed to work and are not afraid of change.\\n  * You have excellent written and oral expression in English.\\n\\n  \\n\\nIn the first weeks, we will coach you intensively to improve your knowledge of\\nHexaly's products and acquire Hexaly's software development best practices.\\n\\n  \\n\\n**Your perks and benefits**\\n\\n  \\n\\nAnnual gross salary between €55,000 and €85,000 depending on profile and\\nexperience, plus various advantages (RTT, excellent healthcare insurance,\\nlunch, public transportation, coffee and tea, etc.).\\n\\n  \\n\\n**Recruitment process**\\n\\n  \\n\\n  * Remote interview with Julien, founder & Head of Science (30 minutes);\\n  * On-site assessment with 2 Senior Optimization Scientists (1 hour);\\n  * Welcome among us! A welcome breakfast with your future colleagues awaits you :-)\\n\\nShow more  Show less\\n\\n\",\n",
       "  'oracle': {'evaluation_grid': [{'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': '\"Hexaly is a fast-growing software company in Mathematical Optimization\"; \"We currently have 5 Senior Operations Research Scientist positions open\"; \"You have at least 5 years of experience in Mathematical Optimization\"; \"the fastest solver for Routing, Scheduling, Packing\"',\n",
       "     'score': 2},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'Location: \"Paris, Île-de-France, France\"; \"You have excellent written and oral expression in English.\"',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': '\"You have a Ph.D. or a Master\\'s in Operations Research, Computer Science, or Applied Mathematics (university or engineering school).\"',\n",
       "     'score': 1.5}],\n",
       "   'score': 4,\n",
       "   'time': 80.34622192382812},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': \"The job description states: 'Hexaly is a fast-growing software company in Mathematical Optimization, also known as Operations Research or Decision Science.' and 'You have at least 5 years of experience in Mathematical Optimization, preferably in business and industry.'\",\n",
       "     'score': 2},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': \"Location: Paris, Île-de-France, France. The job description is in English and states: 'You have excellent written and oral expression in English.'\",\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus) (has to be explicitly mentioned in the job description. Having experience leading research teams does not imply a PhD)',\n",
       "     'evidence': \"The job description states: 'You have a Ph.D. or a Master's in Operations Research, Computer Science, or Applied Mathematics (university or engineering school).'\",\n",
       "     'score': 1.5},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'No such requirement is present; all required domains (optimization, algorithms, coding) are covered in the profile.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Requires a programming language I am not familiar with, AND does not mention Python',\n",
       "     'evidence': 'No programming language is specified as required, and the profile covers Python and general coding skills.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'More focused on infrastructure (databases, cloud, Docker) than on algorithms',\n",
       "     'evidence': 'The job is focused on algorithms and optimization, not infrastructure.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Vague description of actual tasks for a data scientist/engineer job',\n",
       "     'evidence': 'The tasks are clearly described: developing features, algorithm improvements, bug fixes, user support, and turnkey application development.',\n",
       "     'score': 0},\n",
       "    {'criteria': \"'Optimization' mentioned primarily for performance/infrastructure (e.g., inference speed, cloud costs, MLOps)\",\n",
       "     'evidence': 'Optimization is mentioned in the context of mathematical optimization, not infrastructure.',\n",
       "     'score': 0},\n",
       "    {'criteria': \"'optimization' mentioned primarily in the context of quantum algorithms\",\n",
       "     'evidence': 'No mention of quantum algorithms.',\n",
       "     'score': 0},\n",
       "    {'criteria': \"Requires 'deep expertise' / 'senior-level experience' / 'mastery' of MLOps, large-scale training, or inference optimization (beyond just 'good fundamentals' or 'being comfortable')\",\n",
       "     'evidence': 'No such requirement is present; the focus is on mathematical optimization and algorithms.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'More managerial than technical role',\n",
       "     'evidence': 'The role is technical, not managerial.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Involves leading a team of highly qualified/experienced people (junior excluded)',\n",
       "     'evidence': 'No mention of team leadership; the role is as a member of the tech team.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Involves coaching world-class scientists',\n",
       "     'evidence': 'No such requirement is present.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Top-tier company (e.g., Google, Apple, Meta, Helsing, Mistral AI, Perplexity, OpenAI, Anthropic, Nvidia)',\n",
       "     'evidence': 'Hexaly is not a top-tier company by the given definition.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'No explicit mention of company size, but the description suggests a smaller, fast-growing company.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Offers a full-remote option',\n",
       "     'evidence': 'No mention of a full-remote option; the process includes an on-site assessment and a welcome breakfast.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Consulting job for a standard/low-tier consulting firm',\n",
       "     'evidence': 'Hexaly is a product company with professional services, not a standard consulting firm.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'In the defense sector',\n",
       "     'evidence': 'Hexaly is not in the defense sector.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'In the robotics sector',\n",
       "     'evidence': 'Hexaly is not in the robotics sector.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'If not french, requires security clearance',\n",
       "     'evidence': 'No mention of security clearance.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "     'evidence': 'No mention of Reinforcement Learning (RL) as a requirement.',\n",
       "     'score': 0},\n",
       "    {'criteria': 'Agentic workflows (ie. langchain, tool use, prompt engineering, etc.) are part of the job',\n",
       "     'evidence': 'No mention of agentic workflows, langchain, or similar technologies.',\n",
       "     'score': 0}],\n",
       "   'score': 4.0,\n",
       "   'time': 10.983829021453857},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': '\"Hexaly is a fast-growing software company in Mathematical Optimization ... The team develops Hexaly Optimizer, the fastest solver for Routing, Scheduling, Packing, and more.\"; job title: \"Senior Operations Research Scientist\"',\n",
       "     'score': 2},\n",
       "    {'criteria': 'The job is based in France and the description is in English',\n",
       "     'evidence': 'Location: \"Paris, Île-de-France, France\"; job description text is in English',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a Ph.D. in a field close to mine (or even if it is just a plus) (explicitly mentioned)',\n",
       "     'evidence': '\"You have a Ph.D. or a Master\\'s in Operations Research, Computer Science, or Applied Mathematics (university or engineering school).\"',\n",
       "     'score': 1.5}],\n",
       "   'score': 4.0,\n",
       "   'time': 21.336983919143677},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [{'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "     'evidence': 'Hexaly is a fast-growing software company in Mathematical Optimization, also known as Operations Research or Decision Science. [...] We currently have 5 Senior Operations Research Scientist positions open at Hexaly. [...] You have at least 5 years of experience in Mathematical Optimization, preferably in business and industry.',\n",
       "     'score': 2.0},\n",
       "    {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "     'evidence': 'location: Paris, Île-de-France, France [...] You have excellent written and oral expression in English.',\n",
       "     'score': 0.5},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': \"You have a Ph.D. or a Master's in Operations Research, Computer Science, or Applied Mathematics (university or engineering school).\",\n",
       "     'score': 1.5}],\n",
       "   'score': 4,\n",
       "   'time': 20.827021837234497}},\n",
       " {'id': 11,\n",
       "  'job_description': \"**Job Title:** Research Scientist – GC-MS/MS\\n\\n**Contract Type:** Full-Time Contract\\n\\n**Location:** North Ryde, Sydney\\n\\n**Clearance Required:** Australian Citizenship + Ability to Obtain Baseline\\nSecurity Clearance\\n\\n**Duration:** 12 months to commence mid-October – early November\\n\\n  \\n\\n**About the Opportunity**\\n\\nAn exciting opportunity is available for an experienced **Research Scientist\\nwith expertise in GC-MS/MS** to join a leading, WADA-accredited laboratory\\ninvolved in the testing and research of anti-doping samples for elite athletes\\nacross Australia, New Zealand, and beyond.\\n\\n  \\n\\nThis role forms part of a dynamic, high-performing team focused on delivering\\nworld-class scientific outcomes in a time-sensitive, quality-driven\\nenvironment. You will be based in **North Ryde, Sydney** , working with\\ncutting-edge instrumentation and contributing to innovative research and\\ndevelopment in anti-doping science.\\n\\n  \\n\\n**About You – Skills & Experience**\\n\\nThe ideal candidate will demonstrate:\\n\\n  * Proven experience in **method development** (eg, sample preparation/extraction, instrument optimisation) **and validation** of analytical techniques in biological samples (essential).\\n  * Strong hands-on expertise with **GC-MS/MS** , including maintenance, troubleshooting, and optimisation (essential).\\n  * Demonstrated experience with gas chromatography tandem mass spectrometry (GC-MSMS) applied to the analysis of small molecules\\n  * Ability to work independently on complex analyses, meet tight deadlines, and contribute to team goals (desirable).\\n  * Proficient communication skills and ability to document scientific work clearly (desirable).\\n  * Experience applying **statistical tools** in chemical/biochemical analysis (desirable).\\n  * Understanding of **ISO 17025 quality systems** in a laboratory setting (desirable).\\n\\n  \\n\\n**Key Responsibilities**\\n\\n  * **Method Development & Validation:** Assist in developing and validating analytical methods in biological matrices, focusing on GC-MS/MS technologies.\\n  * **Research Projects:** Manage and conduct analytical research projects, including data collation, statistical analysis, and reporting.\\n  * **Instrument Operation & Maintenance:** Operate, troubleshoot, and maintain GC-MS/MS instrumentation; assist in training colleagues as needed.\\n  * **Documentation & Reporting:** Accurately document observations and results; contribute to method reports and validation documentation.\\n  * **Scientific Literature Reviews:** Support the team by reviewing current scientific literature relevant to anti-doping and analytical chemistry.\\n\\n  \\n\\n**Qualifications & Eligibility**\\n\\n  * **Bachelors of Science** (Analytical Chemistry or similar) is essential.\\n  * **Minimum 5 years' experience** as a research scientist working with GC-MS/MS.\\n  * Postgraduate qualifications (Master’s or PhD) are highly desirable.\\n  * **Australian Citizenship** is mandatory.\\n  * Must be able to obtain and maintain a **Baseline security clearance**.\\n\\n  \\n\\n**Additional Information**\\n\\n  * Flexible working arrangements are available.\\n  * During major international sporting events, **shift work (including weekends and public holidays)** may be required.\\n\\n  \\n\\n**How to Apply**\\n\\nIf you’re a driven scientist passionate about high-integrity testing and\\nadvanced analytical chemistry, we encourage you to apply today with your CV\\noutlining your relevant experience.\\n\\nor email to ekta.ojha@persolapac.com\\n\\nShow more  Show less\\n\\n\",\n",
       "  'oracle': {'evaluation_grid': [{'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': '“Strong hands-on expertise with GC-MS/MS, including maintenance, troubleshooting, and optimisation (essential).”',\n",
       "     'score': -2},\n",
       "    {'criteria': 'If not french, requires security clearance',\n",
       "     'evidence': '“Clearance Required: Australian Citizenship + Ability to Obtain Baseline Security Clearance.”',\n",
       "     'score': -1.5}],\n",
       "   'score': -3.5,\n",
       "   'time': 70.46337795257568},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences (-2 if this domain/tool is central to the role, defined as being in the job title, company name, or a primary responsibility/requirement; -1 if it is a secondary qualification).\",\n",
       "     'evidence': \"The job title is 'Research Scientist – GC-MS/MS' and the description repeatedly emphasizes required expertise in GC-MS/MS (gas chromatography tandem mass spectrometry), method development and validation in biological samples, and experience in anti-doping/analytical chemistry. These are not present in your profile, which is focused on AI, optimization, RL, and computer vision.\",\n",
       "     'score': -2},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus) (has to be explicitly mentioned in the job description. Having experience leading research teams does not imply a PhD): (+1.5)',\n",
       "     'evidence': 'Postgraduate qualifications (Master’s or PhD) are highly desirable.',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'If not french, requires security clearance: (-1.5)',\n",
       "     'evidence': 'Clearance Required: Australian Citizenship + Ability to Obtain Baseline Security Clearance.',\n",
       "     'score': -1.5}],\n",
       "   'score': -2,\n",
       "   'time': 21.458253145217896},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'Job Title: \"Research Scientist – GC-MS/MS\"; \"Strong hands-on expertise with GC-MS/MS, including maintenance, troubleshooting, and optimisation (essential).\"; \"Proven experience in method development (eg, sample preparation/extraction, instrument optimisation) and validation of analytical techniques in biological samples (essential).\"',\n",
       "     'score': -2},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': '\"Postgraduate qualifications (Master’s or PhD) are highly desirable.\"',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'If not french, requires security clearance',\n",
       "     'evidence': '\"Australian Citizenship + Ability to Obtain Baseline Security Clearance\"; \"Must be able to obtain and maintain a Baseline security clearance.\"',\n",
       "     'score': -1.5}],\n",
       "   'score': -2,\n",
       "   'time': 26.78138494491577},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [{'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'Strong hands-on expertise with GC-MS/MS, including maintenance, troubleshooting, and optimisation (essential).',\n",
       "     'score': -2.0},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'Proven experience in method development (eg, sample preparation/extraction, instrument optimisation) and validation of analytical techniques in biological samples (essential).',\n",
       "     'score': -2.0},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "     'evidence': 'Understanding of ISO 17025 quality systems in a laboratory setting (desirable).',\n",
       "     'score': -1.0},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'Postgraduate qualifications (Master’s or PhD) are highly desirable.',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'If not french, requires security clearance',\n",
       "     'evidence': 'Australian Citizenship is mandatory. Must be able to obtain and maintain a Baseline security clearance.',\n",
       "     'score': -1.5}],\n",
       "   'score': -5,\n",
       "   'time': 21.411925077438354}},\n",
       " {'id': 12,\n",
       "  'job_description': \"At Google, we have a vision of empowerment and equitable opportunity for all\\nAboriginal and Torres Strait Islander peoples and commit to building\\nreconciliation through Google’s technology, platforms and people and we\\nwelcome Indigenous applicants. Please see our Reconciliation Action Plan for\\nmore information.  \\n  \\n**Minimum qualifications:  \\n  \\n**\\n\\n  * PhD degree in Computer Science, a related field, or equivalent practical experience.\\n  * One or more scientific publication submissions for conferences, journals, or public repositories (such as CVPR, ICCV, NeurIPS, ICML, ICLR, etc.).  \\n  \\n\\n**Preferred qualifications:  \\n  \\n**\\n\\n  * Experience in areas like face anti-spoofing, biometrics, 3D/2.5D vision, facial landmark/pose estimation.\\n  * Experience with TensorFlow, Flume, common computer vision libraries/frameworks and Android.\\n  * Interest to build production systems.\\n  * Excellent software engineering skills (e.g., C++, python, data processing, production backend development, etc.).  \\n  \\n\\n**About The Job  \\n  \\n** As an organization, Google maintains a portfolio of research projects\\ndriven by fundamental research, new product innovation, product contribution\\nand infrastructure goals, while providing individuals and teams the freedom to\\nemphasize specific types of work. As a Research Scientist, you'll setup large-\\nscale tests and deploy promising ideas quickly and broadly, managing deadlines\\nand deliverables while applying the latest theories to develop new and\\nimproved products, processes, or technologies. From creating experiments and\\nprototyping implementations to designing new architectures, our research\\nscientists work on real-world problems that span the breadth of computer\\nscience, such as machine (and deep) learning, data mining, natural language\\nprocessing, hardware and software performance analysis, improving compilers\\nfor mobile platforms, as well as core search and much more.  \\n  \\nAs a Research Scientist, you'll also actively contribute to the wider research\\ncommunity by sharing and publishing your findings, with ideas inspired by\\ninternal projects as well as from collaborations with research programs at\\npartner universities and technical institutes all over the world.  \\n  \\nIn this role, you will develop models and architectures grounded in foundation\\nmodels, data-efficient algorithms, and federated learning.  \\n  \\n**Responsibilities  \\n  \\n**\\n\\n  * Author research papers to share and generate impact of research results across the team and in the research community.\\n  * Help in growing research business across teams by sharing research trends and best practices within the community.\\n  * Define the data structure, framework, design, and evaluation metrics for research solution development and implementation under minimal guidance. Identify timelines and obtain resources needed.\\n  * Identify new and upcoming research areas by interacting with external and internal collaborators. Help in developing research strategy and plans to expand the impact of Google research with some guidance. \\n  * Contribute to conducting experiments based on the research question. Develop research prototypes or conduct simulations to further evaluate the impact of research, finalize hypotheses, and refine the research methodology under minimal guidance.  \\n  \\n  \\n\\nGoogle is proud to be an equal opportunity workplace and is an affirmative\\naction employer. We are committed to equal employment opportunity regardless\\nof race, color, ancestry, religion, sex, national origin, sexual orientation,\\nage, citizenship, marital status, disability, gender identity or Veteran\\nstatus. We also consider qualified applicants regardless of criminal\\nhistories, consistent with legal requirements. See also Google's EEO Policy\\nand EEO is the Law. If you have a disability or special need that requires\\naccommodation, please let us know by completing our Accommodations for\\nApplicants form .\\n\\nShow more  Show less\\n\\n\",\n",
       "  'oracle': {'evaluation_grid': [{'criteria': 'Top-tier company (e.g., Google, Apple, Meta, Helsing, Mistral AI, Perplexity, OpenAI, Anthropic, Nvidia)',\n",
       "     'evidence': 'company name: Google',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'Minimum qualifications: PhD degree in Computer Science, a related field, or equivalent practical experience.',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'More than 150 employees',\n",
       "     'evidence': 'company name: Google',\n",
       "     'score': -1}],\n",
       "   'score': 2.5,\n",
       "   'time': 55.94720816612244},\n",
       "  'gpt-4.1': {'evaluation_grid': [{'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "     'evidence': 'Minimum qualifications: PhD degree in Computer Science, a related field, or equivalent practical experience.',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'Top-tier company (e.g., Google, Apple, Meta, etc.)',\n",
       "     'evidence': 'company name: Google',\n",
       "     'score': 2},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences (secondary qualification)\",\n",
       "     'evidence': 'Preferred qualifications: Experience in areas like face anti-spoofing, biometrics, 3D/2.5D vision, facial landmark/pose estimation. (Your profile does not mention experience in face anti-spoofing, biometrics, or facial landmark/pose estimation, which are secondary qualifications.)',\n",
       "     'score': -1},\n",
       "    {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences (secondary qualification)\",\n",
       "     'evidence': 'Preferred qualifications: Experience with TensorFlow, Flume, common computer vision libraries/frameworks and Android. (You have OpenCV and YOLOv8, but no mention of TensorFlow, Flume, or Android. These are secondary qualifications.)',\n",
       "     'score': -1}],\n",
       "   'score': 1.5,\n",
       "   'time': 11.846091985702515},\n",
       "  'gpt-5-mini': {'evaluation_grid': [{'criteria': 'Requires a PhD in a field close to mine (+1.5)',\n",
       "     'evidence': '\"PhD degree in Computer Science, a related field, or equivalent practical experience.\"',\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile (secondary qualification) (-1)',\n",
       "     'evidence': '\"Experience with TensorFlow, Flume, common computer vision libraries/frameworks and Android.\"',\n",
       "     'score': -1},\n",
       "    {'criteria': 'Top-tier company (Google) (+2)',\n",
       "     'evidence': 'company name: Google',\n",
       "     'score': 2},\n",
       "    {'criteria': 'Company >150 employees (-1)',\n",
       "     'evidence': 'company name: Google',\n",
       "     'score': -1}],\n",
       "   'score': 1.5,\n",
       "   'time': 27.367816925048828},\n",
       "  'gemini-2.5-flash': {'evaluation_grid': [{'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences: (-2 if this domain/tool is central to the role, defined as being in the job title, company name, or a primary responsibility/requirement; -1 if it is a secondary qualification).\",\n",
       "     'evidence': \"Job Title: 'Research Scientist, Vision Language and Multimodal Modeling' - The profile has Computer Vision but not explicit Language and Multimodal Modeling expertise. Preferred qualifications: 'Experience in areas like face anti-spoofing, biometrics, 3D/2.5D vision, facial landmark/pose estimation.' - The profile has general Computer Vision but not these specific sub-domains. Preferred qualifications: 'Experience with TensorFlow, Flume, common computer vision libraries/frameworks and Android.' - The profile mentions PyTorch, JAX, OpenCV, but not explicitly Flume or Android.\",\n",
       "     'score': -4.0},\n",
       "    {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus) (has to be explicitly mentioned in the job description. Having experience leading research teams does not imply a PhD): (+1.5)',\n",
       "     'evidence': \"Minimum qualifications: 'PhD degree in Computer Science, a related field, or equivalent practical experience.'\",\n",
       "     'score': 1.5},\n",
       "    {'criteria': 'Top-tier company (e.g., Google, Apple, Meta, Helsing, Mistral AI, Perplexity, OpenAI, Anthropic, Nvidia): (+2) (Do not trust the description of the company in the job description for this criteria, but your prior knowledge about the company if any.)',\n",
       "     'evidence': 'company name: Google',\n",
       "     'score': 2.0},\n",
       "    {'criteria': 'More than 150 employees: (-1)',\n",
       "     'evidence': 'Google is a very large company.',\n",
       "     'score': -1.0}],\n",
       "   'score': -1.5,\n",
       "   'time': 37.50789189338684}}]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e1a2cb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_description</th>\n",
       "      <th>oracle</th>\n",
       "      <th>gpt-4.1</th>\n",
       "      <th>gpt-5-mini</th>\n",
       "      <th>gemini-2.5-flash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>evaluation_grid</th>\n",
       "      <td>11</td>\n",
       "      <td>**Job Title:** Research Scientist – GC-MS/MS\\n...</td>\n",
       "      <td>[{'criteria': 'Requires demonstrated expertise...</td>\n",
       "      <td>[{'criteria': 'Requires demonstrated expertise...</td>\n",
       "      <td>[{'criteria': 'Requires demonstrated expertise...</td>\n",
       "      <td>[{'criteria': 'Requires demonstrated expertise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>11</td>\n",
       "      <td>**Job Title:** Research Scientist – GC-MS/MS\\n...</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>11</td>\n",
       "      <td>**Job Title:** Research Scientist – GC-MS/MS\\n...</td>\n",
       "      <td>70.463378</td>\n",
       "      <td>21.458253</td>\n",
       "      <td>26.781385</td>\n",
       "      <td>21.411925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                    job_description  \\\n",
       "evaluation_grid  11  **Job Title:** Research Scientist – GC-MS/MS\\n...   \n",
       "score            11  **Job Title:** Research Scientist – GC-MS/MS\\n...   \n",
       "time             11  **Job Title:** Research Scientist – GC-MS/MS\\n...   \n",
       "\n",
       "                                                            oracle  \\\n",
       "evaluation_grid  [{'criteria': 'Requires demonstrated expertise...   \n",
       "score                                                         -3.5   \n",
       "time                                                     70.463378   \n",
       "\n",
       "                                                           gpt-4.1  \\\n",
       "evaluation_grid  [{'criteria': 'Requires demonstrated expertise...   \n",
       "score                                                           -2   \n",
       "time                                                     21.458253   \n",
       "\n",
       "                                                        gpt-5-mini  \\\n",
       "evaluation_grid  [{'criteria': 'Requires demonstrated expertise...   \n",
       "score                                                           -2   \n",
       "time                                                     26.781385   \n",
       "\n",
       "                                                  gemini-2.5-flash  \n",
       "evaluation_grid  [{'criteria': 'Requires demonstrated expertise...  \n",
       "score                                                           -5  \n",
       "time                                                     21.411925  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make df containing score and time for each model\n",
    "df = pd.DataFrame(results[10])\n",
    "# df = df[[\"model\", \"score\", \"time\"]]\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "481f3101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "  'evidence': 'Strong software-design instincts: testing, code review, CI/CD',\n",
       "  'score': -1.0},\n",
       " {'criteria': \"'Optimization' mentioned primarily for performance/infrastructure (e.g., inference speed, cloud costs, MLOps)\",\n",
       "  'evidence': 'you’ll build and optimise the large-scale learning systems',\n",
       "  'score': -3.0},\n",
       " {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "  'evidence': 'Location: Paris / London (hybrid) or remote from EU/UK',\n",
       "  'score': 0.5},\n",
       " {'criteria': 'Requires \"deep expertise\" / \"senior-level experience\" / \"mastery\" of MLOps, large-scale training, or inference optimization (beyond just \"good fundamentals\" or \"being comfortable\")',\n",
       "  'evidence': '4 + years working on large-scale ML codebases',\n",
       "  'score': -1.0},\n",
       " {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "  'evidence': 'Master’s or PhD in Computer Science (or equivalent proven track record)',\n",
       "  'score': 1.5},\n",
       " {'criteria': 'Top-tier company (e.g., Google, Apple, Meta, Helsing, Mistral AI, Perplexity, OpenAI, Anthropic, Nvidia)',\n",
       "  'evidence': 'company name: Mistral AI',\n",
       "  'score': 2.0},\n",
       " {'criteria': 'Offers a full-remote option',\n",
       "  'evidence': 'Location: Paris / London (hybrid) or remote from EU/UK',\n",
       "  'score': 2.0}]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][\"gemini-2.5-flash\"][\"evaluation_grid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8f004f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.1:\n",
      "  Mean Absolute Error: 1.292\n",
      "  Average Time: 10.774s\n",
      "\n",
      "gpt-5-mini:\n",
      "  Mean Absolute Error: 1.792\n",
      "  Average Time: 31.905s\n",
      "\n",
      "gemini-2.5-flash:\n",
      "  Mean Absolute Error: 1.417\n",
      "  Average Time: 27.163s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute mean absolute error and average time for each model compared to oracle\n",
    "model_performance = {}\n",
    "\n",
    "for model_name in [\"gpt-4.1\", \"gpt-5-mini\", \"gemini-2.5-flash\"]:\n",
    "    mae_scores = []\n",
    "    times = []\n",
    "    \n",
    "    for result in results:\n",
    "        oracle_score = result[\"oracle\"][\"score\"]\n",
    "        model_score = result[model_name][\"score\"]\n",
    "        mae = abs(oracle_score - model_score)\n",
    "        mae_scores.append(mae)\n",
    "        times.append(result[model_name][\"time\"])\n",
    "    \n",
    "    model_performance[model_name] = {\n",
    "        \"mean_absolute_error\": sum(mae_scores) / len(mae_scores),\n",
    "        \"average_time\": sum(times) / len(times)\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "for model, metrics in model_performance.items():\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  Mean Absolute Error: {metrics['mean_absolute_error']:.3f}\")\n",
    "    print(f\"  Average Time: {metrics['average_time']:.3f}s\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a487cb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job offer score variances (sorted by variance):\n",
      "==================================================\n",
      "Job ID: 7\n",
      "  Variance: 7.688\n",
      "  Scores: [-1.5, 3.5, 3.5, -2.5]\n",
      "\n",
      "Job ID: 4\n",
      "  Variance: 6.688\n",
      "  Scores: [6, 4, 2.0, 9]\n",
      "\n",
      "Job ID: 5\n",
      "  Variance: 6.562\n",
      "  Scores: [-1.5, 3, 4.5, 5]\n",
      "\n",
      "Job ID: 11\n",
      "  Variance: 2.250\n",
      "  Scores: [2.5, 1.5, 1.5, -1.5]\n",
      "\n",
      "Job ID: 10\n",
      "  Variance: 1.547\n",
      "  Scores: [-3.5, -2, -2, -5]\n",
      "\n",
      "Job ID: 2\n",
      "  Variance: 0.750\n",
      "  Scores: [-4, -4, -2, -4]\n",
      "\n",
      "Job ID: 1\n",
      "  Variance: 0.188\n",
      "  Scores: [2, 2, 2, 3]\n",
      "\n",
      "Job ID: 3\n",
      "  Variance: 0.188\n",
      "  Scores: [3, 3, 4, 3]\n",
      "\n",
      "Job ID: 8\n",
      "  Variance: 0.188\n",
      "  Scores: [1, 0, 1.0, 1]\n",
      "\n",
      "Job ID: 0\n",
      "  Variance: 0.172\n",
      "  Scores: [0, -0.5, -1, 0]\n",
      "\n",
      "Job ID: 6\n",
      "  Variance: 0.000\n",
      "  Scores: [1, 1, 1.0, 1]\n",
      "\n",
      "Job ID: 9\n",
      "  Variance: 0.000\n",
      "  Scores: [4, 4.0, 4.0, 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute variance in scores for each job offer\n",
    "job_variances = []\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    scores = [\n",
    "        result[\"oracle\"][\"score\"],\n",
    "        result[\"gpt-4.1\"][\"score\"],\n",
    "        result[\"gpt-5-mini\"][\"score\"],\n",
    "        result[\"gemini-2.5-flash\"][\"score\"]\n",
    "    ]\n",
    "    \n",
    "    # Calculate variance\n",
    "    mean_score = sum(scores) / len(scores)\n",
    "    variance = sum((score - mean_score) ** 2 for score in scores) / len(scores)\n",
    "    \n",
    "    job_variances.append({\n",
    "        \"job_id\": i,  # Assuming job_id corresponds to index in results\n",
    "        \"variance\": variance,\n",
    "        \"scores\": scores\n",
    "    })\n",
    "\n",
    "# Display results sorted by variance (highest first)\n",
    "job_variances_sorted = sorted(job_variances, key=lambda x: x[\"variance\"], reverse=True)\n",
    "\n",
    "print(\"Job offer score variances (sorted by variance):\")\n",
    "print(\"=\" * 50)\n",
    "for job in job_variances_sorted:\n",
    "    print(f\"Job ID: {job['job_id']}\")\n",
    "    print(f\"  Variance: {job['variance']:.3f}\")\n",
    "    print(f\"  Scores: {job['scores']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a5697c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a research engineer on our team, you will partner with research scientists\\nto turn research ideas into working systems; building the data, tooling, and\\ninfrastructure that enable rapid iteration, trustworthy evaluation, and a\\nsmooth path from prototype to production.  \\n  \\nBuilding on our proven track record of AI-powered solutions (e.g., Bits AI,\\nWatchdog, and Toto), Datadog AI Research is tackling high-risk, high-reward\\nprojects grounded in real-world challenges in cloud observability and\\nsecurity.  \\n  \\nWe are currently focused on three key research areas:  \\n  \\n\\n  * Observability Foundation Models – Building state-of-the-art models for advanced forecasting, anomaly detection, and multi-modal telemetry analysis (logs, metrics, traces, etc.). These models will also provide the foundation for our agents (described below) to natively analyze telemetry data. \\n  * Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents in production environments, pushing the boundaries of multi-step planning, reasoning, and domain-specific knowledge. \\n  * Production Code Repair Agents – Developing agents and models that leverage code, logs, runtime data, and other signals to identify, fix, and even preempt performance issues and security vulnerabilities in production code.   \\n  \\n\\n**What You’ll Do:  \\n  \\n**\\n\\n  * Build and operate datasets, training and evaluation pipelines, benchmarks, and internal tooling \\n  * Implement models, run experiments at scale, and profile for reliability, performance, and cost \\n  * Orchestrate distributed training and distributed RL with Ray, including scheduling, scaling, and failure recovery \\n  * Make the research stack observable, reproducible, and easier to use \\n  * Establish rigorous automated benchmarks and regression tests for forecasting, anomaly detection, multi-modal analysis, agents, and code repair tasks \\n  * Collaborate with Research Scientists, Product, and Engineering to integrate advanced AI capabilities into Datadog’s product ecosystem and to harden prototypes into reliable services \\n  * Contribute high-quality code, documentation, and open-source artifacts that enable the community and internal teams to reproduce, extend, and evaluate results   \\n  \\n\\n**Who You Are:  \\n  \\n**\\n\\n  * You have strong software engineering skills with experience in domains such as observability, SRE, or security \\n  * You have depth in distributed computing and ML systems for training and inference at scale; experience with Ray, Slurm, or similar frameworks is a plus \\n  * You are proficient in Python, familiar with a systems language (e.g., Rust, C++, or Go), and you are comfortable with modern cloud and data infrastructure \\n  * You have practical experience implementing and operating ML training and inference systems (e.g., PyTorch or JAX), including containerization, orchestration, and GPU acceleration \\n  * You are familiar with efficient training, fine-tuning, and inference techniques for large foundation models \\n  * You can explain design and performance trade-offs clearly to both technical and non-technical audiences \\n  * You have a strong interest in open-science and open-source contributions, including establishing rigorous benchmarks and sharing artifacts with the community   \\n  \\n\\n**Bonus Points:  \\n  \\n**\\n\\n  * You have a demonstrated ability to bridge cutting-edge research prototypes and real-world product applications, ideally with large foundation models, generative AI agents, or domain-specific LLM deployments \\n  * You are passionate about pushing the boundaries of AI while maintaining a strong focus on customer impact, scalability, and responsible deployment of new technologies \\n  * You have hands-on experience with GPU programming and optimization, including experience in CUDA \\n  * You have experience writing production data pipelines and applications \\n  * You have experience supporting or contributing to research publications   \\n  \\n\\n_Datadog values people from all walks of life. We understand not everyone will\\nmeet all the above qualifications on day one. That's okay. If you’re\\npassionate about AI Research and want to grow your skills, we encourage you to\\napply.  \\n  \\n_**Benefits and Growth:  \\n  \\n**\\n\\n  * Competitive global benefits \\n  * New hire stock equity (RSUs) and employee stock purchase plan (ESPP) \\n  * Opportunity to collaborate closely with colleagues across the Datadog offices in New York City and Paris \\n  * Opportunity to attend and present at conferences and meetups \\n  * Intra-departmental mentor and buddy program for in-house networking \\n  * An inclusive company culture, ability to join our Community Guilds (Datadog employee resource groups)   \\n  \\n\\n_Benefits and Growth listed above may vary based on the country of your\\nemployment and the nature of your employment with Datadog.  \\n  \\n_**About Datadog:  \\n  \\n** Datadog (NASDAQ: DDOG) is a global SaaS business, delivering a rare\\ncombination of growth and profitability. We are on a mission to break down\\nsilos and solve complexity in the cloud age by enabling digital\\ntransformation, cloud migration, and infrastructure monitoring of our\\ncustomers’ entire technology stacks. Built by engineers, for engineers,\\nDatadog is used by organizations of all sizes across a wide range of\\nindustries. Together, we champion professional development, diversity of\\nthought, innovation, and work excellence to empower continuous growth. Join\\nthe pack and become part of a collaborative, pragmatic, and thoughtful people-\\nfirst community where we solve tough problems, take smart risks, and celebrate\\none another. Learn more about #DatadogLife on Instagram, LinkedIn, and Datadog\\nLearning Center.  \\n  \\n**Equal Opportunity at Datadog:  \\n  \\n** Datadog is proud to offer equal employment opportunity to everyone\\nregardless of race, color, ancestry, religion, sex, national origin, sexual\\norientation, age, citizenship, marital status, disability, gender identity,\\nveteran status, and other characteristics protected by law. We also consider\\nqualified applicants regardless of criminal histories, consistent with legal\\nrequirements. Here are our Candidate Legal Notices for your reference.  \\n  \\nDatadog endeavors to make our Careers Page accessible to all users. If you\\nwould like to contact us regarding the accessibility of our website or need\\nassistance completing the application process, please complete this form. This\\nform is for accommodation requests only and cannot be used to inquire about\\nthe status of applications.  \\n  \\n**Privacy and AI Guidelines:  \\n  \\n** Any information you submit to Datadog as part of your application will be\\nprocessed in accordance with Datadog’s Applicant and Candidate Privacy Notice.\\nFor information on our AI policy, please visit Interviewing at Datadog AI\\nGuidelines.\\n\\nShow more  Show less\\n\\n\""
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[7][\"job_description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4544d0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'criteria': 'Explicitly mentions Reinforcement Learning (RL) as a key requirement or skill',\n",
       "  'evidence': \"The job description states: 'Orchestrate distributed training and distributed RL with Ray, including scheduling, scaling, and failure recovery.'\",\n",
       "  'score': 2},\n",
       " {'criteria': 'Mentions explicitly algorithmic/mathematical optimization (e.g., Operations Research, planning, combinatorial optimization, MILP)',\n",
       "  'evidence': 'The job description does not mention algorithmic/mathematical optimization, operations research, or related terms.',\n",
       "  'score': 0},\n",
       " {'criteria': 'Agentic workflows (ie. langchain, tool use, prompt engineering, etc.) are part of the job',\n",
       "  'evidence': \"The job description includes: 'Site Reliability Engineering (SRE) Autonomous Agents – Creating AI agents to automatically detect, diagnose, and resolve incidents in production environments...' and 'Production Code Repair Agents – Developing agents and models...' This is about building AI agents, but does not mention agentic workflow frameworks (langchain, tool use, prompt engineering) explicitly. However, the focus on agents is strong, so +2 is justified.\",\n",
       "  'score': 2},\n",
       " {'criteria': \"Requires demonstrated expertise in a specific technical domain or toolset that is absent from my profile's listed skills and experiences\",\n",
       "  'evidence': 'The job requires proficiency in Python and familiarity with a systems language (e.g., Rust, C++, or Go). The profile mentions some exposure to C++ and Java, but not Rust or Go. However, Python is the main language, and the systems language is not central. No penalty.',\n",
       "  'score': 0},\n",
       " {'criteria': 'Requires a programming language I am not familiar with, AND does not mention Python',\n",
       "  'evidence': 'The job requires Python as the main language, which is present in the profile. No penalty.',\n",
       "  'score': 0},\n",
       " {'criteria': 'More focused on infrastructure (databases, cloud, Docker) than on algorithms',\n",
       "  'evidence': 'The job description balances infrastructure (datasets, pipelines, distributed training) and algorithms (model implementation, research, agents). Not more focused on infrastructure than algorithms. No penalty.',\n",
       "  'score': 0},\n",
       " {'criteria': 'Vague description of actual tasks for a data scientist/engineer job',\n",
       "  'evidence': 'The job description is detailed and specific about tasks and responsibilities. No penalty.',\n",
       "  'score': 0},\n",
       " {'criteria': \"'Optimization' mentioned primarily for performance/infrastructure (e.g., inference speed, cloud costs, MLOps)\",\n",
       "  'evidence': 'Optimization is mentioned in the context of reliability, performance, and cost, but not as the primary focus. The main focus is on research and building agents/models. No penalty.',\n",
       "  'score': 0},\n",
       " {'criteria': 'The job is based in France and requires a good english level. If the description is in english and the job is based in France, this criterion is verified.',\n",
       "  'evidence': 'Location: Paris, Île-de-France, France. The job description is in English.',\n",
       "  'score': 0.5},\n",
       " {'criteria': 'More managerial than technical role',\n",
       "  'evidence': 'The role is technical, not managerial. No penalty.',\n",
       "  'score': 0},\n",
       " {'criteria': 'Involves leading a team of highly qualified/experienced people (junior excluded)',\n",
       "  'evidence': 'No mention of team leadership responsibilities. No penalty.',\n",
       "  'score': 0},\n",
       " {'criteria': 'Involves coaching world-class scientists',\n",
       "  'evidence': 'No mention of coaching world-class scientists. No penalty.',\n",
       "  'score': 0},\n",
       " {'criteria': 'Top-tier company (e.g., Google, Apple, Meta, Helsing, Mistral AI, Perplexity, OpenAI, Anthropic, Nvidia)',\n",
       "  'evidence': 'Datadog is not considered a top-tier AI research company by the provided definition.',\n",
       "  'score': 0},\n",
       " {'criteria': 'More than 150 employees',\n",
       "  'evidence': 'Datadog is a global SaaS business, publicly traded (NASDAQ: DDOG), and has more than 150 employees.',\n",
       "  'score': -1},\n",
       " {'criteria': 'Offers a full-remote option',\n",
       "  'evidence': 'No mention of a full-remote option in the job description.',\n",
       "  'score': 0},\n",
       " {'criteria': 'Consulting job for a standard/low-tier consulting firm',\n",
       "  'evidence': 'Not a consulting job.',\n",
       "  'score': 0},\n",
       " {'criteria': 'In the defense sector',\n",
       "  'evidence': 'Not in the defense sector.',\n",
       "  'score': 0},\n",
       " {'criteria': 'In the robotics sector',\n",
       "  'evidence': 'Not in the robotics sector.',\n",
       "  'score': 0},\n",
       " {'criteria': 'If not french, requires security clearance',\n",
       "  'evidence': 'No mention of security clearance.',\n",
       "  'score': 0},\n",
       " {'criteria': \"Requires 'deep expertise' / 'senior-level experience' / 'mastery' of MLOps, large-scale training, or inference optimization (beyond just 'good fundamentals' or 'being comfortable')\",\n",
       "  'evidence': \"The job requires 'depth in distributed computing and ML systems for training and inference at scale', but does not use terms like 'deep expertise', 'senior-level', or 'mastery'. No penalty.\",\n",
       "  'score': 0},\n",
       " {'criteria': 'Requires a PhD in a field close to mine (or even if it is just a plus)',\n",
       "  'evidence': 'The job description does not explicitly mention a PhD as a requirement or a plus.',\n",
       "  'score': 0}]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[7][\"gpt-4.1\"][\"evaluation_grid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "62c5ed3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AryaXAI stands at the forefront of AI innovation, revolutionizing AI for\\nmission-critical businesses by building explainable, safe, and aligned systems\\nthat scale responsibly. Our mission is to create AI tools that empower\\nresearchers, engineers, and organizations to unlock AI's full potential while\\nmaintaining transparency and safety.  \\n  \\nOur team thrives on a shared passion for cutting-edge innovation,\\ncollaboration, and a relentless drive for excellence. At AryaXAI, everyone\\ncontributes hands-on to our mission in a flat organizational structure that\\nvalues curiosity, initiative, and exceptional performance.  \\n  \\nAs a research scientist at AryaXAI, you will be uniquely positioned in our\\nteam to work on very large-scale industry problems and push forward the\\nfrontiers of AI technologies. You will become a part of the unique atmosphere\\nwhere startup culture meets research innovation, with key outcomes of speed\\nand reliability.  \\n  \\n**Responsibilities  \\n  \\n**\\n\\n  * You'll work on advanced problems related to AI explainability, AI safety, and AI alignment.\\n  * You'll have flexibility in picking up the specialization areas within ML/DL and problem types that address the above challenges.\\n  * Create new techniques around ML Observability & Alignment.\\n  * Collaborate with MLEs and SDE to roll out the features and manage their quality until they are fully stable.\\n  * Create and maintain technical and product documentation.\\n  * Publish papers in open forums like arxiv and present in industry forums like ICLR NeurIPS etc.   \\n  \\n\\n**Qualifications  \\n  \\n**\\n\\n  * Has a solid academic background in concepts of machine learning or deep learning or reinforcement learning.\\n  * Master or Ph.D in key engineering topics like computer science or Mathematics is required\\n  * Should have published peer-reviewed papers or contributed to opensource tools\\n  * Hands-on experience in working with deep learning frameworks like Tensorflow, Pytorch etc\\n  * Enjoys working on various DL problems that involve using different types of training data sets - textual, tabular, categorical, images etc\\n  * Comfortable deploying code in cloud environments/on-premise environments.\\n  * Good fundamentals in MLOps and productionising ML models.\\n  * Prior experience on working on ML explainability methods - LRP, SHAPE, LIME, IG, CEM etc.\\n  * 2+ years of hands-on experience in Deep Learning or Machine Learning.\\n  * Hands-on experience in implementing techniques like Transformer models, GANs, Deep Learning, etc.\\n\\nShow more  Show less\\n\\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_full_job(2)[\"description\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3590db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4b721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ChatOpenAI in module langchain_openai.chat_models.base:\n",
      "\n",
      "class ChatOpenAI(BaseChatOpenAI)\n",
      " |  ChatOpenAI(\n",
      " |      *args: Any,\n",
      " |      name: Optional[str] = None,\n",
      " |      cache: Union[langchain_core.caches.BaseCache, bool, NoneType] = None,\n",
      " |      verbose: bool = <factory>,\n",
      " |      callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None,\n",
      " |      tags: Optional[list[str]] = None,\n",
      " |      metadata: Optional[dict[str, Any]] = None,\n",
      " |      custom_get_token_ids: Optional[Callable[[str], list[int]]] = None,\n",
      " |      callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None,\n",
      " |      rate_limiter: Optional[langchain_core.rate_limiters.BaseRateLimiter] = None,\n",
      " |      disable_streaming: Union[bool, Literal['tool_calling']] = False,\n",
      " |      client: Any = None,\n",
      " |      async_client: Any = None,\n",
      " |      root_client: Any = None,\n",
      " |      root_async_client: Any = None,\n",
      " |      model: str = 'gpt-3.5-turbo',\n",
      " |      temperature: Optional[float] = None,\n",
      " |      model_kwargs: dict[str, typing.Any] = <factory>,\n",
      " |      api_key: Optional[pydantic.types.SecretStr] = <factory>,\n",
      " |      base_url: Optional[str] = None,\n",
      " |      organization: Optional[str] = None,\n",
      " |      openai_proxy: Optional[str] = <factory>,\n",
      " |      timeout: Union[float, tuple[float, float], Any, NoneType] = None,\n",
      " |      stream_usage: Optional[bool] = None,\n",
      " |      max_retries: Optional[int] = None,\n",
      " |      presence_penalty: Optional[float] = None,\n",
      " |      frequency_penalty: Optional[float] = None,\n",
      " |      seed: Optional[int] = None,\n",
      " |      logprobs: Optional[bool] = None,\n",
      " |      top_logprobs: Optional[int] = None,\n",
      " |      logit_bias: Optional[dict[int, int]] = None,\n",
      " |      streaming: bool = False,\n",
      " |      n: Optional[int] = None,\n",
      " |      top_p: Optional[float] = None,\n",
      " |      max_completion_tokens: Optional[int] = None,\n",
      " |      reasoning_effort: Optional[str] = None,\n",
      " |      reasoning: Optional[dict[str, Any]] = None,\n",
      " |      verbosity: Optional[str] = None,\n",
      " |      tiktoken_model_name: Optional[str] = None,\n",
      " |      default_headers: Optional[collections.abc.Mapping[str, str]] = None,\n",
      " |      default_query: Optional[collections.abc.Mapping[str, object]] = None,\n",
      " |      http_client: Optional[Any] = None,\n",
      " |      http_async_client: Optional[Any] = None,\n",
      " |      stop_sequences: Union[list[str], str, NoneType] = None,\n",
      " |      extra_body: Optional[collections.abc.Mapping[str, Any]] = None,\n",
      " |      include_response_headers: bool = False,\n",
      " |      disabled_params: Optional[dict[str, Any]] = None,\n",
      " |      include: Optional[list[str]] = None,\n",
      " |      service_tier: Optional[str] = None,\n",
      " |      store: Optional[bool] = None,\n",
      " |      truncation: Optional[str] = None,\n",
      " |      use_previous_response_id: bool = False,\n",
      " |      use_responses_api: Optional[bool] = None,\n",
      " |      output_version: Literal['v0', 'responses/v1'] = 'v0'\n",
      " |  ) -> None\n",
      " |\n",
      " |  OpenAI chat model integration.\n",
      " |\n",
      " |  .. dropdown:: Setup\n",
      " |      :open:\n",
      " |\n",
      " |      Install ``langchain-openai`` and set environment variable ``OPENAI_API_KEY``.\n",
      " |\n",
      " |      .. code-block:: bash\n",
      " |\n",
      " |          pip install -U langchain-openai\n",
      " |          export OPENAI_API_KEY=\"your-api-key\"\n",
      " |\n",
      " |  .. dropdown:: Key init args — completion params\n",
      " |\n",
      " |      model: str\n",
      " |          Name of OpenAI model to use.\n",
      " |      temperature: float\n",
      " |          Sampling temperature.\n",
      " |      max_tokens: Optional[int]\n",
      " |          Max number of tokens to generate.\n",
      " |      logprobs: Optional[bool]\n",
      " |          Whether to return logprobs.\n",
      " |      stream_options: Dict\n",
      " |          Configure streaming outputs, like whether to return token usage when\n",
      " |          streaming (``{\"include_usage\": True}``).\n",
      " |      use_responses_api: Optional[bool]\n",
      " |          Whether to use the responses API.\n",
      " |\n",
      " |      See full list of supported init args and their descriptions in the params section.\n",
      " |\n",
      " |  .. dropdown:: Key init args — client params\n",
      " |\n",
      " |      timeout: Union[float, Tuple[float, float], Any, None]\n",
      " |          Timeout for requests.\n",
      " |      max_retries: Optional[int]\n",
      " |          Max number of retries.\n",
      " |      api_key: Optional[str]\n",
      " |          OpenAI API key. If not passed in will be read from env var ``OPENAI_API_KEY``.\n",
      " |      base_url: Optional[str]\n",
      " |          Base URL for API requests. Only specify if using a proxy or service\n",
      " |          emulator.\n",
      " |      organization: Optional[str]\n",
      " |          OpenAI organization ID. If not passed in will be read from env\n",
      " |          var ``OPENAI_ORG_ID``.\n",
      " |\n",
      " |      See full list of supported init args and their descriptions in the params section.\n",
      " |\n",
      " |  .. dropdown:: Instantiate\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          llm = ChatOpenAI(\n",
      " |              model=\"gpt-4o\",\n",
      " |              temperature=0,\n",
      " |              max_tokens=None,\n",
      " |              timeout=None,\n",
      " |              max_retries=2,\n",
      " |              # api_key=\"...\",\n",
      " |              # base_url=\"...\",\n",
      " |              # organization=\"...\",\n",
      " |              # other params...\n",
      " |          )\n",
      " |\n",
      " |      .. note::\n",
      " |          Any param which is not explicitly supported will be passed directly to the\n",
      " |          ``openai.OpenAI.chat.completions.create(...)`` API every time to the model is\n",
      " |          invoked. For example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |              import openai\n",
      " |\n",
      " |              ChatOpenAI(..., frequency_penalty=0.2).invoke(...)\n",
      " |\n",
      " |              # results in underlying API call of:\n",
      " |\n",
      " |              openai.OpenAI(..).chat.completions.create(..., frequency_penalty=0.2)\n",
      " |\n",
      " |              # which is also equivalent to:\n",
      " |\n",
      " |              ChatOpenAI(...).invoke(..., frequency_penalty=0.2)\n",
      " |\n",
      " |  .. dropdown:: Invoke\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          messages = [\n",
      " |              (\n",
      " |                  \"system\",\n",
      " |                  \"You are a helpful translator. Translate the user sentence to French.\",\n",
      " |              ),\n",
      " |              (\"human\", \"I love programming.\"),\n",
      " |          ]\n",
      " |          llm.invoke(messages)\n",
      " |\n",
      " |      .. code-block:: pycon\n",
      " |\n",
      " |          AIMessage(\n",
      " |              content=\"J'adore la programmation.\",\n",
      " |              response_metadata={\n",
      " |                  \"token_usage\": {\n",
      " |                      \"completion_tokens\": 5,\n",
      " |                      \"prompt_tokens\": 31,\n",
      " |                      \"total_tokens\": 36,\n",
      " |                  },\n",
      " |                  \"model_name\": \"gpt-4o\",\n",
      " |                  \"system_fingerprint\": \"fp_43dfabdef1\",\n",
      " |                  \"finish_reason\": \"stop\",\n",
      " |                  \"logprobs\": None,\n",
      " |              },\n",
      " |              id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n",
      " |              usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n",
      " |          )\n",
      " |\n",
      " |  .. dropdown:: Stream\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          for chunk in llm.stream(messages):\n",
      " |              print(chunk.text(), end=\"\")\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          AIMessageChunk(content=\"\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      " |          AIMessageChunk(content=\"J\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      " |          AIMessageChunk(\n",
      " |              content=\"'adore\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\"\n",
      " |          )\n",
      " |          AIMessageChunk(content=\" la\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      " |          AIMessageChunk(\n",
      " |              content=\" programmation\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\"\n",
      " |          )\n",
      " |          AIMessageChunk(content=\".\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      " |          AIMessageChunk(\n",
      " |              content=\"\",\n",
      " |              response_metadata={\"finish_reason\": \"stop\"},\n",
      " |              id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\",\n",
      " |          )\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          stream = llm.stream(messages)\n",
      " |          full = next(stream)\n",
      " |          for chunk in stream:\n",
      " |              full += chunk\n",
      " |          full\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          AIMessageChunk(\n",
      " |              content=\"J'adore la programmation.\",\n",
      " |              response_metadata={\"finish_reason\": \"stop\"},\n",
      " |              id=\"run-bf917526-7f58-4683-84f7-36a6b671d140\",\n",
      " |          )\n",
      " |\n",
      " |  .. dropdown:: Async\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          await llm.ainvoke(messages)\n",
      " |\n",
      " |          # stream:\n",
      " |          # async for chunk in (await llm.astream(messages))\n",
      " |\n",
      " |          # batch:\n",
      " |          # await llm.abatch([messages])\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          AIMessage(\n",
      " |              content=\"J'adore la programmation.\",\n",
      " |              response_metadata={\n",
      " |                  \"token_usage\": {\n",
      " |                      \"completion_tokens\": 5,\n",
      " |                      \"prompt_tokens\": 31,\n",
      " |                      \"total_tokens\": 36,\n",
      " |                  },\n",
      " |                  \"model_name\": \"gpt-4o\",\n",
      " |                  \"system_fingerprint\": \"fp_43dfabdef1\",\n",
      " |                  \"finish_reason\": \"stop\",\n",
      " |                  \"logprobs\": None,\n",
      " |              },\n",
      " |              id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n",
      " |              usage_metadata={\n",
      " |                  \"input_tokens\": 31,\n",
      " |                  \"output_tokens\": 5,\n",
      " |                  \"total_tokens\": 36,\n",
      " |              },\n",
      " |          )\n",
      " |\n",
      " |  .. dropdown:: Tool calling\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from pydantic import BaseModel, Field\n",
      " |\n",
      " |\n",
      " |          class GetWeather(BaseModel):\n",
      " |              '''Get the current weather in a given location'''\n",
      " |\n",
      " |              location: str = Field(\n",
      " |                  ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
      " |              )\n",
      " |\n",
      " |\n",
      " |          class GetPopulation(BaseModel):\n",
      " |              '''Get the current population in a given location'''\n",
      " |\n",
      " |              location: str = Field(\n",
      " |                  ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
      " |              )\n",
      " |\n",
      " |\n",
      " |          llm_with_tools = llm.bind_tools(\n",
      " |              [GetWeather, GetPopulation]\n",
      " |              # strict = True  # enforce tool args schema is respected\n",
      " |          )\n",
      " |          ai_msg = llm_with_tools.invoke(\n",
      " |              \"Which city is hotter today and which is bigger: LA or NY?\"\n",
      " |          )\n",
      " |          ai_msg.tool_calls\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          [\n",
      " |              {\n",
      " |                  \"name\": \"GetWeather\",\n",
      " |                  \"args\": {\"location\": \"Los Angeles, CA\"},\n",
      " |                  \"id\": \"call_6XswGD5Pqk8Tt5atYr7tfenU\",\n",
      " |              },\n",
      " |              {\n",
      " |                  \"name\": \"GetWeather\",\n",
      " |                  \"args\": {\"location\": \"New York, NY\"},\n",
      " |                  \"id\": \"call_ZVL15vA8Y7kXqOy3dtmQgeCi\",\n",
      " |              },\n",
      " |              {\n",
      " |                  \"name\": \"GetPopulation\",\n",
      " |                  \"args\": {\"location\": \"Los Angeles, CA\"},\n",
      " |                  \"id\": \"call_49CFW8zqC9W7mh7hbMLSIrXw\",\n",
      " |              },\n",
      " |              {\n",
      " |                  \"name\": \"GetPopulation\",\n",
      " |                  \"args\": {\"location\": \"New York, NY\"},\n",
      " |                  \"id\": \"call_6ghfKxV264jEfe1mRIkS3PE7\",\n",
      " |              },\n",
      " |          ]\n",
      " |\n",
      " |      .. note::\n",
      " |          ``openai >= 1.32`` supports a ``parallel_tool_calls`` parameter\n",
      " |          that defaults to ``True``. This parameter can be set to ``False`` to\n",
      " |          disable parallel tool calls:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              ai_msg = llm_with_tools.invoke(\n",
      " |                  \"What is the weather in LA and NY?\", parallel_tool_calls=False\n",
      " |              )\n",
      " |              ai_msg.tool_calls\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              [\n",
      " |                  {\n",
      " |                      \"name\": \"GetWeather\",\n",
      " |                      \"args\": {\"location\": \"Los Angeles, CA\"},\n",
      " |                      \"id\": \"call_4OoY0ZR99iEvC7fevsH8Uhtz\",\n",
      " |                  }\n",
      " |              ]\n",
      " |\n",
      " |      Like other runtime parameters, ``parallel_tool_calls`` can be bound to a model\n",
      " |      using ``llm.bind(parallel_tool_calls=False)`` or during instantiation by\n",
      " |      setting ``model_kwargs``.\n",
      " |\n",
      " |      See ``ChatOpenAI.bind_tools()`` method for more.\n",
      " |\n",
      " |  .. dropdown:: Built-in tools\n",
      " |\n",
      " |      .. versionadded:: 0.3.9\n",
      " |\n",
      " |      You can access `built-in tools <https://platform.openai.com/docs/guides/tools?api-mode=responses>`_\n",
      " |      supported by the OpenAI Responses API. See LangChain\n",
      " |      `docs <https://python.langchain.com/docs/integrations/chat/openai/>`__ for more\n",
      " |      detail.\n",
      " |\n",
      " |      .. note::\n",
      " |          ``langchain-openai >= 0.3.26`` allows users to opt-in to an updated\n",
      " |          AIMessage format when using the Responses API. Setting\n",
      " |\n",
      " |          ..  code-block:: python\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"...\", output_version=\"responses/v1\")\n",
      " |\n",
      " |          will format output from reasoning summaries, built-in tool invocations, and\n",
      " |          other response items into the message's ``content`` field, rather than\n",
      " |          ``additional_kwargs``. We recommend this format for new applications.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          llm = ChatOpenAI(model=\"gpt-4.1-mini\", output_version=\"responses/v1\")\n",
      " |\n",
      " |          tool = {\"type\": \"web_search\"}\n",
      " |          llm_with_tools = llm.bind_tools([tool])\n",
      " |\n",
      " |          response = llm_with_tools.invoke(\n",
      " |              \"What was a positive news story from today?\"\n",
      " |          )\n",
      " |          response.content\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          [\n",
      " |              {\n",
      " |                  \"type\": \"text\",\n",
      " |                  \"text\": \"Today, a heartwarming story emerged from ...\",\n",
      " |                  \"annotations\": [\n",
      " |                      {\n",
      " |                          \"end_index\": 778,\n",
      " |                          \"start_index\": 682,\n",
      " |                          \"title\": \"Title of story\",\n",
      " |                          \"type\": \"url_citation\",\n",
      " |                          \"url\": \"<url of story>\",\n",
      " |                      }\n",
      " |                  ],\n",
      " |              }\n",
      " |          ]\n",
      " |\n",
      " |  .. dropdown:: Managing conversation state\n",
      " |\n",
      " |      .. versionadded:: 0.3.9\n",
      " |\n",
      " |      OpenAI's Responses API supports management of\n",
      " |      `conversation state <https://platform.openai.com/docs/guides/conversation-state?api-mode=responses>`_.\n",
      " |      Passing in response IDs from previous messages will continue a conversational\n",
      " |      thread. See LangChain\n",
      " |      `conversation docs <https://python.langchain.com/docs/integrations/chat/openai/>`__ for more\n",
      " |      detail.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          llm = ChatOpenAI(\n",
      " |              model=\"gpt-4.1-mini\",\n",
      " |              use_responses_api=True,\n",
      " |              output_version=\"responses/v1\",\n",
      " |          )\n",
      " |          response = llm.invoke(\"Hi, I'm Bob.\")\n",
      " |          response.text()\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          \"Hi Bob! How can I assist you today?\"\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          second_response = llm.invoke(\n",
      " |              \"What is my name?\",\n",
      " |              previous_response_id=response.response_metadata[\"id\"],\n",
      " |          )\n",
      " |          second_response.text()\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          \"Your name is Bob. How can I help you today, Bob?\"\n",
      " |\n",
      " |      .. versionadded:: 0.3.26\n",
      " |\n",
      " |      You can also initialize ChatOpenAI with :attr:`use_previous_response_id`.\n",
      " |      Input messages up to the most recent response will then be dropped from request\n",
      " |      payloads, and ``previous_response_id`` will be set using the ID of the most\n",
      " |      recent response.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          llm = ChatOpenAI(model=\"gpt-4.1-mini\", use_previous_response_id=True)\n",
      " |\n",
      " |  .. dropdown:: Reasoning output\n",
      " |\n",
      " |      OpenAI's Responses API supports `reasoning models <https://platform.openai.com/docs/guides/reasoning?api-mode=responses>`_\n",
      " |      that expose a summary of internal reasoning processes.\n",
      " |\n",
      " |      .. note::\n",
      " |          ``langchain-openai >= 0.3.26`` allows users to opt-in to an updated\n",
      " |          AIMessage format when using the Responses API. Setting\n",
      " |\n",
      " |          ..  code-block:: python\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"...\", output_version=\"responses/v1\")\n",
      " |\n",
      " |          will format output from reasoning summaries, built-in tool invocations, and\n",
      " |          other response items into the message's ``content`` field, rather than\n",
      " |          ``additional_kwargs``. We recommend this format for new applications.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          reasoning = {\n",
      " |              \"effort\": \"medium\",  # 'low', 'medium', or 'high'\n",
      " |              \"summary\": \"auto\",  # 'detailed', 'auto', or None\n",
      " |          }\n",
      " |\n",
      " |          llm = ChatOpenAI(\n",
      " |              model=\"o4-mini\", reasoning=reasoning, output_version=\"responses/v1\"\n",
      " |          )\n",
      " |          response = llm.invoke(\"What is 3^3?\")\n",
      " |\n",
      " |          # Response text\n",
      " |          print(f\"Output: {response.text()}\")\n",
      " |\n",
      " |          # Reasoning summaries\n",
      " |          for block in response.content:\n",
      " |              if block[\"type\"] == \"reasoning\":\n",
      " |                  for summary in block[\"summary\"]:\n",
      " |                      print(summary[\"text\"])\n",
      " |\n",
      " |      .. code-block::\n",
      " |\n",
      " |          Output: 3³ = 27\n",
      " |          Reasoning: The user wants to know...\n",
      " |\n",
      " |  .. dropdown:: Structured output\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import Optional\n",
      " |\n",
      " |          from pydantic import BaseModel, Field\n",
      " |\n",
      " |\n",
      " |          class Joke(BaseModel):\n",
      " |              '''Joke to tell user.'''\n",
      " |\n",
      " |              setup: str = Field(description=\"The setup of the joke\")\n",
      " |              punchline: str = Field(description=\"The punchline to the joke\")\n",
      " |              rating: Optional[int] = Field(\n",
      " |                  description=\"How funny the joke is, from 1 to 10\"\n",
      " |              )\n",
      " |\n",
      " |\n",
      " |          structured_llm = llm.with_structured_output(Joke)\n",
      " |          structured_llm.invoke(\"Tell me a joke about cats\")\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          Joke(\n",
      " |              setup=\"Why was the cat sitting on the computer?\",\n",
      " |              punchline=\"To keep an eye on the mouse!\",\n",
      " |              rating=None,\n",
      " |          )\n",
      " |\n",
      " |      See ``ChatOpenAI.with_structured_output()`` for more.\n",
      " |\n",
      " |  .. dropdown:: JSON mode\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          json_llm = llm.bind(response_format={\"type\": \"json_object\"})\n",
      " |          ai_msg = json_llm.invoke(\n",
      " |              \"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\"\n",
      " |          )\n",
      " |          ai_msg.content\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          '\\\\n{\\\\n  \"random_ints\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\\\\n}'\n",
      " |\n",
      " |  .. dropdown:: Image input\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          import base64\n",
      " |          import httpx\n",
      " |          from langchain_core.messages import HumanMessage\n",
      " |\n",
      " |          image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
      " |          image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
      " |          message = HumanMessage(\n",
      " |              content=[\n",
      " |                  {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
      " |                  {\n",
      " |                      \"type\": \"image_url\",\n",
      " |                      \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
      " |                  },\n",
      " |              ]\n",
      " |          )\n",
      " |          ai_msg = llm.invoke([message])\n",
      " |          ai_msg.content\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          \"The weather in the image appears to be clear and pleasant. The sky is mostly blue with scattered, light clouds, suggesting a sunny day with minimal cloud cover. There is no indication of rain or strong winds, and the overall scene looks bright and calm. The lush green grass and clear visibility further indicate good weather conditions.\"\n",
      " |\n",
      " |  .. dropdown:: Token usage\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          ai_msg = llm.invoke(messages)\n",
      " |          ai_msg.usage_metadata\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n",
      " |\n",
      " |      When streaming, set the ``stream_usage`` kwarg:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          stream = llm.stream(messages, stream_usage=True)\n",
      " |          full = next(stream)\n",
      " |          for chunk in stream:\n",
      " |              full += chunk\n",
      " |          full.usage_metadata\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n",
      " |\n",
      " |      Alternatively, setting ``stream_usage`` when instantiating the model can be\n",
      " |      useful when incorporating ``ChatOpenAI`` into LCEL chains-- or when using\n",
      " |      methods like ``.with_structured_output``, which generate chains under the\n",
      " |      hood.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          llm = ChatOpenAI(model=\"gpt-4o\", stream_usage=True)\n",
      " |          structured_llm = llm.with_structured_output(...)\n",
      " |\n",
      " |  .. dropdown:: Logprobs\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          logprobs_llm = llm.bind(logprobs=True)\n",
      " |          ai_msg = logprobs_llm.invoke(messages)\n",
      " |          ai_msg.response_metadata[\"logprobs\"]\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          {\n",
      " |              \"content\": [\n",
      " |                  {\n",
      " |                      \"token\": \"J\",\n",
      " |                      \"bytes\": [74],\n",
      " |                      \"logprob\": -4.9617593e-06,\n",
      " |                      \"top_logprobs\": [],\n",
      " |                  },\n",
      " |                  {\n",
      " |                      \"token\": \"'adore\",\n",
      " |                      \"bytes\": [39, 97, 100, 111, 114, 101],\n",
      " |                      \"logprob\": -0.25202933,\n",
      " |                      \"top_logprobs\": [],\n",
      " |                  },\n",
      " |                  {\n",
      " |                      \"token\": \" la\",\n",
      " |                      \"bytes\": [32, 108, 97],\n",
      " |                      \"logprob\": -0.20141791,\n",
      " |                      \"top_logprobs\": [],\n",
      " |                  },\n",
      " |                  {\n",
      " |                      \"token\": \" programmation\",\n",
      " |                      \"bytes\": [\n",
      " |                          32,\n",
      " |                          112,\n",
      " |                          114,\n",
      " |                          111,\n",
      " |                          103,\n",
      " |                          114,\n",
      " |                          97,\n",
      " |                          109,\n",
      " |                          109,\n",
      " |                          97,\n",
      " |                          116,\n",
      " |                          105,\n",
      " |                          111,\n",
      " |                          110,\n",
      " |                      ],\n",
      " |                      \"logprob\": -1.9361265e-07,\n",
      " |                      \"top_logprobs\": [],\n",
      " |                  },\n",
      " |                  {\n",
      " |                      \"token\": \".\",\n",
      " |                      \"bytes\": [46],\n",
      " |                      \"logprob\": -1.2233183e-05,\n",
      " |                      \"top_logprobs\": [],\n",
      " |                  },\n",
      " |              ]\n",
      " |          }\n",
      " |\n",
      " |  .. dropdown:: Response metadata\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          ai_msg = llm.invoke(messages)\n",
      " |          ai_msg.response_metadata\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          {\n",
      " |              \"token_usage\": {\n",
      " |                  \"completion_tokens\": 5,\n",
      " |                  \"prompt_tokens\": 28,\n",
      " |                  \"total_tokens\": 33,\n",
      " |              },\n",
      " |              \"model_name\": \"gpt-4o\",\n",
      " |              \"system_fingerprint\": \"fp_319be4768e\",\n",
      " |              \"finish_reason\": \"stop\",\n",
      " |              \"logprobs\": None,\n",
      " |          }\n",
      " |\n",
      " |  .. dropdown:: Flex processing\n",
      " |\n",
      " |      OpenAI offers a variety of\n",
      " |      `service tiers <https://platform.openai.com/docs/guides/flex-processing>`_.\n",
      " |      The \"flex\" tier offers cheaper pricing for requests, with the trade-off that\n",
      " |      responses may take longer and resources might not always be available.\n",
      " |      This approach is best suited for non-critical tasks, including model testing,\n",
      " |      data enhancement, or jobs that can be run asynchronously.\n",
      " |\n",
      " |      To use it, initialize the model with ``service_tier=\"flex\"``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          llm = ChatOpenAI(model=\"o4-mini\", service_tier=\"flex\")\n",
      " |\n",
      " |      Note that this is a beta feature that is only available for a subset of models.\n",
      " |      See OpenAI `flex processing docs <https://platform.openai.com/docs/guides/flex-processing>`__\n",
      " |      for more detail.\n",
      " |\n",
      " |  .. dropdown:: OpenAI-compatible APIs\n",
      " |\n",
      " |      ``ChatOpenAI`` can be used with OpenAI-compatible APIs like `LM Studio <https://lmstudio.ai/>`__,\n",
      " |      `vLLM <https://github.com/vllm-project/vllm>`__,\n",
      " |      `Ollama <https://ollama.com/>`__, and others.\n",
      " |      To use custom parameters specific to these providers, use the ``extra_body`` parameter.\n",
      " |\n",
      " |      **LM Studio example** with TTL (auto-eviction):\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          llm = ChatOpenAI(\n",
      " |              base_url=\"http://localhost:1234/v1\",\n",
      " |              api_key=\"lm-studio\",  # Can be any string\n",
      " |              model=\"mlx-community/QwQ-32B-4bit\",\n",
      " |              temperature=0,\n",
      " |              extra_body={\n",
      " |                  \"ttl\": 300\n",
      " |              },  # Auto-evict model after 5 minutes of inactivity\n",
      " |          )\n",
      " |\n",
      " |      **vLLM example** with custom parameters:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          llm = ChatOpenAI(\n",
      " |              base_url=\"http://localhost:8000/v1\",\n",
      " |              api_key=\"EMPTY\",\n",
      " |              model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
      " |              extra_body={\"use_beam_search\": True, \"best_of\": 4},\n",
      " |          )\n",
      " |\n",
      " |  .. dropdown:: model_kwargs vs extra_body\n",
      " |\n",
      " |      Use the correct parameter for different types of API arguments:\n",
      " |\n",
      " |      **Use ``model_kwargs`` for:**\n",
      " |\n",
      " |      - Standard OpenAI API parameters not explicitly defined as class parameters\n",
      " |      - Parameters that should be flattened into the top-level request payload\n",
      " |      - Examples: ``max_completion_tokens``, ``stream_options``, ``modalities``, ``audio``\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          # Standard OpenAI parameters\n",
      " |          llm = ChatOpenAI(\n",
      " |              model=\"gpt-4o\",\n",
      " |              model_kwargs={\n",
      " |                  \"stream_options\": {\"include_usage\": True},\n",
      " |                  \"max_completion_tokens\": 300,\n",
      " |                  \"modalities\": [\"text\", \"audio\"],\n",
      " |                  \"audio\": {\"voice\": \"alloy\", \"format\": \"wav\"},\n",
      " |              },\n",
      " |          )\n",
      " |\n",
      " |      **Use ``extra_body`` for:**\n",
      " |\n",
      " |      - Custom parameters specific to OpenAI-compatible providers (vLLM, LM Studio, etc.)\n",
      " |      - Parameters that need to be nested under ``extra_body`` in the request\n",
      " |      - Any non-standard OpenAI API parameters\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          # Custom provider parameters\n",
      " |          llm = ChatOpenAI(\n",
      " |              base_url=\"http://localhost:8000/v1\",\n",
      " |              model=\"custom-model\",\n",
      " |              extra_body={\n",
      " |                  \"use_beam_search\": True,  # vLLM parameter\n",
      " |                  \"best_of\": 4,  # vLLM parameter\n",
      " |                  \"ttl\": 300,  # LM Studio parameter\n",
      " |              },\n",
      " |          )\n",
      " |\n",
      " |      **Key Differences:**\n",
      " |\n",
      " |      - ``model_kwargs``: Parameters are **merged into top-level** request payload\n",
      " |      - ``extra_body``: Parameters are **nested under ``extra_body``** key in request\n",
      " |\n",
      " |      .. important::\n",
      " |          Always use ``extra_body`` for custom parameters, **not** ``model_kwargs``.\n",
      " |          Using ``model_kwargs`` for non-OpenAI parameters will cause API errors.\n",
      " |\n",
      " |  .. dropdown:: Prompt caching optimization\n",
      " |\n",
      " |      For high-volume applications with repetitive prompts, use ``prompt_cache_key``\n",
      " |      per-invocation to improve cache hit rates and reduce costs:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
      " |\n",
      " |          response = llm.invoke(\n",
      " |              messages,\n",
      " |              prompt_cache_key=\"example-key-a\",  # Routes to same machine for cache hits\n",
      " |          )\n",
      " |\n",
      " |          customer_response = llm.invoke(messages, prompt_cache_key=\"example-key-b\")\n",
      " |          support_response = llm.invoke(messages, prompt_cache_key=\"example-key-c\")\n",
      " |\n",
      " |          # Dynamic cache keys based on context\n",
      " |          cache_key = f\"example-key-{dynamic_suffix}\"\n",
      " |          response = llm.invoke(messages, prompt_cache_key=cache_key)\n",
      " |\n",
      " |      Cache keys help ensure requests with the same prompt prefix are routed to\n",
      " |      machines with existing cache, providing cost reduction and latency improvement on\n",
      " |      cached tokens.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      ChatOpenAI\n",
      " |      BaseChatOpenAI\n",
      " |      langchain_core.language_models.chat_models.BaseChatModel\n",
      " |      langchain_core.language_models.base.BaseLanguageModel[BaseMessage]\n",
      " |      langchain_core.language_models.base.BaseLanguageModel\n",
      " |      langchain_core.runnables.base.RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], TypeVar]\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      abc.ABC\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  with_structured_output(\n",
      " |      self,\n",
      " |      schema: 'Optional[_DictOrPydanticClass]' = None,\n",
      " |      *,\n",
      " |      method: \"Literal['function_calling', 'json_mode', 'json_schema']\" = 'json_schema',\n",
      " |      include_raw: 'bool' = False,\n",
      " |      strict: 'Optional[bool]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'Runnable[LanguageModelInput, _DictOrPydantic]'\n",
      " |      Model wrapper that returns outputs formatted to match the given schema.\n",
      " |\n",
      " |      Args:\n",
      " |          schema: The output schema. Can be passed in as:\n",
      " |\n",
      " |              - a JSON Schema,\n",
      " |              - a TypedDict class,\n",
      " |              - or a Pydantic class,\n",
      " |              - an OpenAI function/tool schema.\n",
      " |\n",
      " |              If ``schema`` is a Pydantic class then the model output will be a\n",
      " |              Pydantic instance of that class, and the model-generated fields will be\n",
      " |              validated by the Pydantic class. Otherwise the model output will be a\n",
      " |              dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n",
      " |              for more on how to properly specify types and descriptions of\n",
      " |              schema fields when specifying a Pydantic or TypedDict class.\n",
      " |\n",
      " |          method: The method for steering model generation, one of:\n",
      " |\n",
      " |              - ``'json_schema'``:\n",
      " |                  Uses OpenAI's `Structured Output API <https://platform.openai.com/docs/guides/structured-outputs>`__.\n",
      " |                  Supported for ``'gpt-4o-mini'``, ``'gpt-4o-2024-08-06'``, ``'o1'``, and later\n",
      " |                  models.\n",
      " |              - ``'function_calling'``:\n",
      " |                  Uses OpenAI's tool-calling (formerly called function calling)\n",
      " |                  `API <https://platform.openai.com/docs/guides/function-calling>`__\n",
      " |              - ``'json_mode'``:\n",
      " |                  Uses OpenAI's `JSON mode <https://platform.openai.com/docs/guides/structured-outputs/json-mode>`__.\n",
      " |                  Note that if using JSON mode then you must include instructions for\n",
      " |                  formatting the output into the desired schema into the model call\n",
      " |\n",
      " |              Learn more about the differences between the methods and which models\n",
      " |              support which methods `here <https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format>`__.\n",
      " |\n",
      " |          include_raw:\n",
      " |              If False then only the parsed structured output is returned. If\n",
      " |              an error occurs during model output parsing it will be raised. If True\n",
      " |              then both the raw model response (a BaseMessage) and the parsed model\n",
      " |              response will be returned. If an error occurs during output parsing it\n",
      " |              will be caught and returned as well. The final output is always a dict\n",
      " |              with keys ``'raw'``, ``'parsed'``, and ``'parsing_error'``.\n",
      " |          strict:\n",
      " |\n",
      " |              - True:\n",
      " |                  Model output is guaranteed to exactly match the schema.\n",
      " |                  The input schema will also be validated according to the `supported schemas <https://platform.openai.com/docs/guides/structured-outputs/supported-schemas?api-mode=responses#supported-schemas>`__.\n",
      " |              - False:\n",
      " |                  Input schema will not be validated and model output will not be\n",
      " |                  validated.\n",
      " |              - None:\n",
      " |                  ``strict`` argument will not be passed to the model.\n",
      " |\n",
      " |              If schema is specified via TypedDict or JSON schema, ``strict`` is not\n",
      " |              enabled by default. Pass ``strict=True`` to enable it.\n",
      " |\n",
      " |              .. note::\n",
      " |                  ``strict`` can only be non-null if ``method`` is ``'json_schema'`` or ``'function_calling'``.\n",
      " |          tools:\n",
      " |              A list of tool-like objects to bind to the chat model. Requires that:\n",
      " |\n",
      " |              - ``method`` is ``'json_schema'`` (default).\n",
      " |              - ``strict=True``\n",
      " |              - ``include_raw=True``\n",
      " |\n",
      " |              If a model elects to call a\n",
      " |              tool, the resulting ``AIMessage`` in ``'raw'`` will include tool calls.\n",
      " |\n",
      " |              .. dropdown:: Example\n",
      " |\n",
      " |                  .. code-block:: python\n",
      " |\n",
      " |                      from langchain.chat_models import init_chat_model\n",
      " |                      from pydantic import BaseModel\n",
      " |\n",
      " |\n",
      " |                      class ResponseSchema(BaseModel):\n",
      " |                          response: str\n",
      " |\n",
      " |\n",
      " |                      def get_weather(location: str) -> str:\n",
      " |                          \\\"\\\"\\\"Get weather at a location.\\\"\\\"\\\"\n",
      " |                          pass\n",
      " |\n",
      " |                      llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
      " |\n",
      " |                      structured_llm = llm.with_structured_output(\n",
      " |                          ResponseSchema,\n",
      " |                          tools=[get_weather],\n",
      " |                          strict=True,\n",
      " |                          include_raw=True,\n",
      " |                      )\n",
      " |\n",
      " |                      structured_llm.invoke(\"What's the weather in Boston?\")\n",
      " |\n",
      " |                  .. code-block:: python\n",
      " |\n",
      " |                      {\n",
      " |                          \"raw\": AIMessage(content=\"\", tool_calls=[...], ...),\n",
      " |                          \"parsing_error\": None,\n",
      " |                          \"parsed\": None,\n",
      " |                      }\n",
      " |\n",
      " |          kwargs: Additional keyword args are passed through to the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n",
      " |\n",
      " |          If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs\n",
      " |          an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n",
      " |\n",
      " |          If ``include_raw`` is True, then Runnable outputs a dict with keys:\n",
      " |\n",
      " |          - ``'raw'``: BaseMessage\n",
      " |          - ``'parsed'``: None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n",
      " |          - ``'parsing_error'``: Optional[BaseException]\n",
      " |\n",
      " |      .. versionchanged:: 0.1.20\n",
      " |\n",
      " |          Added support for TypedDict class ``schema``.\n",
      " |\n",
      " |      .. versionchanged:: 0.1.21\n",
      " |\n",
      " |          Support for ``strict`` argument added.\n",
      " |          Support for ``method=\"json_schema\"`` added.\n",
      " |\n",
      " |      .. versionchanged:: 0.3.0\n",
      " |\n",
      " |          ``method`` default changed from \"function_calling\" to \"json_schema\".\n",
      " |\n",
      " |      .. versionchanged:: 0.3.12\n",
      " |          Support for ``tools`` added.\n",
      " |\n",
      " |      .. versionchanged:: 0.3.21\n",
      " |          Pass ``kwargs`` through to the model.\n",
      " |\n",
      " |      .. dropdown:: Example: schema=Pydantic class, method=\"json_schema\", include_raw=False, strict=True\n",
      " |\n",
      " |          Note, OpenAI has a number of restrictions on what types of schemas can be\n",
      " |          provided if ``strict`` = True. When using Pydantic, our model cannot\n",
      " |          specify any Field metadata (like min/max constraints) and fields cannot\n",
      " |          have default values.\n",
      " |\n",
      " |          See all constraints `here <https://platform.openai.com/docs/guides/structured-outputs/supported-schemas>`__.\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from typing import Optional\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |              from pydantic import BaseModel, Field\n",
      " |\n",
      " |\n",
      " |              class AnswerWithJustification(BaseModel):\n",
      " |                  '''An answer to the user question along with justification for the answer.'''\n",
      " |\n",
      " |                  answer: str\n",
      " |                  justification: Optional[str] = Field(\n",
      " |                      default=..., description=\"A justification for the answer.\"\n",
      " |                  )\n",
      " |\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |\n",
      " |              # -> AnswerWithJustification(\n",
      " |              #     answer='They weigh the same',\n",
      " |              #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n",
      " |              # )\n",
      " |\n",
      " |      .. dropdown:: Example: schema=Pydantic class, method=\"function_calling\", include_raw=False, strict=False\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from typing import Optional\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |              from pydantic import BaseModel, Field\n",
      " |\n",
      " |\n",
      " |              class AnswerWithJustification(BaseModel):\n",
      " |                  '''An answer to the user question along with justification for the answer.'''\n",
      " |\n",
      " |                  answer: str\n",
      " |                  justification: Optional[str] = Field(\n",
      " |                      default=..., description=\"A justification for the answer.\"\n",
      " |                  )\n",
      " |\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(\n",
      " |                  AnswerWithJustification, method=\"function_calling\"\n",
      " |              )\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |\n",
      " |              # -> AnswerWithJustification(\n",
      " |              #     answer='They weigh the same',\n",
      " |              #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n",
      " |              # )\n",
      " |\n",
      " |      .. dropdown:: Example: schema=Pydantic class, method=\"json_schema\", include_raw=True\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |              from pydantic import BaseModel\n",
      " |\n",
      " |\n",
      " |              class AnswerWithJustification(BaseModel):\n",
      " |                  '''An answer to the user question along with justification for the answer.'''\n",
      " |\n",
      " |                  answer: str\n",
      " |                  justification: str\n",
      " |\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(\n",
      " |                  AnswerWithJustification, include_raw=True\n",
      " |              )\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n",
      " |              #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n",
      " |              #     'parsing_error': None\n",
      " |              # }\n",
      " |\n",
      " |      .. dropdown:: Example: schema=TypedDict class, method=\"json_schema\", include_raw=False, strict=False\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              # IMPORTANT: If you are using Python <=3.8, you need to import Annotated\n",
      " |              # from typing_extensions, not from typing.\n",
      " |              from typing_extensions import Annotated, TypedDict\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |\n",
      " |              class AnswerWithJustification(TypedDict):\n",
      " |                  '''An answer to the user question along with justification for the answer.'''\n",
      " |\n",
      " |                  answer: str\n",
      " |                  justification: Annotated[\n",
      " |                      Optional[str], None, \"A justification for the answer.\"\n",
      " |                  ]\n",
      " |\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'answer': 'They weigh the same',\n",
      " |              #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
      " |              # }\n",
      " |\n",
      " |      .. dropdown:: Example: schema=OpenAI function schema, method=\"json_schema\", include_raw=False\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |              oai_schema = {\n",
      " |                  'name': 'AnswerWithJustification',\n",
      " |                  'description': 'An answer to the user question along with justification for the answer.',\n",
      " |                  'parameters': {\n",
      " |                      'type': 'object',\n",
      " |                      'properties': {\n",
      " |                          'answer': {'type': 'string'},\n",
      " |                          'justification': {'description': 'A justification for the answer.', 'type': 'string'}\n",
      " |                      },\n",
      " |                     'required': ['answer']\n",
      " |                 }\n",
      " |             }\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(oai_schema)\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'answer': 'They weigh the same',\n",
      " |              #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
      " |              # }\n",
      " |\n",
      " |      .. dropdown:: Example: schema=Pydantic class, method=\"json_mode\", include_raw=True\n",
      " |\n",
      " |          .. code-block::\n",
      " |\n",
      " |              from langchain_openai import ChatOpenAI\n",
      " |              from pydantic import BaseModel\n",
      " |\n",
      " |              class AnswerWithJustification(BaseModel):\n",
      " |                  answer: str\n",
      " |                  justification: str\n",
      " |\n",
      " |              llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(\n",
      " |                  AnswerWithJustification,\n",
      " |                  method=\"json_mode\",\n",
      " |                  include_raw=True\n",
      " |              )\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"Answer the following question. \"\n",
      " |                  \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n",
      " |                  \"What's heavier a pound of bricks or a pound of feathers?\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n",
      " |              #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n",
      " |              #     'parsing_error': None\n",
      " |              # }\n",
      " |\n",
      " |      .. dropdown:: Example: schema=None, method=\"json_mode\", include_raw=True\n",
      " |\n",
      " |          .. code-block::\n",
      " |\n",
      " |              structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n",
      " |\n",
      " |              structured_llm.invoke(\n",
      " |                  \"Answer the following question. \"\n",
      " |                  \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n",
      " |                  \"What's heavier a pound of bricks or a pound of feathers?\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n",
      " |              #     'parsed': {\n",
      " |              #         'answer': 'They are both the same weight.',\n",
      " |              #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'\n",
      " |              #     },\n",
      " |              #     'parsing_error': None\n",
      " |              # }\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  get_lc_namespace() -> 'list[str]'\n",
      " |      Get the namespace of the langchain object.\n",
      " |\n",
      " |  is_lc_serializable() -> 'bool'\n",
      " |      Return whether this model can be serialized by LangChain.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  lc_attributes\n",
      " |      Get the attributes of the langchain object.\n",
      " |\n",
      " |  lc_secrets\n",
      " |      Mapping of secret environment variables.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'max_tokens': 'Optional[int]'}\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __parameters__ = ()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'function': {'function': <function BaseCha...\n",
      " |\n",
      " |  __pydantic_custom_init__ = True\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_fields__ = {'async_client': FieldInfo(annotation=Any, requi...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_setattr_handlers__ = {}\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"ChatOpenAI\", validator...\n",
      " |\n",
      " |  __signature__ = <Signature (*args: Any, name: Optional[str] = No...n: ...\n",
      " |\n",
      " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'ignore', 'p...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseChatOpenAI:\n",
      " |\n",
      " |  bind_functions(\n",
      " |      self,\n",
      " |      functions: 'Sequence[Union[dict[str, Any], type[BaseModel], Callable, BaseTool]]',\n",
      " |      function_call: \"Optional[Union[_FunctionCall, str, Literal['auto', 'none']]]\" = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
      " |      .. deprecated:: 0.2.1 Use :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind_tools` instead. It will not be removed until langchain-openai==1.0.0.\n",
      " |\n",
      " |      Bind functions (and other objects) to this chat model.\n",
      " |\n",
      " |      Assumes model is compatible with OpenAI function-calling API.\n",
      " |\n",
      " |      .. note::\n",
      " |          Using ``bind_tools()`` is recommended instead, as the ``functions`` and\n",
      " |          ``function_call`` request parameters are officially marked as deprecated by\n",
      " |          OpenAI.\n",
      " |\n",
      " |      Args:\n",
      " |          functions: A list of function definitions to bind to this chat model.\n",
      " |              Can be  a dictionary, pydantic model, or callable. Pydantic\n",
      " |              models and callables will be automatically converted to\n",
      " |              their schema dictionary representation.\n",
      " |          function_call: Which function to require the model to call.\n",
      " |              Must be the name of the single provided function or\n",
      " |              ``'auto'`` to automatically determine which function to call\n",
      " |              (if any).\n",
      " |          **kwargs: Any additional parameters to pass to the\n",
      " |              :class:`~langchain.runnable.Runnable` constructor.\n",
      " |\n",
      " |  bind_tools(\n",
      " |      self,\n",
      " |      tools: 'Sequence[Union[dict[str, Any], type, Callable, BaseTool]]',\n",
      " |      *,\n",
      " |      tool_choice: \"Optional[Union[dict, str, Literal['auto', 'none', 'required', 'any'], bool]]\" = None,\n",
      " |      strict: 'Optional[bool]' = None,\n",
      " |      parallel_tool_calls: 'Optional[bool]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
      " |      Bind tool-like objects to this chat model.\n",
      " |\n",
      " |      Assumes model is compatible with OpenAI tool-calling API.\n",
      " |\n",
      " |      Args:\n",
      " |          tools: A list of tool definitions to bind to this chat model.\n",
      " |              Supports any tool definition handled by\n",
      " |              :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.\n",
      " |          tool_choice: Which tool to require the model to call. Options are:\n",
      " |\n",
      " |              - str of the form ``'<<tool_name>>'``: calls <<tool_name>> tool.\n",
      " |              - ``'auto'``: automatically selects a tool (including no tool).\n",
      " |              - ``'none'``: does not call a tool.\n",
      " |              - ``'any'`` or ``'required'`` or ``True``: force at least one tool to be called.\n",
      " |              - dict of the form ``{\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}``: calls <<tool_name>> tool.\n",
      " |              - ``False`` or ``None``: no effect, default OpenAI behavior.\n",
      " |          strict: If True, model output is guaranteed to exactly match the JSON Schema\n",
      " |              provided in the tool definition. The input schema will also be validated according to the\n",
      " |              `supported schemas <https://platform.openai.com/docs/guides/structured-outputs/supported-schemas?api-mode=responses#supported-schemas>`__.\n",
      " |              If False, input schema will not be validated and model output will not\n",
      " |              be validated.\n",
      " |              If None, ``strict`` argument will not be passed to the model.\n",
      " |          parallel_tool_calls: Set to ``False`` to disable parallel tool use.\n",
      " |              Defaults to ``None`` (no specification, which allows parallel tool use).\n",
      " |          kwargs: Any additional parameters are passed directly to\n",
      " |              :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind`.\n",
      " |\n",
      " |      .. versionchanged:: 0.1.21\n",
      " |\n",
      " |          Support for ``strict`` argument added.\n",
      " |\n",
      " |  get_num_tokens_from_messages(\n",
      " |      self,\n",
      " |      messages: 'list[BaseMessage]',\n",
      " |      tools: 'Optional[Sequence[Union[dict[str, Any], type, Callable, BaseTool]]]' = None\n",
      " |  ) -> 'int'\n",
      " |      Calculate num tokens for ``gpt-3.5-turbo`` and ``gpt-4`` with ``tiktoken`` package.\n",
      " |\n",
      " |      **Requirements**: You must have the ``pillow`` installed if you want to count\n",
      " |      image tokens if you are specifying the image as a base64 string, and you must\n",
      " |      have both ``pillow`` and ``httpx`` installed if you are specifying the image\n",
      " |      as a URL. If these aren't installed image inputs will be ignored in token\n",
      " |      counting.\n",
      " |\n",
      " |      `OpenAI reference <https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb>`__\n",
      " |\n",
      " |      Args:\n",
      " |          messages: The message inputs to tokenize.\n",
      " |          tools: If provided, sequence of dict, BaseModel, function, or BaseTools\n",
      " |              to be converted to tool schemas.\n",
      " |\n",
      " |  get_token_ids(self, text: 'str') -> 'list[int]'\n",
      " |      Get the tokens present in the text with tiktoken package.\n",
      " |\n",
      " |  validate_environment(self) -> 'Self'\n",
      " |      Validate that api key and python package exists in environment.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from BaseChatOpenAI:\n",
      " |\n",
      " |  build_extra(values: 'dict[str, Any]') -> 'Any'\n",
      " |      Build extra kwargs from additional params that were passed in.\n",
      " |\n",
      " |  validate_temperature(values: 'dict[str, Any]') -> 'Any'\n",
      " |      Validate temperature parameter for different models.\n",
      " |\n",
      " |      - o1 models only allow temperature=1\n",
      " |      - gpt-5 models (excluding gpt-5-chat) only allow temperature=1 or unset\n",
      " |        (defaults to 1)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |\n",
      " |  __call__(\n",
      " |      self,\n",
      " |      messages: 'list[BaseMessage]',\n",
      " |      stop: 'Optional[list[str]]' = None,\n",
      " |      callbacks: 'Callbacks' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'BaseMessage'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |\n",
      " |      Call the model.\n",
      " |\n",
      " |      Args:\n",
      " |          messages: List of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If the generation is not a chat generation.\n",
      " |\n",
      " |      Returns:\n",
      " |          The model output message.\n",
      " |\n",
      " |  async agenerate(\n",
      " |      self,\n",
      " |      messages: 'list[list[BaseMessage]]',\n",
      " |      stop: 'Optional[list[str]]' = None,\n",
      " |      callbacks: 'Callbacks' = None,\n",
      " |      *,\n",
      " |      tags: 'Optional[list[str]]' = None,\n",
      " |      metadata: 'Optional[dict[str, Any]]' = None,\n",
      " |      run_name: 'Optional[str]' = None,\n",
      " |      run_id: 'Optional[uuid.UUID]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |\n",
      " |      1. Take advantage of batched calls,\n",
      " |      2. Need more output from the model than just the top generated value,\n",
      " |      3. Are building chains that are agnostic to the underlying language model\n",
      " |         type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          tags: The tags to apply.\n",
      " |          metadata: The metadata to apply.\n",
      " |          run_name: The name of the run.\n",
      " |          run_id: The ID of the run.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |          prompt and additional model provider-specific output.\n",
      " |\n",
      " |  async agenerate_prompt(\n",
      " |      self,\n",
      " |      prompts: 'list[PromptValue]',\n",
      " |      stop: 'Optional[list[str]]' = None,\n",
      " |      callbacks: 'Callbacks' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts and return model generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |\n",
      " |      1. Take advantage of batched calls,\n",
      " |      2. Need more output from the model than just the top generated value,\n",
      " |      3. Are building chains that are agnostic to the underlying language model\n",
      " |         type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An ``LLMResult``, which contains a list of candidate Generations for each\n",
      " |          input prompt and additional model provider-specific output.\n",
      " |\n",
      " |  async ainvoke(\n",
      " |      self,\n",
      " |      input: 'LanguageModelInput',\n",
      " |      config: 'Optional[RunnableConfig]' = None,\n",
      " |      *,\n",
      " |      stop: 'Optional[list[str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'BaseMessage'\n",
      " |      Transform a single input into an output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``.\n",
      " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
      " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
      " |              for more details. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the ``Runnable``.\n",
      " |\n",
      " |  async apredict(\n",
      " |      self,\n",
      " |      text: 'str',\n",
      " |      *,\n",
      " |      stop: 'Optional[Sequence[str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~ainvoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |\n",
      " |  async apredict_messages(\n",
      " |      self,\n",
      " |      messages: 'list[BaseMessage]',\n",
      " |      *,\n",
      " |      stop: 'Optional[Sequence[str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'BaseMessage'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~ainvoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |\n",
      " |  async astream(\n",
      " |      self,\n",
      " |      input: 'LanguageModelInput',\n",
      " |      config: 'Optional[RunnableConfig]' = None,\n",
      " |      *,\n",
      " |      stop: 'Optional[list[str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'AsyncIterator[BaseMessageChunk]'\n",
      " |      Default implementation of ``astream``, which calls ``ainvoke``.\n",
      " |\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the ``Runnable``.\n",
      " |\n",
      " |  call_as_llm(\n",
      " |      self,\n",
      " |      message: 'str',\n",
      " |      stop: 'Optional[list[str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |\n",
      " |      Call the model.\n",
      " |\n",
      " |      Args:\n",
      " |          message: The input message.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          The model output string.\n",
      " |\n",
      " |  dict(self, **kwargs: 'Any') -> 'dict'\n",
      " |      Return a dictionary of the LLM.\n",
      " |\n",
      " |  generate(\n",
      " |      self,\n",
      " |      messages: 'list[list[BaseMessage]]',\n",
      " |      stop: 'Optional[list[str]]' = None,\n",
      " |      callbacks: 'Callbacks' = None,\n",
      " |      *,\n",
      " |      tags: 'Optional[list[str]]' = None,\n",
      " |      metadata: 'Optional[dict[str, Any]]' = None,\n",
      " |      run_name: 'Optional[str]' = None,\n",
      " |      run_id: 'Optional[uuid.UUID]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |\n",
      " |      1. Take advantage of batched calls,\n",
      " |      2. Need more output from the model than just the top generated value,\n",
      " |      3. Are building chains that are agnostic to the underlying language model\n",
      " |         type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          tags: The tags to apply.\n",
      " |          metadata: The metadata to apply.\n",
      " |          run_name: The name of the run.\n",
      " |          run_id: The ID of the run.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |          prompt and additional model provider-specific output.\n",
      " |\n",
      " |  generate_prompt(\n",
      " |      self,\n",
      " |      prompts: 'list[PromptValue]',\n",
      " |      stop: 'Optional[list[str]]' = None,\n",
      " |      callbacks: 'Callbacks' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |\n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |\n",
      " |      Use this method when you want to:\n",
      " |\n",
      " |      1. Take advantage of batched calls,\n",
      " |      2. Need more output from the model than just the top generated value,\n",
      " |      3. Are building chains that are agnostic to the underlying language model\n",
      " |         type (e.g., pure text completion models vs chat models).\n",
      " |\n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |\n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |          prompt and additional model provider-specific output.\n",
      " |\n",
      " |  invoke(\n",
      " |      self,\n",
      " |      input: 'LanguageModelInput',\n",
      " |      config: 'Optional[RunnableConfig]' = None,\n",
      " |      *,\n",
      " |      stop: 'Optional[list[str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'BaseMessage'\n",
      " |      Transform a single input into an output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``.\n",
      " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
      " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
      " |              for more details. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the ``Runnable``.\n",
      " |\n",
      " |  predict(\n",
      " |      self,\n",
      " |      text: 'str',\n",
      " |      *,\n",
      " |      stop: 'Optional[Sequence[str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |\n",
      " |      Predict the next message.\n",
      " |\n",
      " |      Args:\n",
      " |          text: The input message.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If the output is not a string.\n",
      " |\n",
      " |      Returns:\n",
      " |          The predicted output string.\n",
      " |\n",
      " |  predict_messages(\n",
      " |      self,\n",
      " |      messages: 'list[BaseMessage]',\n",
      " |      *,\n",
      " |      stop: 'Optional[Sequence[str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'BaseMessage'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |\n",
      " |  stream(\n",
      " |      self,\n",
      " |      input: 'LanguageModelInput',\n",
      " |      config: 'Optional[RunnableConfig]' = None,\n",
      " |      *,\n",
      " |      stop: 'Optional[list[str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'Iterator[BaseMessageChunk]'\n",
      " |      Default implementation of ``stream``, which calls ``invoke``.\n",
      " |\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the ``Runnable``.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |\n",
      " |  raise_deprecation(values: 'dict') -> 'Any'\n",
      " |      Emit deprecation warning if ``callback_manager`` is used.\n",
      " |\n",
      " |      Args:\n",
      " |          values (Dict): Values to validate.\n",
      " |\n",
      " |      Returns:\n",
      " |          Dict: Validated values.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |\n",
      " |  OutputType\n",
      " |      Get the output type for this runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |\n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |\n",
      " |      Useful for checking if an input fits in a model's context window.\n",
      " |\n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |\n",
      " |      Returns:\n",
      " |          The integer number of tokens in the text.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |\n",
      " |  set_verbose(verbose: 'Optional[bool]') -> 'bool'\n",
      " |      If verbose is None, set it.\n",
      " |\n",
      " |      This allows users to pass in None as verbose to access the global setting.\n",
      " |\n",
      " |      Args:\n",
      " |          verbose: The verbosity setting to use.\n",
      " |\n",
      " |      Returns:\n",
      " |          The verbosity setting to use.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |\n",
      " |  InputType\n",
      " |      Get the input type for this runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  configurable_alternatives(\n",
      " |      self,\n",
      " |      which: 'ConfigurableField',\n",
      " |      *,\n",
      " |      default_key: 'str' = 'default',\n",
      " |      prefix_keys: 'bool' = False,\n",
      " |      **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]'\n",
      " |  ) -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure alternatives for ``Runnables`` that can be set at runtime.\n",
      " |\n",
      " |      Args:\n",
      " |          which: The ``ConfigurableField`` instance that will be used to select the\n",
      " |              alternative.\n",
      " |          default_key: The default key to use if no alternative is selected.\n",
      " |              Defaults to ``'default'``.\n",
      " |          prefix_keys: Whether to prefix the keys with the ``ConfigurableField`` id.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: A dictionary of keys to ``Runnable`` instances or callables that\n",
      " |              return ``Runnable`` instances.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the alternatives configured.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_anthropic import ChatAnthropic\n",
      " |          from langchain_core.runnables.utils import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          model = ChatAnthropic(\n",
      " |              model_name=\"claude-3-7-sonnet-20250219\"\n",
      " |          ).configurable_alternatives(\n",
      " |              ConfigurableField(id=\"llm\"),\n",
      " |              default_key=\"anthropic\",\n",
      " |              openai=ChatOpenAI(),\n",
      " |          )\n",
      " |\n",
      " |          # uses the default model ChatAnthropic\n",
      " |          print(model.invoke(\"which organization created you?\").content)\n",
      " |\n",
      " |          # uses ChatOpenAI\n",
      " |          print(\n",
      " |              model.with_config(configurable={\"llm\": \"openai\"})\n",
      " |              .invoke(\"which organization created you?\")\n",
      " |              .content\n",
      " |          )\n",
      " |\n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure particular ``Runnable`` fields at runtime.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: A dictionary of ``ConfigurableField`` instances to configure.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If a configuration key is not found in the ``Runnable``.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the fields configured.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |\n",
      " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
      " |              max_tokens=ConfigurableField(\n",
      " |                  id=\"output_token_number\",\n",
      " |                  name=\"Max tokens in the output\",\n",
      " |                  description=\"The maximum number of tokens in the output\",\n",
      " |              )\n",
      " |          )\n",
      " |\n",
      " |          # max_tokens = 20\n",
      " |          print(\n",
      " |              \"max_tokens_20: \", model.invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |\n",
      " |          # max_tokens = 200\n",
      " |          print(\n",
      " |              \"max_tokens_200: \",\n",
      " |              model.with_config(configurable={\"output_token_number\": 200})\n",
      " |              .invoke(\"tell me something about chess\")\n",
      " |              .content,\n",
      " |          )\n",
      " |\n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the ``Runnable`` to JSON.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON-serializable representation of the ``Runnable``.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  __orig_bases__ = (<class 'langchain_core.load.serializable.Serializabl...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  __init__(self, *args: Any, **kwargs: Any) -> None\n",
      " |      # Remove default BaseModel init docstring.\n",
      " |\n",
      " |  __repr_args__(self) -> Any\n",
      " |\n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |      Serialize a \"not implemented\" object.\n",
      " |\n",
      " |      Returns:\n",
      " |          SerializedNotImplemented.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  lc_id() -> list[str]\n",
      " |      Return a unique identifier for this class for serialization purposes.\n",
      " |\n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |      For example, for the class `langchain.llms.openai.OpenAI`, the id is\n",
      " |      [\"langchain\", \"llms\", \"openai\", \"OpenAI\"].\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'Callable[[Any], Any]', **kwargs: 'Any') -> 'Generator[Any]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Returns the string representation of a recursive object.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'AbstractSetIntStr | MappingIntStrAny | None' = None,\n",
      " |      exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None,\n",
      " |      update: 'Dict[str, Any] | None' = None,\n",
      " |      deep: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  json(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      by_alias: 'bool' = False,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      encoder: 'Callable[[Any], Any] | None' = PydanticUndefined,\n",
      " |      models_as_dict: 'bool' = PydanticUndefined,\n",
      " |      **dumps_kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |\n",
      " |  model_copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      update: 'Mapping[str, Any] | None' = None,\n",
      " |      deep: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_copy`](../concepts/models.md#model-copy)\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! note\n",
      " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
      " |          might have unexpected side effects if you store anything in it, on top of the model\n",
      " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(\n",
      " |      self,\n",
      " |      *,\n",
      " |      mode: \"Literal['json', 'python'] | str\" = 'python',\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      exclude_computed_fields: 'bool' = False,\n",
      " |      round_trip: 'bool' = False,\n",
      " |      warnings: \"bool | Literal['none', 'warn', 'error']\" = True,\n",
      " |      fallback: 'Callable[[Any], Any] | None' = None,\n",
      " |      serialize_as_any: 'bool' = False\n",
      " |  ) -> 'dict[str, Any]'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump`](../concepts/serialization.md#python-mode)\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended tu use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(\n",
      " |      self,\n",
      " |      *,\n",
      " |      indent: 'int | None' = None,\n",
      " |      ensure_ascii: 'bool' = False,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      exclude_computed_fields: 'bool' = False,\n",
      " |      round_trip: 'bool' = False,\n",
      " |      warnings: \"bool | Literal['none', 'warn', 'error']\" = True,\n",
      " |      fallback: 'Callable[[Any], Any] | None' = None,\n",
      " |      serialize_as_any: 'bool' = False\n",
      " |  ) -> 'str'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump_json`](../concepts/serialization.md#json-mode)\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          ensure_ascii: If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped.\n",
      " |              If `False` (the default), these characters will be output as-is.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, context: 'Any', /) -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      " |\n",
      " |  __get_pydantic_core_schema__(\n",
      " |      source: 'type[BaseModel]',\n",
      " |      handler: 'GetCoreSchemaHandler',\n",
      " |      /\n",
      " |  ) -> 'CoreSchema'\n",
      " |\n",
      " |  __get_pydantic_json_schema__(\n",
      " |      core_schema: 'CoreSchema',\n",
      " |      handler: 'GetJsonSchemaHandler',\n",
      " |      /\n",
      " |  ) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after basic class initialization is complete. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called, but forward annotations are not guaranteed to be resolved yet,\n",
      " |      meaning that creating an instance of the class may fail.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by Pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by Pydantic.\n",
      " |\n",
      " |      Note:\n",
      " |          You may want to override [`__pydantic_on_complete__()`][pydantic.main.BaseModel.__pydantic_on_complete__]\n",
      " |          instead, which is called once the class and its fields are fully initialized and ready for validation.\n",
      " |\n",
      " |  __pydantic_on_complete__() -> 'None'\n",
      " |      This is called once the class and its fields are fully initialized and ready to be used.\n",
      " |\n",
      " |      This typically happens when the class is created (just before\n",
      " |      [`__pydantic_init_subclass__()`][pydantic.main.BaseModel.__pydantic_init_subclass__] is called on the superclass),\n",
      " |      except when forward annotations are used that could not immediately be resolved.\n",
      " |      In that case, it will be called later, when the model is rebuilt automatically or explicitly using\n",
      " |      [`model_rebuild()`][pydantic.main.BaseModel.model_rebuild].\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(\n",
      " |      by_alias: 'bool' = True,\n",
      " |      ref_template: 'str' = '#/$defs/{model}',\n",
      " |      schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>,\n",
      " |      mode: 'JsonSchemaMode' = 'validation',\n",
      " |      *,\n",
      " |      union_format: \"Literal['any_of', 'primitive_type_array']\" = 'any_of'\n",
      " |  ) -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          union_format: The format to use when combining schemas from unions together. Can be one of:\n",
      " |\n",
      " |              - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n",
      " |              keyword to combine schemas (the default).\n",
      " |              - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n",
      " |              keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n",
      " |              type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n",
      " |              `any_of`.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(\n",
      " |      *,\n",
      " |      force: 'bool' = False,\n",
      " |      raise_errors: 'bool' = True,\n",
      " |      _parent_namespace_depth: 'int' = 2,\n",
      " |      _types_namespace: 'MappingNamespace | None' = None\n",
      " |  ) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(\n",
      " |      obj: 'Any',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      from_attributes: 'bool | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(\n",
      " |      json_data: 'str | bytes | bytearray',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |\n",
      " |  model_validate_strings(\n",
      " |      obj: 'Any',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(\n",
      " |      path: 'str | Path',\n",
      " |      *,\n",
      " |      content_type: 'str | None' = None,\n",
      " |      encoding: 'str' = 'utf8',\n",
      " |      proto: 'DeprecatedParseProtocol | None' = None,\n",
      " |      allow_pickle: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  parse_raw(\n",
      " |      b: 'str | bytes',\n",
      " |      *,\n",
      " |      content_type: 'str | None' = None,\n",
      " |      encoding: 'str' = 'utf8',\n",
      " |      proto: 'DeprecatedParseProtocol | None' = None,\n",
      " |      allow_pickle: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |\n",
      " |  schema_json(\n",
      " |      *,\n",
      " |      by_alias: 'bool' = True,\n",
      " |      ref_template: 'str' = '#/$defs/{model}',\n",
      " |      **dumps_kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      " |\n",
      " |  model_computed_fields = {}\n",
      " |\n",
      " |  model_fields = {'async_client': FieldInfo(annotation=Any, required=Fal...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  __or__(\n",
      " |      self,\n",
      " |      other: 'Union[Runnable[Any, Other], Callable[[Iterator[Any]], Iterator[Other]], Callable[[AsyncIterator[Any]], AsyncIterator[Other]], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]'\n",
      " |  ) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Runnable \"or\" operator.\n",
      " |\n",
      " |      Compose this ``Runnable`` with another object to create a\n",
      " |      ``RunnableSequence``.\n",
      " |\n",
      " |      Args:\n",
      " |          other: Another ``Runnable`` or a ``Runnable``-like object.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable``.\n",
      " |\n",
      " |  __ror__(\n",
      " |      self,\n",
      " |      other: 'Union[Runnable[Other, Any], Callable[[Iterator[Other]], Iterator[Any]], Callable[[AsyncIterator[Other]], AsyncIterator[Any]], Callable[[Other], Any], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]'\n",
      " |  ) -> 'RunnableSerializable[Other, Output]'\n",
      " |      Runnable \"reverse-or\" operator.\n",
      " |\n",
      " |      Compose this ``Runnable`` with another object to create a\n",
      " |      ``RunnableSequence``.\n",
      " |\n",
      " |      Args:\n",
      " |          other: Another ``Runnable`` or a ``Runnable``-like object.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable``.\n",
      " |\n",
      " |  async abatch(\n",
      " |      self,\n",
      " |      inputs: 'list[Input]',\n",
      " |      config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None,\n",
      " |      *,\n",
      " |      return_exceptions: 'bool' = False,\n",
      " |      **kwargs: 'Optional[Any]'\n",
      " |  ) -> 'list[Output]'\n",
      " |      Default implementation runs ``ainvoke`` in parallel using ``asyncio.gather``.\n",
      " |\n",
      " |      The default implementation of ``batch`` works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying ``Runnable`` uses an API which supports a batch mode.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``.\n",
      " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
      " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
      " |              for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of outputs from the ``Runnable``.\n",
      " |\n",
      " |  async abatch_as_completed(\n",
      " |      self,\n",
      " |      inputs: 'Sequence[Input]',\n",
      " |      config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None,\n",
      " |      *,\n",
      " |      return_exceptions: 'bool' = False,\n",
      " |      **kwargs: 'Optional[Any]'\n",
      " |  ) -> 'AsyncIterator[tuple[int, Union[Output, Exception]]]'\n",
      " |      Run ``ainvoke`` in parallel on a list of inputs.\n",
      " |\n",
      " |      Yields results as they complete.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``.\n",
      " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
      " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
      " |              for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |\n",
      " |      Yields:\n",
      " |          A tuple of the index of the input and the output from the ``Runnable``.\n",
      " |\n",
      " |  as_tool(\n",
      " |      self,\n",
      " |      args_schema: 'Optional[type[BaseModel]]' = None,\n",
      " |      *,\n",
      " |      name: 'Optional[str]' = None,\n",
      " |      description: 'Optional[str]' = None,\n",
      " |      arg_types: 'Optional[dict[str, type]]' = None\n",
      " |  ) -> 'BaseTool'\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |\n",
      " |      Create a ``BaseTool`` from a ``Runnable``.\n",
      " |\n",
      " |      ``as_tool`` will instantiate a ``BaseTool`` with a name, description, and\n",
      " |      ``args_schema`` from a ``Runnable``. Where possible, schemas are inferred\n",
      " |      from ``runnable.get_input_schema``. Alternatively (e.g., if the\n",
      " |      ``Runnable`` takes a dict as input and the specific dict keys are not typed),\n",
      " |      the schema can be specified directly with ``args_schema``. You can also\n",
      " |      pass ``arg_types`` to just specify the required arguments and their types.\n",
      " |\n",
      " |      Args:\n",
      " |          args_schema: The schema for the tool. Defaults to None.\n",
      " |          name: The name of the tool. Defaults to None.\n",
      " |          description: The description of the tool. Defaults to None.\n",
      " |          arg_types: A dictionary of argument names to types. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A ``BaseTool`` instance.\n",
      " |\n",
      " |      Typed dict input:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing_extensions import TypedDict\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          class Args(TypedDict):\n",
      " |              a: int\n",
      " |              b: list[int]\n",
      " |\n",
      " |\n",
      " |          def f(x: Args) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      ``dict`` input, specifying schema via ``args_schema``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import Any\n",
      " |          from pydantic import BaseModel, Field\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          class FSchema(BaseModel):\n",
      " |              \"\"\"Apply a function to an integer and list of integers.\"\"\"\n",
      " |\n",
      " |              a: int = Field(..., description=\"Integer\")\n",
      " |              b: list[int] = Field(..., description=\"List of ints\")\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(FSchema)\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      ``dict`` input, specifying schema via ``arg_types``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import Any\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def f(x: dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      String input:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          def f(x: str) -> str:\n",
      " |              return x + \"a\"\n",
      " |\n",
      " |\n",
      " |          def g(x: str) -> str:\n",
      " |              return x + \"z\"\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(f) | g\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke(\"b\")\n",
      " |\n",
      " |      .. versionadded:: 0.2.14\n",
      " |\n",
      " |  assign(\n",
      " |      self,\n",
      " |      **kwargs: 'Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any], Mapping[str, Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]]]]'\n",
      " |  ) -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this ``Runnable``.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
      " |          from langchain_core.runnables import Runnable\n",
      " |          from operator import itemgetter\n",
      " |\n",
      " |          prompt = (\n",
      " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
      " |              + \"{question}\"\n",
      " |          )\n",
      " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
      " |\n",
      " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
      " |\n",
      " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
      " |\n",
      " |          print(chain_with_assign.input_schema.model_json_schema())\n",
      " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
      " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
      " |          print(chain_with_assign.output_schema.model_json_schema())\n",
      " |          # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
      " |          {'str': {'title': 'Str',\n",
      " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: A mapping of keys to ``Runnable`` or ``Runnable``-like objects\n",
      " |              that will be invoked with the entire output dict of this ``Runnable``.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable``.\n",
      " |\n",
      " |  async astream_events(\n",
      " |      self,\n",
      " |      input: 'Any',\n",
      " |      config: 'Optional[RunnableConfig]' = None,\n",
      " |      *,\n",
      " |      version: \"Literal['v1', 'v2']\" = 'v2',\n",
      " |      include_names: 'Optional[Sequence[str]]' = None,\n",
      " |      include_types: 'Optional[Sequence[str]]' = None,\n",
      " |      include_tags: 'Optional[Sequence[str]]' = None,\n",
      " |      exclude_names: 'Optional[Sequence[str]]' = None,\n",
      " |      exclude_types: 'Optional[Sequence[str]]' = None,\n",
      " |      exclude_tags: 'Optional[Sequence[str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'AsyncIterator[StreamEvent]'\n",
      " |      Generate a stream of events.\n",
      " |\n",
      " |      Use to create an iterator over ``StreamEvents`` that provide real-time information\n",
      " |      about the progress of the ``Runnable``, including ``StreamEvents`` from intermediate\n",
      " |      results.\n",
      " |\n",
      " |      A ``StreamEvent`` is a dictionary with the following schema:\n",
      " |\n",
      " |      - ``event``: **str** - Event names are of the format:\n",
      " |        ``on_[runnable_type]_(start|stream|end)``.\n",
      " |      - ``name``: **str** - The name of the ``Runnable`` that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given\n",
      " |        execution of the ``Runnable`` that emitted the event. A child ``Runnable`` that gets\n",
      " |        invoked as part of the execution of a parent ``Runnable`` is assigned its own\n",
      " |        unique ID.\n",
      " |      - ``parent_ids``: **list[str]** - The IDs of the parent runnables that generated\n",
      " |        the event. The root ``Runnable`` will have an empty list. The order of the parent\n",
      " |        IDs is from the root to the immediate parent. Only available for v2 version of\n",
      " |        the API. The v1 version of the API will return an empty list.\n",
      " |      - ``tags``: **Optional[list[str]]** - The tags of the ``Runnable`` that generated\n",
      " |        the event.\n",
      " |      - ``metadata``: **Optional[dict[str, Any]]** - The metadata of the ``Runnable`` that\n",
      " |        generated the event.\n",
      " |      - ``data``: **dict[str, Any]**\n",
      " |\n",
      " |      Below is a table that illustrates some events that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |\n",
      " |      .. note::\n",
      " |          This reference table is for the v2 version of the schema.\n",
      " |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | event                    | name             | chunk                               | input                                             | output                                              |\n",
      " |      +==========================+==================+=====================================+===================================================+=====================================================+\n",
      " |      | ``on_chat_model_start``  | [model name]     |                                     | ``{\"messages\": [[SystemMessage, HumanMessage]]}`` |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_chat_model_stream`` | [model name]     | ``AIMessageChunk(content=\"hello\")`` |                                                   |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_chat_model_end``    | [model name]     |                                     | ``{\"messages\": [[SystemMessage, HumanMessage]]}`` | ``AIMessageChunk(content=\"hello world\")``           |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_llm_start``         | [model name]     |                                     | ``{'input': 'hello'}``                            |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_llm_stream``        | [model name]     | ``'Hello' ``                        |                                                   |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_llm_end``           | [model name]     |                                     | ``'Hello human!'``                                |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_chain_start``       | format_docs      |                                     |                                                   |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_chain_stream``      | format_docs      | ``'hello world!, goodbye world!'``  |                                                   |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_chain_end``         | format_docs      |                                     | ``[Document(...)]``                               | ``'hello world!, goodbye world!'``                  |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_tool_start``        | some_tool        |                                     | ``{\"x\": 1, \"y\": \"2\"}``                            |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_tool_end``          | some_tool        |                                     |                                                   | ``{\"x\": 1, \"y\": \"2\"}``                              |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_retriever_start``   | [retriever name] |                                     | ``{\"query\": \"hello\"}``                            |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_retriever_end``     | [retriever name] |                                     | ``{\"query\": \"hello\"}``                            | ``[Document(...), ..]``                             |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_prompt_start``      | [template_name]  |                                     | ``{\"question\": \"hello\"}``                         |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_prompt_end``        | [template_name]  |                                     | ``{\"question\": \"hello\"}``                         | ``ChatPromptValue(messages: [SystemMessage, ...])`` |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |\n",
      " |      In addition to the standard events, users can also dispatch custom events (see example below).\n",
      " |\n",
      " |      Custom events will be only be surfaced with in the v2 version of the API!\n",
      " |\n",
      " |      A custom event has following format:\n",
      " |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | Attribute | Type | Description                                                                                               |\n",
      " |      +===========+======+===========================================================================================================+\n",
      " |      | name      | str  | A user defined name for the event.                                                                        |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |\n",
      " |      Here are declarations associated with the standard events shown above:\n",
      " |\n",
      " |      ``format_docs``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          def format_docs(docs: list[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |\n",
      " |\n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |\n",
      " |      ``some_tool``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |\n",
      " |      ``prompt``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [\n",
      " |                  (\"system\", \"You are Cat Agent 007\"),\n",
      " |                  (\"human\", \"{question}\"),\n",
      " |              ]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |\n",
      " |\n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |\n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
      " |          ]\n",
      " |\n",
      " |          # will produce the following events (run_id, and parent_ids\n",
      " |          # has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |\n",
      " |\n",
      " |      Example: Dispatch Custom Event\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.callbacks.manager import (\n",
      " |              adispatch_custom_event,\n",
      " |          )\n",
      " |          from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
      " |          import asyncio\n",
      " |\n",
      " |\n",
      " |          async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
      " |              \"\"\"Do something that takes a long time.\"\"\"\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 1 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 2 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              return \"Done\"\n",
      " |\n",
      " |          slow_thing = RunnableLambda(slow_thing)\n",
      " |\n",
      " |          async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
      " |              print(event)\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``.\n",
      " |          version: The version of the schema to use either ``'v2'`` or ``'v1'``.\n",
      " |                   Users should use ``'v2'``.\n",
      " |                   ``'v1'`` is for backwards compatibility and will be deprecated\n",
      " |                   in 0.4.0.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |                   custom events will only be surfaced in ``'v2'``.\n",
      " |          include_names: Only include events from ``Runnables`` with matching names.\n",
      " |          include_types: Only include events from ``Runnables`` with matching types.\n",
      " |          include_tags: Only include events from ``Runnables`` with matching tags.\n",
      " |          exclude_names: Exclude events from ``Runnables`` with matching names.\n",
      " |          exclude_types: Exclude events from ``Runnables`` with matching types.\n",
      " |          exclude_tags: Exclude events from ``Runnables`` with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |              These will be passed to ``astream_log`` as this implementation\n",
      " |              of ``astream_events`` is built on top of ``astream_log``.\n",
      " |\n",
      " |      Yields:\n",
      " |          An async stream of ``StreamEvents``.\n",
      " |\n",
      " |      Raises:\n",
      " |          NotImplementedError: If the version is not ``'v1'`` or ``'v2'``.\n",
      " |\n",
      " |  async astream_log(\n",
      " |      self,\n",
      " |      input: 'Any',\n",
      " |      config: 'Optional[RunnableConfig]' = None,\n",
      " |      *,\n",
      " |      diff: 'bool' = True,\n",
      " |      with_streamed_output_list: 'bool' = True,\n",
      " |      include_names: 'Optional[Sequence[str]]' = None,\n",
      " |      include_types: 'Optional[Sequence[str]]' = None,\n",
      " |      include_tags: 'Optional[Sequence[str]]' = None,\n",
      " |      exclude_names: 'Optional[Sequence[str]]' = None,\n",
      " |      exclude_types: 'Optional[Sequence[str]]' = None,\n",
      " |      exclude_tags: 'Optional[Sequence[str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a ``Runnable``, as reported to the callback system.\n",
      " |\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |\n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      Jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |\n",
      " |      The Jsonpatch ops can be applied in order to construct state.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``.\n",
      " |          diff: Whether to yield diffs between each step or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the ``streamed_output`` list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |\n",
      " |      Yields:\n",
      " |          A ``RunLogPatch`` or ``RunLog`` object.\n",
      " |\n",
      " |  async atransform(\n",
      " |      self,\n",
      " |      input: 'AsyncIterator[Input]',\n",
      " |      config: 'Optional[RunnableConfig]' = None,\n",
      " |      **kwargs: 'Optional[Any]'\n",
      " |  ) -> 'AsyncIterator[Output]'\n",
      " |      Transform inputs to outputs.\n",
      " |\n",
      " |      Default implementation of atransform, which buffers input and calls ``astream``.\n",
      " |\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An async iterator of inputs to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the ``Runnable``.\n",
      " |\n",
      " |  batch(\n",
      " |      self,\n",
      " |      inputs: 'list[Input]',\n",
      " |      config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None,\n",
      " |      *,\n",
      " |      return_exceptions: 'bool' = False,\n",
      " |      **kwargs: 'Optional[Any]'\n",
      " |  ) -> 'list[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying ``Runnable`` uses an API which supports a batch mode.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``. The config supports\n",
      " |              standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work\n",
      " |              to do in parallel, and other keys. Please refer to the\n",
      " |              ``RunnableConfig`` for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of outputs from the ``Runnable``.\n",
      " |\n",
      " |  batch_as_completed(\n",
      " |      self,\n",
      " |      inputs: 'Sequence[Input]',\n",
      " |      config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None,\n",
      " |      *,\n",
      " |      return_exceptions: 'bool' = False,\n",
      " |      **kwargs: 'Optional[Any]'\n",
      " |  ) -> 'Iterator[tuple[int, Union[Output, Exception]]]'\n",
      " |      Run ``invoke`` in parallel on a list of inputs.\n",
      " |\n",
      " |      Yields results as they complete.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``.\n",
      " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
      " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
      " |              for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |\n",
      " |      Yields:\n",
      " |          Tuples of the index of the input and the output from the ``Runnable``.\n",
      " |\n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a ``Runnable``, returning a new ``Runnable``.\n",
      " |\n",
      " |      Useful when a ``Runnable`` in a chain requires an argument that is not\n",
      " |      in the output of the previous ``Runnable`` or included in the user input.\n",
      " |\n",
      " |      Args:\n",
      " |          kwargs: The arguments to bind to the ``Runnable``.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the arguments bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_ollama import ChatOllama\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |\n",
      " |          llm = ChatOllama(model=\"llama2\")\n",
      " |\n",
      " |          # Without bind.\n",
      " |          chain = llm | StrOutputParser()\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two three four five.'\n",
      " |\n",
      " |          # With bind.\n",
      " |          chain = llm.bind(stop=[\"three\"]) | StrOutputParser()\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two'\n",
      " |\n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'type[BaseModel]'\n",
      " |      The type of config this ``Runnable`` accepts specified as a pydantic model.\n",
      " |\n",
      " |      To mark a field as configurable, see the ``configurable_fields``\n",
      " |      and ``configurable_alternatives`` methods.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |\n",
      " |  get_config_jsonschema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the config of the ``Runnable``.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the config of the ``Runnable``.\n",
      " |\n",
      " |      .. versionadded:: 0.3.0\n",
      " |\n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this ``Runnable``.\n",
      " |\n",
      " |  get_input_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the input to the ``Runnable``.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the input to the ``Runnable``.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |\n",
      " |\n",
      " |              runnable = RunnableLambda(add_one)\n",
      " |\n",
      " |              print(runnable.get_input_jsonschema())\n",
      " |\n",
      " |      .. versionadded:: 0.3.0\n",
      " |\n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate input to the Runnable.\n",
      " |\n",
      " |      ``Runnable``s that leverage the ``configurable_fields`` and\n",
      " |      ``configurable_alternatives`` methods will have a dynamic input schema that\n",
      " |      depends on which configuration the ``Runnable`` is invoked with.\n",
      " |\n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |\n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the ``Runnable``.\n",
      " |\n",
      " |      Args:\n",
      " |          suffix: An optional suffix to append to the name.\n",
      " |          name: An optional name to use instead of the ``Runnable``'s name.\n",
      " |\n",
      " |      Returns:\n",
      " |          The name of the ``Runnable``.\n",
      " |\n",
      " |  get_output_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the output of the ``Runnable``.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema that represents the output of the ``Runnable``.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |\n",
      " |\n",
      " |              runnable = RunnableLambda(add_one)\n",
      " |\n",
      " |              print(runnable.get_output_jsonschema())\n",
      " |\n",
      " |      .. versionadded:: 0.3.0\n",
      " |\n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the ``Runnable``.\n",
      " |\n",
      " |      ``Runnable``s that leverage the ``configurable_fields`` and\n",
      " |      ``configurable_alternatives`` methods will have a dynamic output schema that\n",
      " |      depends on which configuration the ``Runnable`` is invoked with.\n",
      " |\n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |\n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'list[BasePromptTemplate]'\n",
      " |      Return a list of prompts used by this ``Runnable``.\n",
      " |\n",
      " |  map(self) -> 'Runnable[list[Input], list[Output]]'\n",
      " |      Return a new ``Runnable`` that maps a list of inputs to a list of outputs.\n",
      " |\n",
      " |      Calls ``invoke`` with each input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable`` that maps a list of inputs to a list of outputs.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |                  from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |                  def _lambda(x: int) -> int:\n",
      " |                      return x + 1\n",
      " |\n",
      " |\n",
      " |                  runnable = RunnableLambda(_lambda)\n",
      " |                  print(runnable.map().invoke([1, 2, 3]))  # [2, 3, 4]\n",
      " |\n",
      " |  pick(self, keys: 'Union[str, list[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the output dict of this ``Runnable``.\n",
      " |\n",
      " |      Pick single key:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              import json\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              chain = RunnableMap(str=as_str, json=as_json)\n",
      " |\n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
      " |\n",
      " |              json_only_chain = chain.pick(\"json\")\n",
      " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> [1, 2, 3]\n",
      " |\n",
      " |      Pick list of keys:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from typing import Any\n",
      " |\n",
      " |              import json\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |\n",
      " |\n",
      " |              def as_bytes(x: Any) -> bytes:\n",
      " |                  return bytes(x, \"utf-8\")\n",
      " |\n",
      " |\n",
      " |              chain = RunnableMap(\n",
      " |                  str=as_str, json=as_json, bytes=RunnableLambda(as_bytes)\n",
      " |              )\n",
      " |\n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |\n",
      " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
      " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |\n",
      " |      Args:\n",
      " |          keys: A key or list of keys to pick from the output dict.\n",
      " |\n",
      " |      Returns:\n",
      " |          a new ``Runnable``.\n",
      " |\n",
      " |  pipe(\n",
      " |      self,\n",
      " |      *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]',\n",
      " |      name: 'Optional[str]' = None\n",
      " |  ) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Pipe runnables.\n",
      " |\n",
      " |      Compose this ``Runnable`` with ``Runnable``-like objects to make a\n",
      " |      ``RunnableSequence``.\n",
      " |\n",
      " |      Equivalent to ``RunnableSequence(self, *others)`` or ``self | others[0] | ...``\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |\n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |\n",
      " |\n",
      " |              def mul_two(x: int) -> int:\n",
      " |                  return x * 2\n",
      " |\n",
      " |\n",
      " |              runnable_1 = RunnableLambda(add_one)\n",
      " |              runnable_2 = RunnableLambda(mul_two)\n",
      " |              sequence = runnable_1.pipe(runnable_2)\n",
      " |              # Or equivalently:\n",
      " |              # sequence = runnable_1 | runnable_2\n",
      " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |              sequence.invoke(1)\n",
      " |              await sequence.ainvoke(1)\n",
      " |              # -> 4\n",
      " |\n",
      " |              sequence.batch([1, 2, 3])\n",
      " |              await sequence.abatch([1, 2, 3])\n",
      " |              # -> [4, 6, 8]\n",
      " |\n",
      " |      Args:\n",
      " |          *others: Other ``Runnable`` or ``Runnable``-like objects to compose\n",
      " |          name: An optional name for the resulting ``RunnableSequence``.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable``.\n",
      " |\n",
      " |  transform(\n",
      " |      self,\n",
      " |      input: 'Iterator[Input]',\n",
      " |      config: 'Optional[RunnableConfig]' = None,\n",
      " |      **kwargs: 'Optional[Any]'\n",
      " |  ) -> 'Iterator[Output]'\n",
      " |      Transform inputs to outputs.\n",
      " |\n",
      " |      Default implementation of transform, which buffers input and calls ``astream``.\n",
      " |\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An iterator of inputs to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the ``Runnable``.\n",
      " |\n",
      " |  with_alisteners(\n",
      " |      self,\n",
      " |      *,\n",
      " |      on_start: 'Optional[AsyncListener]' = None,\n",
      " |      on_end: 'Optional[AsyncListener]' = None,\n",
      " |      on_error: 'Optional[AsyncListener]' = None\n",
      " |  ) -> 'Runnable[Input, Output]'\n",
      " |      Bind async lifecycle listeners to a ``Runnable``.\n",
      " |\n",
      " |      Returns a new ``Runnable``.\n",
      " |\n",
      " |      The Run object contains information about the run, including its ``id``,\n",
      " |      ``type``, ``input``, ``output``, ``error``, ``start_time``, ``end_time``, and\n",
      " |      any tags or metadata added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Called asynchronously before the ``Runnable`` starts running,\n",
      " |              with the ``Run`` object. Defaults to None.\n",
      " |          on_end: Called asynchronously after the ``Runnable`` finishes running,\n",
      " |              with the ``Run`` object. Defaults to None.\n",
      " |          on_error: Called asynchronously if the ``Runnable`` throws an error,\n",
      " |              with the ``Run`` object. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda, Runnable\n",
      " |          from datetime import datetime, timezone\n",
      " |          import time\n",
      " |          import asyncio\n",
      " |\n",
      " |          def format_t(timestamp: float) -> str:\n",
      " |              return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n",
      " |\n",
      " |          async def test_runnable(time_to_sleep : int):\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(time_to_sleep)\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          async def fn_start(run_obj : Runnable):\n",
      " |              print(f\"on start callback starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(3)\n",
      " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          async def fn_end(run_obj : Runnable):\n",
      " |              print(f\"on end callback starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(2)\n",
      " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          async def concurrent_runs():\n",
      " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
      " |\n",
      " |          asyncio.run(concurrent_runs())\n",
      " |          Result:\n",
      " |          on start callback starts at 2025-03-01T07:05:22.875378+00:00\n",
      " |          on start callback starts at 2025-03-01T07:05:22.875495+00:00\n",
      " |          on start callback ends at 2025-03-01T07:05:25.878862+00:00\n",
      " |          on start callback ends at 2025-03-01T07:05:25.878947+00:00\n",
      " |          Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n",
      " |          Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n",
      " |          Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n",
      " |          on end callback starts at 2025-03-01T07:05:27.882360+00:00\n",
      " |          Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n",
      " |          on end callback starts at 2025-03-01T07:05:28.882428+00:00\n",
      " |          on end callback ends at 2025-03-01T07:05:29.883893+00:00\n",
      " |          on end callback ends at 2025-03-01T07:05:30.884831+00:00\n",
      " |\n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a ``Runnable``, returning a new ``Runnable``.\n",
      " |\n",
      " |      Args:\n",
      " |          config: The config to bind to the ``Runnable``.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the config bound.\n",
      " |\n",
      " |  with_fallbacks(\n",
      " |      self,\n",
      " |      fallbacks: 'Sequence[Runnable[Input, Output]]',\n",
      " |      *,\n",
      " |      exceptions_to_handle: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,),\n",
      " |      exception_key: 'Optional[str]' = None\n",
      " |  ) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a ``Runnable``, returning a new ``Runnable``.\n",
      " |\n",
      " |      The new ``Runnable`` will try the original ``Runnable``, and then each fallback\n",
      " |      in order, upon failures.\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original ``Runnable``\n",
      " |              fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |              Defaults to ``(Exception,)``.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key.\n",
      " |              If None, exceptions will not be passed to fallbacks.\n",
      " |              If used, the base ``Runnable`` and its fallbacks must accept a\n",
      " |              dictionary as input. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable`` that will try the original ``Runnable``, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from typing import Iterator\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableGenerator\n",
      " |\n",
      " |\n",
      " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      " |                  raise ValueError()\n",
      " |                  yield \"\"\n",
      " |\n",
      " |\n",
      " |              def _generate(input: Iterator) -> Iterator[str]:\n",
      " |                  yield from \"foo bar\"\n",
      " |\n",
      " |\n",
      " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      " |                  [RunnableGenerator(_generate)]\n",
      " |              )\n",
      " |              print(\"\".join(runnable.stream({})))  # foo bar\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original ``Runnable``\n",
      " |              fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key.\n",
      " |              If None, exceptions will not be passed to fallbacks.\n",
      " |              If used, the base ``Runnable`` and its fallbacks must accept a\n",
      " |              dictionary as input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable`` that will try the original ``Runnable``, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |\n",
      " |  with_listeners(\n",
      " |      self,\n",
      " |      *,\n",
      " |      on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None,\n",
      " |      on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None,\n",
      " |      on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None\n",
      " |  ) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a ``Runnable``, returning a new ``Runnable``.\n",
      " |\n",
      " |      The Run object contains information about the run, including its ``id``,\n",
      " |      ``type``, ``input``, ``output``, ``error``, ``start_time``, ``end_time``, and\n",
      " |      any tags or metadata added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Called before the ``Runnable`` starts running, with the ``Run``\n",
      " |              object. Defaults to None.\n",
      " |          on_end: Called after the ``Runnable`` finishes running, with the ``Run``\n",
      " |              object. Defaults to None.\n",
      " |          on_error: Called if the ``Runnable`` throws an error, with the ``Run``\n",
      " |              object. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          from langchain_core.tracers.schemas import Run\n",
      " |\n",
      " |          import time\n",
      " |\n",
      " |\n",
      " |          def test_runnable(time_to_sleep: int):\n",
      " |              time.sleep(time_to_sleep)\n",
      " |\n",
      " |\n",
      " |          def fn_start(run_obj: Run):\n",
      " |              print(\"start_time:\", run_obj.start_time)\n",
      " |\n",
      " |\n",
      " |          def fn_end(run_obj: Run):\n",
      " |              print(\"end_time:\", run_obj.end_time)\n",
      " |\n",
      " |\n",
      " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
      " |              on_start=fn_start, on_end=fn_end\n",
      " |          )\n",
      " |          chain.invoke(2)\n",
      " |\n",
      " |  with_retry(\n",
      " |      self,\n",
      " |      *,\n",
      " |      retry_if_exception_type: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,),\n",
      " |      wait_exponential_jitter: 'bool' = True,\n",
      " |      exponential_jitter_params: 'Optional[ExponentialJitterParams]' = None,\n",
      " |      stop_after_attempt: 'int' = 3\n",
      " |  ) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original Runnable on exceptions.\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on.\n",
      " |              Defaults to (Exception,).\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait\n",
      " |              time between retries. Defaults to True.\n",
      " |          stop_after_attempt: The maximum number of attempts to make before\n",
      " |              giving up. Defaults to 3.\n",
      " |          exponential_jitter_params: Parameters for\n",
      " |              ``tenacity.wait_exponential_jitter``. Namely: ``initial``, ``max``,\n",
      " |              ``exp_base``, and ``jitter`` (all float values).\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          count = 0\n",
      " |\n",
      " |\n",
      " |          def _lambda(x: int) -> None:\n",
      " |              global count\n",
      " |              count = count + 1\n",
      " |              if x == 1:\n",
      " |                  raise ValueError(\"x is 1\")\n",
      " |              else:\n",
      " |                  pass\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          try:\n",
      " |              runnable.with_retry(\n",
      " |                  stop_after_attempt=2,\n",
      " |                  retry_if_exception_type=(ValueError,),\n",
      " |              ).invoke(1)\n",
      " |          except ValueError:\n",
      " |              pass\n",
      " |\n",
      " |          assert count == 2\n",
      " |\n",
      " |  with_types(\n",
      " |      self,\n",
      " |      *,\n",
      " |      input_type: 'Optional[type[Input]]' = None,\n",
      " |      output_type: 'Optional[type[Output]]' = None\n",
      " |  ) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a ``Runnable``, returning a new ``Runnable``.\n",
      " |\n",
      " |      Args:\n",
      " |          input_type: The input type to bind to the ``Runnable``. Defaults to None.\n",
      " |          output_type: The output type to bind to the ``Runnable``. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the types bound.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  config_specs\n",
      " |      List configurable fields for this ``Runnable``.\n",
      " |\n",
      " |  input_schema\n",
      " |      The type of input this ``Runnable`` accepts specified as a pydantic model.\n",
      " |\n",
      " |  output_schema\n",
      " |      Output schema.\n",
      " |\n",
      " |      The type of output this ``Runnable`` produces specified as a pydantic model.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __init_subclass__(...)\n",
      " |      Function to initialize subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "706516d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2.352s\n",
      "content='3^3 = 3 × 3 × 3 = 27.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 13, 'total_tokens': 38, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CW79MQhsXMpd9R7dglCCOKEUpxRA6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--dfdd3897-7ce7-42ef-b79b-acdc48396e01-0' usage_metadata={'input_tokens': 13, 'output_tokens': 25, 'total_tokens': 38, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "response = llm.invoke(\"What is 3^3?\")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.3f}s\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296d5329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2.870s\n",
      "content=[{'type': 'text', 'text': '3^3 = 27\\n\\nBecause 3^3 means 3 × 3 × 3 = 9 × 3 = 27.', 'annotations': []}] additional_kwargs={'reasoning': {'id': 'rs_0ba0c05806d029a50069027a1887f48196802c049ed81abfe7', 'summary': [], 'type': 'reasoning'}} response_metadata={'id': 'resp_0ba0c05806d029a50069027a1768388196abbbc47396e30ffc', 'created_at': 1761770007.0, 'metadata': {}, 'model': 'gpt-5-mini-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'completed', 'model_name': 'gpt-5-mini-2025-08-07'} id='msg_0ba0c05806d029a50069027a1904c08196a94855fadeba60b3' usage_metadata={'input_tokens': 13, 'output_tokens': 37, 'total_tokens': 50, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Define the reasoning settings\n",
    "reasoning = {\n",
    "    \"effort\": \"low\",  # 'low', 'medium', or 'high'\n",
    "    \"summary\": None,  # 'detailed', 'auto', or None\n",
    "}\n",
    "\n",
    "# Instantiate the model with these settings\n",
    "start_time = time.time()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\",  # This feature is mentioned with reasoning models\n",
    "    reasoning=reasoning\n",
    ")\n",
    "\n",
    "# This invocation will now use 'low' reasoning effort\n",
    "response = llm.invoke(\"What is 3^3?\")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.3f}s\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc1caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
