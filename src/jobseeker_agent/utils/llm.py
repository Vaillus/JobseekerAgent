# Importe toutes les classes de mod√®les au d√©but
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic
from typing import Optional

# Prix par million de tokens (input/output) - √Ä mettre √† jour r√©guli√®rement selon les prix actuels
MODEL_PRICES = {
    # OpenAI
    "gpt-o4-mini": {"input": 4.00, "output": 16.00},
    "gpt-4.1-nano": {"input": 0.20, "output": 0.80},
    "gpt-4.1-mini": {"input": 0.80, "output": 3.20},
    "gpt-4.1": {"input": 3.00, "output": 12.00}, 
    "gpt-5-nano": {"input": 0.050, "output": 0.40},  
    "gpt-5-mini": {"input": 0.250, "output": 2.00},  
    "gpt-5": {"input": 1.250, "output": 10.00}, 
    "gpt-5-pro": {"input": 15.00, "output": 120.00}, 
    
    # Anthropic
    "claude-3-opus": {"input": 15.00, "output": 75.00},
    "claude-3-5-sonnet": {"input": 3.00, "output": 15.00},
    "claude-3-sonnet": {"input": 3.00, "output": 15.00},
    "claude-3-haiku": {"input": 0.25, "output": 1.25},
    
    # Google Gemini
    "gemini-1.5-pro": {"input": 1.25, "output": 5.00},
    "gemini-1.5-flash": {"input": 0.075, "output": 0.30},
    "gemini-2.0-flash": {"input": 0.10, "output": 0.40},
    "gemini-2.5-flash": {"input": 0.10, "output": 0.40},
    "gemini-2.5-pro": {"input": 1.25, "output": 5.00},  # Estimation - √† v√©rifier
}


def calculate_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:
    """
    Calcule le co√ªt bas√© sur les tokens utilis√©s et le mod√®le.
    
    Args:
        model_name: Nom du mod√®le utilis√©
        input_tokens: Nombre de tokens d'entr√©e
        output_tokens: Nombre de tokens de sortie
    
    Returns:
        Co√ªt total en dollars
    """
    model_lower = model_name.lower()
    
    # Chercher une correspondance exacte
    if model_lower in MODEL_PRICES:
        prices = MODEL_PRICES[model_lower]
    else:
        # Chercher par pr√©fixe (ex: "gpt-4-0125-preview" -> "gpt-4")
        prices = None
        for known_model in MODEL_PRICES.keys():
            if model_lower.startswith(known_model):
                prices = MODEL_PRICES[known_model]
                break
        
        if not prices:
            print(f"‚ö†Ô∏è Prix non trouv√© pour le mod√®le '{model_name}'. Co√ªt = 0.")
            return 0.0
    
    input_cost = (input_tokens / 1_000_000) * prices["input"]
    output_cost = (output_tokens / 1_000_000) * prices["output"]
    
    return input_cost + output_cost


# --- üè≠ Ta fonction "Factory" ---
def get_llm(model_name: str, temperature: float = 0, reasoning: Optional[dict] = None):
    """
    Retourne une instance du mod√®le de chat en fonction de son nom.
    Args:
        model_name: Nom du mod√®le √† utiliser.
        temperature: Temp√©rature du mod√®le.
        reasoning: Configuration du raisonnement. Attention, cette 
        fonctionnalit√© est disponible uniquement pour les mod√®les OpenAI 
        pour le moment.
    Returns:
        Instance du mod√®le de chat.
    """
    if model_name.startswith("gpt"):
        print(f"‚úÖ Chargement du mod√®le OpenAI : {model_name}")
        return ChatOpenAI(model=model_name, temperature=temperature, reasoning=reasoning)
    
    elif "gemini" in model_name:
        print(f"‚úÖ Chargement du mod√®le Gemini : {model_name}")
        return ChatGoogleGenerativeAI(model=model_name, temperature=temperature)
        
    elif "claude" in model_name:
        print(f"‚úÖ Chargement du mod√®le Claude : {model_name}")
        return ChatAnthropic(model=model_name, temperature=temperature)
        
    else:
        raise ValueError(f"Mod√®le inconnu ou non support√© : {model_name}")