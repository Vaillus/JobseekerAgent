{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyse des performances du Keyword Extractor\n",
        "\n",
        "Objectif : Diagnostiquer les problèmes actuels de l'extraction de keywords pour améliorer la qualité des suggestions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# Chemins\n",
        "RESUME_DATA_DIR = Path(\"/Users/hugovaillaud/Documents/code/JobSeekerAgent/src/jobseeker_agent/data/resume\")\n",
        "\n",
        "# Dossiers à analyser\n",
        "JOB_IDS = [18, 71, 100, 270, 399, 405, 434, 771]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chargement des données\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargé 8 dossiers\n",
            "Job 18:\n",
            "  - Highlights: 13\n",
            "  - Keywords groups: 10\n",
            "  - Validated groups: 2\n",
            "Job 71:\n",
            "  - Highlights: 21\n",
            "  - Keywords groups: 9\n",
            "  - Validated groups: 5\n",
            "Job 100:\n",
            "  - Highlights: 13\n",
            "  - Keywords groups: 11\n",
            "  - Validated groups: 4\n",
            "Job 270:\n",
            "  - Highlights: 10\n",
            "  - Keywords groups: 14\n",
            "  - Validated groups: 3\n",
            "Job 399:\n",
            "  - Highlights: 11\n",
            "  - Keywords groups: 9\n",
            "  - Validated groups: 3\n",
            "Job 405:\n",
            "  - Highlights: 20\n",
            "  - Keywords groups: 5\n",
            "  - Validated groups: 4\n",
            "Job 434:\n",
            "  - Highlights: 43\n",
            "  - Keywords groups: 14\n",
            "  - Validated groups: 8\n",
            "Job 771:\n",
            "  - Highlights: 30\n",
            "  - Keywords groups: 10\n",
            "  - Validated groups: 6\n"
          ]
        }
      ],
      "source": [
        "def load_job_data(job_id: int) -> Dict[str, Any]:\n",
        "    \"\"\"Charge les 3 fichiers JSON pour un job donné.\"\"\"\n",
        "    job_dir = RESUME_DATA_DIR / str(job_id)\n",
        "    \n",
        "    data = {\n",
        "        \"job_id\": job_id,\n",
        "        \"highlights\": None,\n",
        "        \"keywords\": None,\n",
        "        \"keywords_validated\": None\n",
        "    }\n",
        "    \n",
        "    # Highlights\n",
        "    highlights_file = job_dir / \"highlights.json\"\n",
        "    if highlights_file.exists():\n",
        "        with open(highlights_file, 'r', encoding='utf-8') as f:\n",
        "            data[\"highlights\"] = json.load(f)\n",
        "    \n",
        "    # Keywords\n",
        "    keywords_file = job_dir / \"keywords.json\"\n",
        "    if keywords_file.exists():\n",
        "        with open(keywords_file, 'r', encoding='utf-8') as f:\n",
        "            data[\"keywords\"] = json.load(f)\n",
        "    \n",
        "    # Keywords validated\n",
        "    keywords_validated_file = job_dir / \"keywords_validated.json\"\n",
        "    if keywords_validated_file.exists():\n",
        "        with open(keywords_validated_file, 'r', encoding='utf-8') as f:\n",
        "            data[\"keywords_validated\"] = json.load(f)\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Charger tous les jobs\n",
        "jobs_data = {job_id: load_job_data(job_id) for job_id in JOB_IDS}\n",
        "\n",
        "print(f\"Chargé {len(jobs_data)} dossiers\")\n",
        "for job_id, data in jobs_data.items():\n",
        "    print(f\"Job {job_id}:\")\n",
        "    print(f\"  - Highlights: {len(data['highlights']) if data['highlights'] else 0}\")\n",
        "    print(f\"  - Keywords groups: {len(data['keywords']) if data['keywords'] else 0}\")\n",
        "    print(f\"  - Validated groups: {len(data['keywords_validated']) if data['keywords_validated'] else 0}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 1 : Vue d'ensemble quantitative\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# À compléter : calculer métriques de base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 2 : Analyse qualitative par échantillonnage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "JOB 100\n",
            "================================================================================\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "HIGHLIGHTS (13)\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            " 1. data pipeline\n",
            " 2. cross-functional team\n",
            " 3. Machine Learning\n",
            " 4. text-to-speech\n",
            " 5. speech-to-text,\n",
            " 6. reinforcement learning \n",
            " 7. vision\n",
            " 8. GCP\n",
            " 9. Python\n",
            "10. Agentic LLM pipeline\n",
            "11. ANN, rerankers, feedbackloops, knowledge graphs\n",
            "12. Classification Algorithms\n",
            "13. minimum cost flow problems\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "KEYWORDS VALIDÉS (sélection manuelle)\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            " 1. combinatorial optimization\n",
            " 2. heuristics\n",
            " 3. Bayesian optimization\n",
            " 4. experiment tracking (WandB)\n",
            " 5. data pipelines\n",
            " 6. data engineering\n",
            " 7. LangChain\n",
            " 8. agent frameworks\n",
            " 9. tool-augmented workflow\n",
            "10. LangGraph\n",
            "\n",
            "Total: 10 keywords\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "KEYWORDS REJETÉS (extraits mais non sélectionnés)\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            " 1. agentic LLM pipeline\n",
            " 2. generative agents\n",
            " 3. prompt engineering\n",
            " 4. prompt A/B testing\n",
            " 5. LLM fine-tuning\n",
            " 6. LLM providers\n",
            " 7. LLM lifecycle\n",
            " 8. knowledge retrieval system\n",
            " 9. ANN (approximate nearest neighbors)\n",
            "10. vector databases\n",
            "11. semantic search\n",
            "12. retrieval-augmented generation (RAG)\n",
            "13. rerankers\n",
            "14. knowledge graphs\n",
            "15. feedback loops\n",
            "16. classical NLP\n",
            "17. classification (NLP)\n",
            "18. intent detection\n",
            "19. sentiment analysis\n",
            "20. text-to-speech\n",
            "21. speech-to-text\n",
            "22. recommender systems\n",
            "23. product recommendation system\n",
            "24. minimum cost flow\n",
            "25. clustering (min-cost-flow)\n",
            "26. continuous training\n",
            "27. model observability\n",
            "28. model monitoring\n",
            "29. model deployment\n",
            "30. retraining pipelines\n",
            "31. Comet ML\n",
            "32. PromptLayer\n",
            "33. HuggingFace\n",
            "34. CI/CD\n",
            "35. test coverage\n",
            "36. FastAPI\n",
            "37. APIs\n",
            "38. batch jobs\n",
            "39. Kubeflow\n",
            "40. cron jobs\n",
            "41. domain-driven design\n",
            "42. k8s\n",
            "43. Terraform permissions\n",
            "44. Datadog\n",
            "45. PagerDuty\n",
            "46. feature pipelines\n",
            "47. BigQuery\n",
            "48. DBT\n",
            "49. GCP\n",
            "50. Vertex AI\n",
            "51. LLM providers (multiple)\n",
            "52. performance analysis post-release\n",
            "53. metrics/evaluation\n",
            "54. A/B testing (experiments)\n",
            "55. prompt A/B testing\n",
            "56. feedback loops\n",
            "\n",
            "Total: 56 keywords rejetés\n",
            "Taux de rétention: 10/66 = 15.2%\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def display_job_analysis(job_id: int):\n",
        "    \"\"\"Affiche les highlights et keywords extraits pour un job.\"\"\"\n",
        "    data = jobs_data.get(job_id)\n",
        "    if not data:\n",
        "        print(f\"Job {job_id} non trouvé\")\n",
        "        return\n",
        "    \n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"JOB {job_id}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    # 1. Highlights\n",
        "    print(f\"{'─'*80}\")\n",
        "    print(f\"HIGHLIGHTS ({len(data['highlights']) if data['highlights'] else 0})\")\n",
        "    print(f\"{'─'*80}\")\n",
        "    if data['highlights']:\n",
        "        for i, highlight in enumerate(data['highlights'], 1):\n",
        "            print(f\"{i:2d}. {highlight}\")\n",
        "    else:\n",
        "        print(\"Aucun highlight\")\n",
        "    \n",
        "    # 2. Keywords validés\n",
        "    print(f\"\\n{'─'*80}\")\n",
        "    print(f\"KEYWORDS VALIDÉS (sélection manuelle)\")\n",
        "    print(f\"{'─'*80}\")\n",
        "    \n",
        "    # Extraire tous les keywords validés de manière plate\n",
        "    validated_keywords = []\n",
        "    if data['keywords_validated']:\n",
        "        for group_name, group_data in data['keywords_validated'].items():\n",
        "            if 'keywords' in group_data:\n",
        "                validated_keywords.extend(group_data['keywords'])\n",
        "    \n",
        "    if validated_keywords:\n",
        "        for i, kw in enumerate(validated_keywords, 1):\n",
        "            print(f\"{i:2d}. {kw}\")\n",
        "        print(f\"\\nTotal: {len(validated_keywords)} keywords\")\n",
        "    else:\n",
        "        print(\"Aucun keyword validé\")\n",
        "    \n",
        "    # 3. Keywords rejetés (extraits mais non validés)\n",
        "    print(f\"\\n{'─'*80}\")\n",
        "    print(f\"KEYWORDS REJETÉS (extraits mais non sélectionnés)\")\n",
        "    print(f\"{'─'*80}\")\n",
        "    \n",
        "    # Extraire tous les keywords de manière plate\n",
        "    all_keywords = []\n",
        "    if data['keywords']:\n",
        "        for group_name, group_data in data['keywords'].items():\n",
        "            if isinstance(group_data, dict):\n",
        "                # Cas de la structure avec match_present, match_absent, mismatch_absent\n",
        "                for category in ['match_present', 'match_absent', 'mismatch_absent']:\n",
        "                    if category in group_data and group_data[category]:\n",
        "                        all_keywords.extend(group_data[category])\n",
        "            else:\n",
        "                # Cas d'une simple liste\n",
        "                all_keywords.extend(group_data)\n",
        "    \n",
        "    # Normaliser pour la comparaison (lowercase, strip)\n",
        "    validated_normalized = {kw.lower().strip() for kw in validated_keywords}\n",
        "    \n",
        "    # Filtrer les keywords non validés\n",
        "    rejected_keywords = [kw for kw in all_keywords if kw.lower().strip() not in validated_normalized]\n",
        "    \n",
        "    if rejected_keywords:\n",
        "        for i, kw in enumerate(rejected_keywords, 1):\n",
        "            print(f\"{i:2d}. {kw}\")\n",
        "        print(f\"\\nTotal: {len(rejected_keywords)} keywords rejetés\")\n",
        "        print(f\"Taux de rétention: {len(validated_keywords)}/{len(all_keywords)} = {len(validated_keywords)/len(all_keywords)*100:.1f}%\")\n",
        "    else:\n",
        "        print(\"Tous les keywords extraits ont été validés\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\\n\")\n",
        "\n",
        "# Exemple : afficher l'analyse pour le job 18\n",
        "display_job_analysis(JOB_IDS[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Génération du rapport complet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rapport généré : keywords_analysis_report.txt\n"
          ]
        }
      ],
      "source": [
        "# Générer le rapport pour tous les jobs\n",
        "import sys\n",
        "from io import StringIO\n",
        "\n",
        "output_file = Path(\"keywords_analysis_report.txt\")\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for job_id in JOB_IDS:\n",
        "        # Capturer l'output\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = StringIO()\n",
        "        \n",
        "        display_job_analysis(job_id)\n",
        "        \n",
        "        output = sys.stdout.getvalue()\n",
        "        sys.stdout = old_stdout\n",
        "        \n",
        "        f.write(output)\n",
        "\n",
        "print(f\"Rapport généré : {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyse des tendances - Keywords rejetés\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total de keywords rejetés (tous jobs): 334\n",
            "\n",
            "Top 20 keywords les plus souvent rejetés:\n",
            "================================================================================\n",
            " 5x - vector databases\n",
            " 5x - semantic search\n",
            " 4x - APIs\n",
            " 3x - root cause analysis\n",
            " 3x - PyTorch\n",
            " 3x - feedback loops\n",
            " 3x - Python\n",
            " 3x - RAG\n",
            " 3x - embeddings\n",
            " 2x - JAX\n",
            " 2x - AWS\n",
            " 2x - CI/CD\n",
            " 2x - BigQuery\n",
            " 2x - context management\n",
            " 2x - vector search\n",
            " 2x - scalable infrastructure\n",
            " 2x - cloud infrastructure\n",
            " 2x - containers\n",
            " 2x - CI/CD workflows\n",
            " 2x - production deployment of GenAI features\n",
            "\n",
            "\n",
            "Keywords rejetés uniques: 293\n"
          ]
        }
      ],
      "source": [
        "# Collecter tous les keywords rejetés\n",
        "from collections import Counter\n",
        "\n",
        "all_rejected = []\n",
        "\n",
        "for job_id in JOB_IDS:\n",
        "    data = jobs_data[job_id]\n",
        "    \n",
        "    # Extraire keywords validés\n",
        "    validated_keywords = []\n",
        "    if data['keywords_validated']:\n",
        "        for group_name, group_data in data['keywords_validated'].items():\n",
        "            if 'keywords' in group_data:\n",
        "                validated_keywords.extend(group_data['keywords'])\n",
        "    \n",
        "    # Extraire tous les keywords\n",
        "    all_keywords = []\n",
        "    if data['keywords']:\n",
        "        for group_name, group_data in data['keywords'].items():\n",
        "            if isinstance(group_data, dict):\n",
        "                for category in ['match_present', 'match_absent', 'mismatch_absent']:\n",
        "                    if category in group_data and group_data[category]:\n",
        "                        all_keywords.extend(group_data[category])\n",
        "            else:\n",
        "                all_keywords.extend(group_data)\n",
        "    \n",
        "    # Identifier les rejetés\n",
        "    validated_normalized = {kw.lower().strip() for kw in validated_keywords}\n",
        "    rejected = [kw for kw in all_keywords if kw.lower().strip() not in validated_normalized]\n",
        "    all_rejected.extend(rejected)\n",
        "\n",
        "# Analyser les tendances\n",
        "print(f\"Total de keywords rejetés (tous jobs): {len(all_rejected)}\")\n",
        "print(f\"\\nTop 20 keywords les plus souvent rejetés:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "most_common = Counter(all_rejected).most_common(20)\n",
        "for kw, count in most_common:\n",
        "    print(f\"{count:2d}x - {kw}\")\n",
        "\n",
        "print(f\"\\n\\nKeywords rejetés uniques: {len(set(all_rejected))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing on job: 18 - Applied ML/AI Engineer - Monitoring\n",
            "\n",
            "Extracting keywords with simplified agent...\n",
            "✅ Chargement du modèle OpenAI : gpt-4o\n",
            "Keyword extraction took 8.88 seconds\n",
            "\n",
            "✅ Extraction réussie!\n",
            "Nombre de keywords extraits: 53\n",
            "\n",
            "Premiers keywords:\n",
            "   1. data observability\n",
            "   2. data quality\n",
            "   3. machine learning\n",
            "   4. time series forecasting\n",
            "   5. intelligent alerting\n",
            "   6. generative AI\n",
            "   7. data profiling\n",
            "   8. root cause analysis\n",
            "   9. metadata\n",
            "  10. Python 3\n",
            "  ... et 43 autres\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structure d'évaluation et métriques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test de la structure d'évaluation et des fonctions de métriques\n",
        "from jobseeker_agent.customizer.evaluation.keyword_evaluator import (\n",
        "    KeywordEvaluationResult,\n",
        "    calculate_metrics,\n",
        "    add_metrics_to_result,\n",
        "    print_evaluation_summary\n",
        ")\n",
        "\n",
        "# Exemple de résultat d'évaluation (simulé pour test)\n",
        "example_result: KeywordEvaluationResult = {\n",
        "    \"true_positives\": [\n",
        "        {\"proposed\": \"agentic workflows\", \"matched_with\": \"AI agents\", \"confidence\": 0.9},\n",
        "        {\"proposed\": \"LangChain\", \"matched_with\": \"LangChain\", \"confidence\": 1.0},\n",
        "        {\"proposed\": \"prompt engineering\", \"matched_with\": \"prompt engineering\", \"confidence\": 1.0},\n",
        "    ],\n",
        "    \"false_positives\": [\n",
        "        {\"proposed\": \"AWS\", \"reason\": \"not in ground truth\"},\n",
        "        {\"proposed\": \"APIs\", \"reason\": \"not in ground truth\"},\n",
        "        {\"proposed\": \"vector databases\", \"reason\": \"not in ground truth\"},\n",
        "    ],\n",
        "    \"false_negatives\": [\n",
        "        {\"ground_truth\": \"context injection\", \"reason\": \"not proposed by agent\"},\n",
        "        {\"ground_truth\": \"tracing\", \"reason\": \"not proposed by agent\"},\n",
        "    ],\n",
        "    \"metrics\": {}  # Sera calculé\n",
        "}\n",
        "\n",
        "# Calculer les métriques\n",
        "result_with_metrics = add_metrics_to_result(example_result)\n",
        "\n",
        "# Afficher le résumé\n",
        "print_evaluation_summary(result_with_metrics, job_id=999)\n",
        "\n",
        "# Vérifier les calculs manuellement\n",
        "print(\"Vérification des calculs:\")\n",
        "print(f\"TP: {len(example_result['true_positives'])}\")\n",
        "print(f\"FP: {len(example_result['false_positives'])}\")\n",
        "print(f\"FN: {len(example_result['false_negatives'])}\")\n",
        "print(f\"Precision attendue: {3/(3+3):.4f} = {result_with_metrics['metrics']['precision']}\")\n",
        "print(f\"Recall attendu: {3/(3+2):.4f} = {result_with_metrics['metrics']['recall']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyse par catégories:\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Infrastructure/DevOps générique (16 occurrences):\n",
            "  2x - AWS\n",
            "  2x - CI/CD\n",
            "  2x - cloud infrastructure\n",
            "  2x - containers\n",
            "  2x - CI/CD workflows\n",
            "  1x - Kubernetes (AWS EKS)\n",
            "  1x - MySQL (AWS RDS)\n",
            "  1x - GCP\n",
            "  1x - AWS Bedrock\n",
            "  1x - CI/CD integration\n",
            "\n",
            "Soft skills vagues (4 occurrences):\n",
            "  1x - best practices\n",
            "  1x - user interactions\n",
            "  1x - cross-functional collaboration with product and design\n",
            "  1x - Collaboration with electronics/software/system teams\n",
            "\n",
            "Termes trop généraux (22 occurrences):\n",
            "  4x - APIs\n",
            "  2x - model monitoring\n",
            "  1x - monitoring engine\n",
            "  1x - model evaluation / monitoring\n",
            "  1x - metadata (monitoring-related)\n",
            "  1x - scalability\n",
            "  1x - write APIs\n",
            "  1x - evaluation & quality monitoring\n",
            "  1x - quality monitoring\n",
            "  1x - performance analysis post-release\n",
            "\n",
            "Outils spécifiques (7 occurrences):\n",
            "  2x - BigQuery\n",
            "  1x - Prometheus\n",
            "  1x - Loki\n",
            "  1x - Grafana\n",
            "  1x - Sentry\n",
            "  1x - Snowflake\n",
            "\n",
            "Déploiement/Production (18 occurrences):\n",
            "  2x - production deployment of GenAI features\n",
            "  2x - model deployment\n",
            "  1x - deploy time series models\n",
            "  1x - deploy and maintain models in production\n",
            "  1x - model deployment pipelines\n",
            "  1x - software engineering: design/implement/deploy/maintain\n",
            "  1x - production ML deployment\n",
            "  1x - MLOps\n",
            "  1x - LLMOps\n",
            "  1x - deploy models into production\n"
          ]
        }
      ],
      "source": [
        "# Catégorisation manuelle des patterns\n",
        "# À compléter en regardant les résultats ci-dessus\n",
        "\n",
        "# Patterns suspects (exemples à ajuster selon les résultats)\n",
        "patterns = {\n",
        "    \"Infrastructure/DevOps générique\": [\"CI/CD\", \"containers\", \"cloud infrastructure\", \"Docker\", \"Kubernetes\", \"AWS\", \"GCP\"],\n",
        "    \"Soft skills vagues\": [\"collaboration\", \"best practices\", \"user interactions\"],\n",
        "    \"Termes trop généraux\": [\"scalability\", \"monitoring\", \"APIs\", \"performance\", \"maintainability\"],\n",
        "    \"Outils spécifiques\": [\"Prometheus\", \"Loki\", \"Grafana\", \"Sentry\", \"BigQuery\", \"Snowflake\"],\n",
        "    \"Déploiement/Production\": [\"deploy\", \"deployment\", \"production\", \"serving\", \"MLOps\", \"LLMOps\"],\n",
        "}\n",
        "\n",
        "print(\"Analyse par catégories:\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for category, keywords in patterns.items():\n",
        "    matches = [kw for kw in all_rejected if any(pattern.lower() in kw.lower() for pattern in keywords)]\n",
        "    if matches:\n",
        "        print(f\"\\n{category} ({len(matches)} occurrences):\")\n",
        "        # Compter les occurrences\n",
        "        match_counts = Counter(matches).most_common(10)\n",
        "        for kw, count in match_counts:\n",
        "            print(f\"  {count}x - {kw}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 3 : Identification des problèmes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# À compléter : synthétiser les patterns d'erreurs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes et observations\n",
        "\n",
        "### Patterns d'erreurs identifiés\n",
        "\n",
        "1. ...\n",
        "2. ...\n",
        "3. ...\n",
        "\n",
        "### Hypothèses\n",
        "\n",
        "- ...\n",
        "- ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
